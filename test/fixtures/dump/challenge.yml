---
challenge_43:
  id: 43
  organizer_id: 25
  challenge: League of Nations Archives Digitization Challenge
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &1 2018-07-23 15:55:27.670829000 Z
    zone: &2 !ruby/object:ActiveSupport::TimeZone
      name: Etc/UTC
    time: *1
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &3 2018-10-04 09:22:22.099570000 Z
    zone: *2
    time: *3
  tagline: Help us share the archives of the League of Nations, a vital part of world
    history
  primary_sort_order_cd: descending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 3158
  participant_count: 111
  submission_count: 262
  score_title: F1_Score
  score_secondary_title: Log Loss
  slug: league-of-nations-archives-digitization-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: un_library_challenge_2018
  online_grading: true
  vote_count: 14
  description_markdown: "**Starter Kit** : [https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit](https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit)\r\n\r\nThe
    documents in the archives of the [ League of Nations ](https://www.unog.ch/80256EDD006AC19C/(httpPages)/242056AEA671DEF780256EF30037A2A8?OpenDocument){:target='lon_history'}
    (1919-1946) contain important historical information about the origins of the
    United Nations that are relevant to understanding the UN and its many roles today,
    as well being a critical resource for the history of international relations during
    the interwar period. The official documents in particular are a vital source for
    researchers, as they provide the official output of the various bodies of the
    League, from the well-known League Council and Assembly, to the most technical
    sub-committees and conferences, including minutes, final reports, and official
    working papers.\r\n\r\nBecause these data exist only in the form of paper archives
    at the [ UN Library in Geneva ](https://www.unog.ch/library){:target='un_library_page'},
    the digital transformation of the documents represents an important challenge.
    Digitization of these archives will enable far wider access to these documents,
    which are currently only accessible to researchers who can visit the archives
    in person. Furthermore, while digitization itself is obviously critical, providing
    intellectual access to the documents is no less necessary. \r\n\r\nCurrently,
    there are no comprehensive indexes or catalogues of the official documents collection,
    and it is therefore necessary to provide basic points of access to the materials,
    including the title, document symbol, date, and language of each document. This
    is currently done in a largely manual process, which is extremely labour intensive
    and relatively slow. \r\n\r\nThe documents themselves unfortunately present further
    challenges to rendering their contents accessible. There is a wide variety in
    the formats and layouts of title pages, titles may be complex and multi-faceted,
    and documents exist in multiple languages, including bi- and multi-lingual texts.
    Moreover, the quality of the printing and reproduction methods used has often
    resulted in poor quality text, for which it can be difficult to achieve good results
    using Optical Character Recognition (OCR) analyses.\r\n\r\nThe training dataset
    contains more than 4500 documents in English or French. \r\nMore details can be
    found in the [Dataset](https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/dataset_files)
    of the challenge. \r\n\r\n**The challenge is to identify the language in each
    of the documents accurately using any model for prediction.**"
  description: "<p><strong>Starter Kit</strong> : <a href=\"https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit\">https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit</a></p>\n\n<p>The
    documents in the archives of the <a href=\"https://www.unog.ch/80256EDD006AC19C/(httpPages)/242056AEA671DEF780256EF30037A2A8?OpenDocument\"
    target=\"lon_history\"> League of Nations </a> (1919-1946) contain important historical
    information about the origins of the United Nations that are relevant to understanding
    the UN and its many roles today, as well being a critical resource for the history
    of international relations during the interwar period. The official documents
    in particular are a vital source for researchers, as they provide the official
    output of the various bodies of the League, from the well-known League Council
    and Assembly, to the most technical sub-committees and conferences, including
    minutes, final reports, and official working papers.</p>\n\n<p>Because these data
    exist only in the form of paper archives at the <a href=\"https://www.unog.ch/library\"
    target=\"un_library_page\"> UN Library in Geneva </a>, the digital transformation
    of the documents represents an important challenge. Digitization of these archives
    will enable far wider access to these documents, which are currently only accessible
    to researchers who can visit the archives in person. Furthermore, while digitization
    itself is obviously critical, providing intellectual access to the documents is
    no less necessary.</p>\n\n<p>Currently, there are no comprehensive indexes or
    catalogues of the official documents collection, and it is therefore necessary
    to provide basic points of access to the materials, including the title, document
    symbol, date, and language of each document. This is currently done in a largely
    manual process, which is extremely labour intensive and relatively slow.</p>\n\n<p>The
    documents themselves unfortunately present further challenges to rendering their
    contents accessible. There is a wide variety in the formats and layouts of title
    pages, titles may be complex and multi-faceted, and documents exist in multiple
    languages, including bi- and multi-lingual texts. Moreover, the quality of the
    printing and reproduction methods used has often resulted in poor quality text,
    for which it can be difficult to achieve good results using Optical Character
    Recognition (OCR) analyses.</p>\n\n<p>The training dataset contains more than
    4500 documents in English or French. \nMore details can be found in the <a href=\"https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/dataset_files\">Dataset</a>
    of the challenge.</p>\n\n<p><strong>The challenge is to identify the language
    in each of the documents accurately using any model for prediction.</strong></p>\n"
  evaluation_markdown: "The primary metric for evaluation will be the Mean [F1-Score](https://en.wikipedia.org/wiki/F1_score),
    and the secondary metric for the evaluation with be the Mean [Log Loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss)\r\n\r\nThe
    **Mean Log Loss** is defined by\r\n\r\n$ L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^{C}
    y_{nc} \\ln(p_{nc}), $\r\n\r\nwhere\r\n\r\n* $N= 14216$ is the number of examples
    in the test set,\r\n* $C=2$ is the number of class labels, i.e. languages : [en,
    fr],\r\n* $y_{nc}$ is a binary value indicating if the n-th instance belongs to
    the c-th label,\r\n* $p_{nc}$ is the probability according to your submission
    that the n-th instance belongs to the c-th label,\r\n* $\\ln$ is the natural logarithmic
    function.\r\n\r\nThe $F_1$ score for a particular class $c$ is given by\r\n\r\n$
    F_1^c = 2\\frac{p^c r^c}{p^c + r^c}, $\r\n\r\nwhere\r\n\r\n* $p^c = \\frac{tp^c}{tp^c
    + fp^c}$ is the precision for class $c$,\r\n* $r^c = \\frac{tp^c}{tp^c + fn^c}$
    is the recall for class $c$,\r\n* $tp^c$ refers to the number of True Positives
    for class $c$,\r\n* $fp^c$ refers to the number of False Positives for class $c$,\r\n*
    $fn^c$ refers to the number of False Negatives for class $c$.\r\n\r\nThe final
    **Mean $F_1$ Score** is then defined as\r\n\r\n$ F_1 = \\frac{1}{C} \\sum_{c=1}^{C}
    F_1^c. $\r\n\r\nThe participants have to submit a CSV file with the following
    header:\r\n\r\n```\r\nfilename,en,fr\r\n```\r\n\r\nEach row is then an entry for
    every file in the test set (in the sorted order of the `filename`s). The first
    column in every row represents the `filename` (which is the name of the test file
    with its '.jpg' extension) and the rest of the $C=2$ columns are the predicted
    probabilities for each class in the order mentioned in the above CSV header.\r\n\r\nA
    sample row would look like. :    \r\n\r\n```\r\n58dc3fae0f039187c6615393b31c4978.jpg,
    0.765, 0.235\r\n```\r\n   \r\nwhich means that for the image in the file `58dc3fae0f039187c6615393b31c4978.jpg`,
    you are `76.5%` confident that it belongs to the class `en` and `23.5%` confident
    that it belongs to the class `fr`.\r\n\r\nAn example of a French document will
    be:\r\n![ french_lon ](https://preview.ibb.co/g2dV0p/C_1930_345_410_0022_result.jpg)\r\n\r\nAn
    example of an English document will be:\r\n![ english_lon ](https://preview.ibb.co/knpdLp/C_1930_345_410_0026_result.jpg)"
  evaluation: |
    <p>The primary metric for evaluation will be the Mean <a href="https://en.wikipedia.org/wiki/F1_score">F1-Score</a>, and the secondary metric for the evaluation with be the Mean <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss">Log Loss</a></p>

    <p>The <strong>Mean Log Loss</strong> is defined by</p>

    <p>$ L = - \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^{C} y_{nc} \ln(p_{nc}), $</p>

    <p>where</p>

    <ul>
      <li>$N= 14216$ is the number of examples in the test set,</li>
      <li>$C=2$ is the number of class labels, i.e. languages : [en, fr],</li>
      <li>$y_{nc}$ is a binary value indicating if the n-th instance belongs to the c-th label,</li>
      <li>$p_{nc}$ is the probability according to your submission that the n-th instance belongs to the c-th label,</li>
      <li>$\ln$ is the natural logarithmic function.</li>
    </ul>

    <p>The $F_1$ score for a particular class $c$ is given by</p>

    <p>$ F_1^c = 2\frac{p^c r^c}{p^c + r^c}, $</p>

    <p>where</p>

    <ul>
      <li>$p^c = \frac{tp^c}{tp^c + fp^c}$ is the precision for class $c$,</li>
      <li>$r^c = \frac{tp^c}{tp^c + fn^c}$ is the recall for class $c$,</li>
      <li>$tp^c$ refers to the number of True Positives for class $c$,</li>
      <li>$fp^c$ refers to the number of False Positives for class $c$,</li>
      <li>$fn^c$ refers to the number of False Negatives for class $c$.</li>
    </ul>

    <p>The final <strong>Mean $F_1$ Score</strong> is then defined as</p>

    <p>$ F_1 = \frac{1}{C} \sum_{c=1}^{C} F_1^c. $</p>

    <p>The participants have to submit a CSV file with the following header:</p>

    <p><code class="highlighter-rouge">
    filename,en,fr
    </code></p>

    <p>Each row is then an entry for every file in the test set (in the sorted order of the <code class="highlighter-rouge">filename</code>s). The first column in every row represents the <code class="highlighter-rouge">filename</code> (which is the name of the test file with its ‘.jpg’ extension) and the rest of the $C=2$ columns are the predicted probabilities for each class in the order mentioned in the above CSV header.</p>

    <p>A sample row would look like. :</p>

    <p><code class="highlighter-rouge">
    58dc3fae0f039187c6615393b31c4978.jpg, 0.765, 0.235
    </code></p>

    <p>which means that for the image in the file <code class="highlighter-rouge">58dc3fae0f039187c6615393b31c4978.jpg</code>, you are <code class="highlighter-rouge">76.5%</code> confident that it belongs to the class <code class="highlighter-rouge">en</code> and <code class="highlighter-rouge">23.5%</code> confident that it belongs to the class <code class="highlighter-rouge">fr</code>.</p>

    <p>An example of a French document will be:
    <img src="https://preview.ibb.co/g2dV0p/C_1930_345_410_0022_result.jpg" alt=" french_lon " /></p>

    <p>An example of an English document will be:
    <img src="https://preview.ibb.co/knpdLp/C_1930_345_410_0026_result.jpg" alt=" english_lon " /></p>
  rules_markdown: "- Participants are allowed at most 10 submissions per 24h.\r\n-
    Participants are welcome to form teams. Teams should submit their predictions
    under a single account. \r\n- Participants have to release their solution under
    an Open Source License of their choice to be eligible for prizes. We encourage
    all participants to open-source their code!\r\n- The use of pre-trained models
    is nevertheless permitted.\r\n- While submissions by Admins and Organizers can
    serve as baselines, they won’t be considered in the final leaderboard.\r\n- In
    case of conflicts, the decision of the Organizers will be final and binding.\r\n-
    Organizers reserve the right to make changes to the rules and timeline.\r\n- Violation
    of the rules or other unfair activity may result in disqualification."
  rules: |
    <ul>
      <li>Participants are allowed at most 10 submissions per 24h.</li>
      <li>Participants are welcome to form teams. Teams should submit their predictions under a single account.</li>
      <li>Participants have to release their solution under an Open Source License of their choice to be eligible for prizes. We encourage all participants to open-source their code!</li>
      <li>The use of pre-trained models is nevertheless permitted.</li>
      <li>While submissions by Admins and Organizers can serve as baselines, they won’t be considered in the final leaderboard.</li>
      <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
      <li>Organizers reserve the right to make changes to the rules and timeline.</li>
      <li>Violation of the rules or other unfair activity may result in disqualification.</li>
    </ul>
  prizes_markdown: "The winning participant will be extended a travel grant by [Citizen
    Cyberlab ](http://www.citizencyberlab.org/){:target='citizencyberlabwebsite'}
    to attend the [ AI for Good summit ](https://www.itu.int/en/ITU-T/AI/2018/Pages/default.aspx){:target='aiforgoodwebsite'}
    2019 in Geneva, Switzerland.\r\nThe travel grant covers the travel and accommodation
    expenses up to **CHF 1500**."
  prizes: |
    <p>The winning participant will be extended a travel grant by <a href="http://www.citizencyberlab.org/" target="citizencyberlabwebsite">Citizen Cyberlab </a> to attend the <a href="https://www.itu.int/en/ITU-T/AI/2018/Pages/default.aspx" target="aiforgoodwebsite"> AI for Good summit </a> 2019 in Geneva, Switzerland.
    The travel grant covers the travel and accommodation expenses up to <strong>CHF 1500</strong>.</p>
  resources_markdown: "* The starter kit for making submissions can be found at :
    [https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit](https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit)\r\n\r\n*
    More information about the background of the project can also be found at: [ Crowdsourcing
    Campaign ](https://www.zooniverse.org/projects/nshreyasvi/league-of-nations-in-the-digital-age){:target='zooniversecampaign'}\r\n\r\n\r\n##
    Contact Us\r\n\r\nUse one of the public channels:\r\n\r\n* Gitter Channel : [crowdAI/un_library_challenge](https://gitter.im/crowdAI/un_library_challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics](https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n\r\n*  [sharada.mohanty@epfl.ch](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <ul>
      <li>
        <p>The starter kit for making submissions can be found at : <a href="https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit">https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit</a></p>
      </li>
      <li>
        <p>More information about the background of the project can also be found at: <a href="https://www.zooniverse.org/projects/nshreyasvi/league-of-nations-in-the-digital-age" target="zooniversecampaign"> Crowdsourcing Campaign </a></p>
      </li>
    </ul>

    <h2 id="contact-us">Contact Us</h2>

    <p>Use one of the public channels:</p>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/un_library_challenge">crowdAI/un_library_challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics">https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank">sharada.mohanty@epfl.ch</a></li>
    </ul>
  submission_instructions_markdown: "- The code must be well packaged and well organized\r\n-
    The contents of the submission must include a documentation of the approach used,
    code snippets and prediction results from the machine learning model approach
    taken\r\n- In case any special requirements are needed for re-running the code,
    specific information must be highlighted in the documentation of the project."
  submission_instructions: |
    <ul>
      <li>The code must be well packaged and well organized</li>
      <li>The contents of the submission must include a documentation of the approach used, code snippets and prediction results from the machine learning model approach taken</li>
      <li>In case any special requirements are needed for re-running the code, specific information must be highlighted in the documentation of the project.</li>
    </ul>
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: 488px-Small_Flag_of_the_United_Nations_ZP.svg.png
  featured_sequence: 11
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: crowdAI_GRADER_POOL
  online_submissions: true
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_31:
  id: 31
  organizer_id: 4
  challenge: 'NIPS 2018: AI for Prosthetics Challenge'
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &4 2018-04-12 23:15:15.820658000 Z
    zone: *2
    time: *4
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &5 2018-10-04 10:56:40.793873000 Z
    zone: *2
    time: *5
  tagline: Reinforcement learning with musculoskeletal models
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 38056
  participant_count: 349
  submission_count: 3282
  score_title: Cumulative Reward
  score_secondary_title: ''
  slug: nips-2018-ai-for-prosthetics-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: ai-for-prosthetics-challenge-2018
  online_grading: true
  vote_count: 160
  description_markdown: "> **Update 08/27: We've updated osim-rl package to version
    2.1. If you joined before 08/27/2018 please run in your conda environment:**<br><br>\r\n```\r\npip
    install git+https://github.com/stanfordnmbl/osim-rl.git -U\r\n```\r\n\r\n> **Update
    07/27: Google Cloud Platform will sponsor participants of the challenge. Top 400
    teams with positive (>0) number of points will be awarded 250$ cloud credits!**
    \r\n\r\n> **Update 07/30: Watch our [webinar](https://www.youtube.com/watch?v=M2D5xSSxshE)
    to learn more about biomechanics, neuroscience and reinforcement learning!**\r\n\r\nWelcome
    to **AI for Prosthetics challenge**, one of the official challenges in the [NIPS
    2018 Competition Track](https://nips.cc/Conferences/2018/CompetitionTrack). In
    this competition, you are tasked with developing a controller to enable a physiologically-based
    human model with a prosthetic leg to walk and run. You are provided with a human
    musculoskeletal model, a physics-based simulation environment [OpenSim](http://opensim.stanford.edu/)
    where you can synthesize physically and physiologically accurate motion, and datasets
    of normal gait kinematics. You are scored based on how well your agent adapts
    to the requested velocity vector changing in real time.\r\n\r\nFollow the instructions
    on our [github repo](https://github.com/stanfordnmbl/osim-rl) to get started!\r\n\r\n[![AI
    for prosthetics](https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/images/ai-prosthetics.jpg)](https://github.com/stanfordnmbl/osim-rl)\r\n\r\nOur
    objectives are to:\r\n\r\n* bring Deep Reinforcement Learning to solve problems
    in medicine,\r\n* promote open-source tools in RL research (the physics simulator
    [OpenSim](http://opensim.stanford.edu/), the RL environment, and the competition
    platform are all open-source),\r\n* encourage RL research in computationally complex
    environments, with stochasticity and highly-dimensional action spaces.\r\n\r\nVisit
    our [github repo](https://github.com/stanfordnmbl/osim-rl) to get started!\r\n\r\n##
    What's new compared to NIPS 2017: Learning to run?\r\n\r\nWe took into account
    comments from the last challenge and there are several changes:\r\n\r\n* **You
    can use experimental data (to greatly speed up the learning process)**\r\n* We
    released the 3rd dimensions [OpenSim](http://opensim.stanford.edu/) model (the
    model can fall sideways)\r\n* We added a prosthetic leg -- the goal is to solve
    a medical challenge on modeling how walking will change after getting a prosthesis.
    Your work can speed up design, prototying, or tuning prosthetics!\r\n\r\nYou haven't
    heard of NIPS 2017: Learning to run? [Watch this video!](https://www.youtube.com/watch?v=rhNxt0VccsE)"
  description: |
    <blockquote>
      <p><strong>Update 08/27: We’ve updated osim-rl package to version 2.1. If you joined before 08/27/2018 please run in your conda environment:</strong><br /><br />
    <code class="highlighter-rouge">
    pip install git+https://github.com/stanfordnmbl/osim-rl.git -U
    </code></p>
    </blockquote>

    <blockquote>
      <p><strong>Update 07/27: Google Cloud Platform will sponsor participants of the challenge. Top 400 teams with positive (&gt;0) number of points will be awarded 250$ cloud credits!</strong></p>
    </blockquote>

    <blockquote>
      <p><strong>Update 07/30: Watch our <a href="https://www.youtube.com/watch?v=M2D5xSSxshE">webinar</a> to learn more about biomechanics, neuroscience and reinforcement learning!</strong></p>
    </blockquote>

    <p>Welcome to <strong>AI for Prosthetics challenge</strong>, one of the official challenges in the <a href="https://nips.cc/Conferences/2018/CompetitionTrack">NIPS 2018 Competition Track</a>. In this competition, you are tasked with developing a controller to enable a physiologically-based human model with a prosthetic leg to walk and run. You are provided with a human musculoskeletal model, a physics-based simulation environment <a href="http://opensim.stanford.edu/">OpenSim</a> where you can synthesize physically and physiologically accurate motion, and datasets of normal gait kinematics. You are scored based on how well your agent adapts to the requested velocity vector changing in real time.</p>

    <p>Follow the instructions on our <a href="https://github.com/stanfordnmbl/osim-rl">github repo</a> to get started!</p>

    <p><a href="https://github.com/stanfordnmbl/osim-rl"><img src="https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/images/ai-prosthetics.jpg" alt="AI for prosthetics" /></a></p>

    <p>Our objectives are to:</p>

    <ul>
      <li>bring Deep Reinforcement Learning to solve problems in medicine,</li>
      <li>promote open-source tools in RL research (the physics simulator <a href="http://opensim.stanford.edu/">OpenSim</a>, the RL environment, and the competition platform are all open-source),</li>
      <li>encourage RL research in computationally complex environments, with stochasticity and highly-dimensional action spaces.</li>
    </ul>

    <p>Visit our <a href="https://github.com/stanfordnmbl/osim-rl">github repo</a> to get started!</p>

    <h2 id="whats-new-compared-to-nips-2017-learning-to-run">What’s new compared to NIPS 2017: Learning to run?</h2>

    <p>We took into account comments from the last challenge and there are several changes:</p>

    <ul>
      <li><strong>You can use experimental data (to greatly speed up the learning process)</strong></li>
      <li>We released the 3rd dimensions <a href="http://opensim.stanford.edu/">OpenSim</a> model (the model can fall sideways)</li>
      <li>We added a prosthetic leg – the goal is to solve a medical challenge on modeling how walking will change after getting a prosthesis. Your work can speed up design, prototying, or tuning prosthetics!</li>
    </ul>

    <p>You haven’t heard of NIPS 2017: Learning to run? <a href="https://www.youtube.com/watch?v=rhNxt0VccsE">Watch this video!</a></p>
  evaluation_markdown: "Your task is to build a function f which takes the current
    state observation (a dictionary describing the current state) and returns the
    muscle excitations action (19-dimensional vector) maximizing the total reward.
    The objective is to follow the requested velocity vector. The trial ends either
    if the pelvis of the model falls below 0.6 meters or if you reach 1000 iterations
    (corresponding to 10 seconds in the virtual environment).\r\n\r\n[![Solution from
    2017](https://s3.amazonaws.com/osim-rl/videos/running.gif)](https://github.com/stanfordnmbl/osim-rl)\r\n\r\n###
    Round 1\r\nThe total reward is 9 * s - p * p where s is the number of steps before
    reaching one of the stop criteria and p is the absolute difference between horizonal
    velocity and 3. You can interpret it as a request to run at a constat speed of
    3 meters per second.\r\n\r\n### Round 2\r\nIn the second round the task is also
    to follow a requested velocity vector. However, in this round the vector will
    change in time and it will be a random process. We will provide the distribution
    of this process in mid-July.\r\n\r\n### Timeline\r\n\r\n* Round 1 : 16.06.2018
    - 21.10.2018\r\n* Round 2 : 21.10.2018 - 28.10.2018 (tentative)"
  evaluation: |
    <p>Your task is to build a function f which takes the current state observation (a dictionary describing the current state) and returns the muscle excitations action (19-dimensional vector) maximizing the total reward. The objective is to follow the requested velocity vector. The trial ends either if the pelvis of the model falls below 0.6 meters or if you reach 1000 iterations (corresponding to 10 seconds in the virtual environment).</p>

    <p><a href="https://github.com/stanfordnmbl/osim-rl"><img src="https://s3.amazonaws.com/osim-rl/videos/running.gif" alt="Solution from 2017" /></a></p>

    <h3 id="round-1">Round 1</h3>
    <p>The total reward is 9 * s - p * p where s is the number of steps before reaching one of the stop criteria and p is the absolute difference between horizonal velocity and 3. You can interpret it as a request to run at a constat speed of 3 meters per second.</p>

    <h3 id="round-2">Round 2</h3>
    <p>In the second round the task is also to follow a requested velocity vector. However, in this round the vector will change in time and it will be a random process. We will provide the distribution of this process in mid-July.</p>

    <h3 id="timeline">Timeline</h3>

    <ul>
      <li>Round 1 : 16.06.2018 - 21.10.2018</li>
      <li>Round 2 : 21.10.2018 - 28.10.2018 (tentative)</li>
    </ul>
  rules_markdown: "In order to avoid overfitting to the training environment, top
    participants will be asked to resubmit their solutions in the second round of
    the challenge. The final ranking will be based on results from the second round.\r\n\r\nAdditional
    rules:\r\n\r\n* Organizers reserve the right to modify challenge rules as required."
  rules: |
    <p>In order to avoid overfitting to the training environment, top participants will be asked to resubmit their solutions in the second round of the challenge. The final ranking will be based on results from the second round.</p>

    <p>Additional rules:</p>

    <ul>
      <li>Organizers reserve the right to modify challenge rules as required.</li>
    </ul>
  prizes_markdown: "We will provide the full list of prizes in mid-July.\r\nPrizes
    confirmed for now include:\r\n\r\n* 1st - 2x [NVIDIA Titan V](https://www.nvidia.com/en-us/titan/titan-v/){:target='_blank'}
    \r\n* 2nd - [NVIDIA Titan V](https://www.nvidia.com/en-us/titan/titan-v/){:target='_blank'}
    \r\n* 3rd - [NVIDIA Titan V](https://www.nvidia.com/en-us/titan/titan-v/){:target='_blank'}
    \r\n\r\nAdditionally:\r\n\r\n* Invitation to publish articles in the NIPS competition
    book.\r\n* Invitation to the 3rd [Applied Machine Learning Days](https://www.appliedmldays.org)
    at EPFL in Switzerland on January 26 - 29, 2019, with travel and accommodation
    covered.\r\n* Invitation to give a research talk at Stanford, with travel and
    accommodation covered.\r\n* Reimbursement of travel and accommodation at NIPS
    2018"
  prizes: |
    <p>We will provide the full list of prizes in mid-July.
    Prizes confirmed for now include:</p>

    <ul>
      <li>1st - 2x <a href="https://www.nvidia.com/en-us/titan/titan-v/" target="_blank">NVIDIA Titan V</a></li>
      <li>2nd - <a href="https://www.nvidia.com/en-us/titan/titan-v/" target="_blank">NVIDIA Titan V</a></li>
      <li>3rd - <a href="https://www.nvidia.com/en-us/titan/titan-v/" target="_blank">NVIDIA Titan V</a></li>
    </ul>

    <p>Additionally:</p>

    <ul>
      <li>Invitation to publish articles in the NIPS competition book.</li>
      <li>Invitation to the 3rd <a href="https://www.appliedmldays.org">Applied Machine Learning Days</a> at EPFL in Switzerland on January 26 - 29, 2019, with travel and accommodation covered.</li>
      <li>Invitation to give a research talk at Stanford, with travel and accommodation covered.</li>
      <li>Reimbursement of travel and accommodation at NIPS 2018</li>
    </ul>
  resources_markdown: "Please visit [osim-rl project's website](http://osim-rl.stanford.edu)
    for resources on biomechanics and reinforcement learning, solutions from the NIPS
    2017 and other materials. \r\nVisit [OpenSim](http://opensim.stanford.edu/) website
    for materials on musculoskeletal simulations.\r\n\r\nHere are some interesting
    blog posts written by participants:\r\n\r\n* [Understanding the Challenge](https://www.endtoend.ai/blog/ai-for-prosthetics-1)\r\n*
    [Understanding the Action Space](https://www.endtoend.ai/blog/ai-for-prosthetics-2)\r\n*
    [Understanding the Observation Space](https://www.endtoend.ai/blog/ai-for-prosthetics-3)\r\n*
    [Understanding the Reward](https://www.endtoend.ai/blog/ai-for-prosthetics-5)\r\n\r\nHere
    are some helper libraries written by participants:\r\n\r\n* [https://github.com/seungjaeryanlee/osim-rl-helper](https://github.com/seungjaeryanlee/osim-rl-helper)\r\n\r\nHere
    are some articles and blog posts written by participants about NIPS 2017: Learning
    to Run Challenge (can very helpful for this challenge as well):\r\n\r\n* [https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5](https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5)\r\n*
    [https://arxiv.org/abs/1711.06922](https://arxiv.org/abs/1711.06922)\r\n* [https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8](https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8)\r\n\r\n##
    Contact Us\r\n\r\nUse one of the public channels:\r\n\r\n* Gitter Channel : [crowdAI/NIPS-Learning-To-Run-Challenge](https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge)\r\n*
    Technical issues : [https://github.com/stanfordnmbl/osim-rl/issues ](https://github.com/stanfordnmbl/osim-rl/issues){:target='_blank'}\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics](https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organisers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  [lukasz.kidzinski@stanford.edu](mailto:lukasz.kidzinski@stanford.edu){:target='_blank'}\r\n*
    \ [sharada.mohanty@epfl.ch](mailto:sharada.mohanty@epfl.ch){:target='_blank'}\r\n\r\n##
    Media\r\n\r\n[![TechCrunch](https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png){:class='img-logo'}](https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/)\r\n[![Stanford
    News](https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW){:class='img-logo'}](http://med.stanford.edu/news/all-news/2018/07/virtual-athletes-compete-to-take-on-a-medical-challenge.html)\r\n[![IEEE](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png){:class='img-logo'}](http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy)"
  resources: "<p>Please visit <a href=\"http://osim-rl.stanford.edu\">osim-rl project’s
    website</a> for resources on biomechanics and reinforcement learning, solutions
    from the NIPS 2017 and other materials. \nVisit <a href=\"http://opensim.stanford.edu/\">OpenSim</a>
    website for materials on musculoskeletal simulations.</p>\n\n<p>Here are some
    interesting blog posts written by participants:</p>\n\n<ul>\n  <li><a href=\"https://www.endtoend.ai/blog/ai-for-prosthetics-1\">Understanding
    the Challenge</a></li>\n  <li><a href=\"https://www.endtoend.ai/blog/ai-for-prosthetics-2\">Understanding
    the Action Space</a></li>\n  <li><a href=\"https://www.endtoend.ai/blog/ai-for-prosthetics-3\">Understanding
    the Observation Space</a></li>\n  <li><a href=\"https://www.endtoend.ai/blog/ai-for-prosthetics-5\">Understanding
    the Reward</a></li>\n</ul>\n\n<p>Here are some helper libraries written by participants:</p>\n\n<ul>\n
    \ <li><a href=\"https://github.com/seungjaeryanlee/osim-rl-helper\">https://github.com/seungjaeryanlee/osim-rl-helper</a></li>\n</ul>\n\n<p>Here
    are some articles and blog posts written by participants about NIPS 2017: Learning
    to Run Challenge (can very helpful for this challenge as well):</p>\n\n<ul>\n
    \ <li><a href=\"https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5\">https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5</a></li>\n
    \ <li><a href=\"https://arxiv.org/abs/1711.06922\">https://arxiv.org/abs/1711.06922</a></li>\n
    \ <li><a href=\"https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8\">https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8</a></li>\n</ul>\n\n<h2
    id=\"contact-us\">Contact Us</h2>\n\n<p>Use one of the public channels:</p>\n\n<ul>\n
    \ <li>Gitter Channel : <a href=\"https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge\">crowdAI/NIPS-Learning-To-Run-Challenge</a></li>\n
    \ <li>Technical issues : <a href=\"https://github.com/stanfordnmbl/osim-rl/issues\"
    target=\"_blank\">https://github.com/stanfordnmbl/osim-rl/issues </a></li>\n  <li>Discussion
    Forum : <a href=\"https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics\">https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics</a></li>\n</ul>\n\n<p>We
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organisers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :</p>\n\n<ul>\n  <li><a href=\"mailto:lukasz.kidzinski@stanford.edu\"
    target=\"_blank\">lukasz.kidzinski@stanford.edu</a></li>\n  <li><a href=\"mailto:sharada.mohanty@epfl.ch\"
    target=\"_blank\">sharada.mohanty@epfl.ch</a></li>\n</ul>\n\n<h2 id=\"media\">Media</h2>\n\n<p><a
    href=\"https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/\"><img
    src=\"https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png\"
    alt=\"TechCrunch\" class=\"img-logo\" /></a>\n<a href=\"http://med.stanford.edu/news/all-news/2018/07/virtual-athletes-compete-to-take-on-a-medical-challenge.html\"><img
    src=\"https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW\"
    alt=\"Stanford News\" class=\"img-logo\" /></a>\n<a href=\"http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy\"><img
    src=\"https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png\"
    alt=\"IEEE\" class=\"img-logo\" /></a></p>\n"
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: OpenSim-1.jpg
  featured_sequence: 10
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: ''
  winner_description: "\n"
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_37:
  id: 37
  organizer_id: 21
  challenge: Train Schedule Optimisation Challenge
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &6 2018-05-25 13:20:30.792116000 Z
    zone: *2
    time: *6
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &7 2018-10-04 10:27:03.630163000 Z
    zone: *2
    time: *7
  tagline: 'Optimizing train schedules '
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: not_used
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 17567
  participant_count: 333
  submission_count: 90
  score_title: Score
  score_secondary_title: Secondary Score
  slug: train-schedule-optimisation-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: sbb_train_schedule_optimisation_challenge_2018
  online_grading: true
  vote_count: 68
  description_markdown: "SBB Swiss Federal Railways manages one of the most densely-used
    mixed-traffic railway networks in the world. Every day we transport over 1.2 million
    people and carry 210'000 ton-kilometers of freight on more than 10'000 trains,
    all while achieving world-leading punctuality metrics.\r\n\r\nWhile average utilisation
    of our infrastructure (measured in average number of trains per kilometer of track)
    is already second-to-none, traffic is expected to increase even further. There
    are also financial incentives to use the available infrastructure to its maximum.
    However, before trains can run, they must first be accepted into the timetable.\r\n\r\nGenerating
    a railway timetable is known to be an NP-hard problem. While scheduling few trains
    is easy, complications quickly explode with increasing number of trains. The most
    important factors contributing to complexity are interdepencies between trains
    (such as connections) and the inability of trains to overtake one another on the
    same track.\r\n\r\nOur goal with this challenge is to solicit ingenious ways to
    tackle the timetable generation/optimization problem. Do you see a suitable algorithm?
    A promising AI-approach? A powerful heuristic? We can't wait to see it in action!\r\n\r\nWe
    provide you with a set of sample problem instances consisting of a list of trains
    to be scheduled, their commercial requirements to be respected and a set of routes
    they can take through the network. Your challenge is to come up with a timetable
    for these problem instances.\r\n\r\n![dark-evening-light-trails-434415.jpg](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1f9a8f0b83d464ebc5933ac806ca0c7e_dark-evening-light-trails-434415.jpg)\r\n"
  description: |
    <p>SBB Swiss Federal Railways manages one of the most densely-used mixed-traffic railway networks in the world. Every day we transport over 1.2 million people and carry 210’000 ton-kilometers of freight on more than 10’000 trains, all while achieving world-leading punctuality metrics.</p>

    <p>While average utilisation of our infrastructure (measured in average number of trains per kilometer of track) is already second-to-none, traffic is expected to increase even further. There are also financial incentives to use the available infrastructure to its maximum. However, before trains can run, they must first be accepted into the timetable.</p>

    <p>Generating a railway timetable is known to be an NP-hard problem. While scheduling few trains is easy, complications quickly explode with increasing number of trains. The most important factors contributing to complexity are interdepencies between trains (such as connections) and the inability of trains to overtake one another on the same track.</p>

    <p>Our goal with this challenge is to solicit ingenious ways to tackle the timetable generation/optimization problem. Do you see a suitable algorithm? A promising AI-approach? A powerful heuristic? We can’t wait to see it in action!</p>

    <p>We provide you with a set of sample problem instances consisting of a list of trains to be scheduled, their commercial requirements to be respected and a set of routes they can take through the network. Your challenge is to come up with a timetable for these problem instances.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1f9a8f0b83d464ebc5933ac806ca0c7e_dark-evening-light-trails-434415.jpg" alt="dark-evening-light-trails-434415.jpg" /></p>
  evaluation_markdown: "Each train in a problem instance will define a latest desired
    arrival times at its stops. It will also define its importance, relative to other
    trains.\r\n\r\nThe evaluation criterion for each individual solution will be the
    weighted sum of all delays. The delay is zero if all trains are scheduled such
    that they arrive at each of their stops no later than desired. There is also the
    possibility that some routes for a train are more desirable than others, in which
    case a solution incurs additional \"routing penalty\" if an undesired route is
    chosen.\r\n\r\nThe overall evaluation criterion will be the sum of each individual
    problem instance. Missing or invalid solutions are penalized with a large constant,
    so that it is generally better to find valid solutions to as many instances as
    possible, even if these solutions have large delays/routing penalties.\r\n\r\nIf
    you would like to know the precise formulation, head over to our [Starter Kit](https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit),
    in particular the formal [definition of the objective function](https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/documentation/business_rules.md#objective-function).
    Be advised that that definition is rather technical. It is important to understand
    the meaning behind it."
  evaluation: |
    <p>Each train in a problem instance will define a latest desired arrival times at its stops. It will also define its importance, relative to other trains.</p>

    <p>The evaluation criterion for each individual solution will be the weighted sum of all delays. The delay is zero if all trains are scheduled such that they arrive at each of their stops no later than desired. There is also the possibility that some routes for a train are more desirable than others, in which case a solution incurs additional “routing penalty” if an undesired route is chosen.</p>

    <p>The overall evaluation criterion will be the sum of each individual problem instance. Missing or invalid solutions are penalized with a large constant, so that it is generally better to find valid solutions to as many instances as possible, even if these solutions have large delays/routing penalties.</p>

    <p>If you would like to know the precise formulation, head over to our <a href="https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit">Starter Kit</a>, in particular the formal <a href="https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/documentation/business_rules.md#objective-function">definition of the objective function</a>. Be advised that that definition is rather technical. It is important to understand the meaning behind it.</p>
  rules_markdown: "The following rules have to be observed by all participants:\r\n\r\n*
    participants are allowed at most 5 submissions per day\r\n* when evaluation individual
    solutions directly via the [REST-API](https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/utils/validate_solution.ipynb),
    a limit of one REST-call per minute shall be observed\r\n* In order to be eligible
    for prizes\r\n     - a participant must release their solution unter the [ MIT
    License ](https://opensource.org/licenses/MIT){:target='_blank'}. We encourage
    all participants to open-source their code!\r\n     - the solutions must be found
    by the participants' solver in no more than 15 minutes per problem instance. The
    resource limits are: 32 CPU cores, 256 GB of RAM. You may use GPUs if desired.
    If you want to use more exotic hardware, please get in touch with us and we will
    let you know if your desired setup would still qualify for the awards. <br>Before
    awarding prizes, the organizers reserve the right to verify these limits by running
    your solver on the crowdAI cluster\r\n     - The results achieved by the solver
    must be reproducible. If there are randomized portions of your approach, be sure
    to include seeds to make the runs repeatable.\r\n* Sample solutions by Admins
    and Organizers can serve as baselines/benchmarks. However, your solutions must
    not be based on those. Your solutions must be produced from the problem instances
    by _your code_.\r\n* It is in principle allowed to use commercial solving engines
    (such as e.g. CPLEX/Gurobi for a Mixed Integer Approach) to attack the problem.
    However, please contact us if you intend to do that, so we can confirm that the
    product is accepted for this challenge.\r\n* In case of conflicts, the decision
    of the Organizers will be final and binding.\r\n* Organizers reserve the right
    to make changes to the rules and timeline.\r\n* Violation of the rules or other
    unfair activity may result in disqualification."
  rules: |
    <p>The following rules have to be observed by all participants:</p>

    <ul>
      <li>participants are allowed at most 5 submissions per day</li>
      <li>when evaluation individual solutions directly via the <a href="https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/utils/validate_solution.ipynb">REST-API</a>, a limit of one REST-call per minute shall be observed</li>
      <li>In order to be eligible for prizes
        <ul>
          <li>a participant must release their solution unter the <a href="https://opensource.org/licenses/MIT" target="_blank"> MIT License </a>. We encourage all participants to open-source their code!</li>
          <li>the solutions must be found by the participants’ solver in no more than 15 minutes per problem instance. The resource limits are: 32 CPU cores, 256 GB of RAM. You may use GPUs if desired. If you want to use more exotic hardware, please get in touch with us and we will let you know if your desired setup would still qualify for the awards. <br />Before awarding prizes, the organizers reserve the right to verify these limits by running your solver on the crowdAI cluster</li>
          <li>The results achieved by the solver must be reproducible. If there are randomized portions of your approach, be sure to include seeds to make the runs repeatable.</li>
        </ul>
      </li>
      <li>Sample solutions by Admins and Organizers can serve as baselines/benchmarks. However, your solutions must not be based on those. Your solutions must be produced from the problem instances by <em>your code</em>.</li>
      <li>It is in principle allowed to use commercial solving engines (such as e.g. CPLEX/Gurobi for a Mixed Integer Approach) to attack the problem. However, please contact us if you intend to do that, so we can confirm that the product is accepted for this challenge.</li>
      <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
      <li>Organizers reserve the right to make changes to the rules and timeline.</li>
      <li>Violation of the rules or other unfair activity may result in disqualification.</li>
    </ul>
  prizes_markdown: "The top three submissions will be awarded the following cash prizes
    (in Swiss Francs):\r\n\r\n* **CHF 7'000.- for first prize**\r\n* **CHF 5'000.-
    for second prize**\r\n* **CHF 3'500.- for third prize**\r\n\r\nIn addition, we
    allow for the possibility of awarding several travel grants to the [Applied Machine
    Learning Days 2019](https://www.appliedmldays.org/) in Lausanne, Switzerland.
    Participants with promising solutions may be invited to present their solutions
    in person.\r\n\r\nNote that the travel grants are not automatically awarded to
    the top-rated submissions. It may be possible that a submission does not score
    very highly because, for example, only half of the problem instances could be
    solved. However, the approach used demonstrates an original and promising idea
    that SBB would like to expand upon. In this case, SBB would reserve the right
    to award a travel grant to such a submission.\r\n\r\nIn case more than one submission
    receives the same score, the tie will be broken as follows\r\n1. preference to
    the submission which requires less total computation time\r\n2. preference to
    the submission which requires less computing resources\r\n3. if neither of these
    can be adequately measured or distinguished, the organizers break the ties or
    decide to award joint prizes.\r\n\r\n\r\n![train.png](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/3f97ddda4fe1141ff8eb79d883aa3fb5_train.png)"
  prizes: |
    <p>The top three submissions will be awarded the following cash prizes (in Swiss Francs):</p>

    <ul>
      <li><strong>CHF 7’000.- for first prize</strong></li>
      <li><strong>CHF 5’000.- for second prize</strong></li>
      <li><strong>CHF 3’500.- for third prize</strong></li>
    </ul>

    <p>In addition, we allow for the possibility of awarding several travel grants to the <a href="https://www.appliedmldays.org/">Applied Machine Learning Days 2019</a> in Lausanne, Switzerland. Participants with promising solutions may be invited to present their solutions in person.</p>

    <p>Note that the travel grants are not automatically awarded to the top-rated submissions. It may be possible that a submission does not score very highly because, for example, only half of the problem instances could be solved. However, the approach used demonstrates an original and promising idea that SBB would like to expand upon. In this case, SBB would reserve the right to award a travel grant to such a submission.</p>

    <p>In case more than one submission receives the same score, the tie will be broken as follows
    1. preference to the submission which requires less total computation time
    2. preference to the submission which requires less computing resources
    3. if neither of these can be adequately measured or distinguished, the organizers break the ties or decide to award joint prizes.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/3f97ddda4fe1141ff8eb79d883aa3fb5_train.png" alt="train.png" /></p>
  resources_markdown: "This Challenge is a little bit special in that to provide solutions,
    you need to understand a few things about the data format of the problem instances
    and the solutions, as well as the rules that a solution must adhere to.\r\n\r\nWe
    have put together a Starter Kit that contains the necessary documentation in,
    hopefully, easily understandable form, works through some step-by-step examples
    and provides some utility scripts that help you get startet. Please head over
    to the [Starter Kit](https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/#starter-kit-repo-for-the-sbb-train-schedule-optimisation-challenge-on-crowdai),
    where the README should guide you through the content available there.\r\n\r\nIn
    case of questions about the material, please do not hesitate to contact us.\r\n\r\nHere
    are some interesting blog posts:\r\n\r\n* [Can You Make Swiss Trains Even More
    Punctual?](https://medium.com/crowdai/can-you-make-swiss-trains-even-more-punctual-ec9aa73d6e35)\r\n\r\n##
    Contact Us\r\n\r\n### For Challenge-related questions (technical and/or content
    questions)\r\n\r\n* Gitter Channel : [crowdAI/sbb-challenges](https://gitter.im/crowdAI/sbb-challenges)
    \  \r\n* Technical Issues : [https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues](https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues)
    \  \r\n* Discussion Forum : [https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics](https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n* [julian.jordi@sbb.ch](mailto:julian.jordi@sbb.ch){:target='_blank'}\r\n*
    [ sharada.mohanty@epfl.ch\r\n ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}\r\n\r\n###
    For Press inquiries\r\nPlease contact SBB Media Relations at [press@sbb.ch](mailto:press@sbb.ch)"
  resources: |
    <p>This Challenge is a little bit special in that to provide solutions, you need to understand a few things about the data format of the problem instances and the solutions, as well as the rules that a solution must adhere to.</p>

    <p>We have put together a Starter Kit that contains the necessary documentation in, hopefully, easily understandable form, works through some step-by-step examples and provides some utility scripts that help you get startet. Please head over to the <a href="https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/#starter-kit-repo-for-the-sbb-train-schedule-optimisation-challenge-on-crowdai">Starter Kit</a>, where the README should guide you through the content available there.</p>

    <p>In case of questions about the material, please do not hesitate to contact us.</p>

    <p>Here are some interesting blog posts:</p>

    <ul>
      <li><a href="https://medium.com/crowdai/can-you-make-swiss-trains-even-more-punctual-ec9aa73d6e35">Can You Make Swiss Trains Even More Punctual?</a></li>
    </ul>

    <h2 id="contact-us">Contact Us</h2>

    <h3 id="for-challenge-related-questions-technical-andor-content-questions">For Challenge-related questions (technical and/or content questions)</h3>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/sbb-challenges">crowdAI/sbb-challenges</a></li>
      <li>Technical Issues : <a href="https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues">https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics">https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:julian.jordi@sbb.ch" target="_blank">julian.jordi@sbb.ch</a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>

    <h3 id="for-press-inquiries">For Press inquiries</h3>
    <p>Please contact SBB Media Relations at <a href="mailto:press@sbb.ch">press@sbb.ch</a></p>
  submission_instructions_markdown: "See [here](https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/documentation/what_to_submit.md)
    for submission instructions and a sample submission file.\r\n"
  submission_instructions: '<p>See <a href="https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/documentation/what_to_submit.md">here</a>
    for submission instructions and a sample submission file.</p>

'
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: 'You may use the data, specifically the problem instances,
    provided for this Challenge only in the context of this Challenge. '
  dataset_description: "<p>You may use the data, specifically the problem instances,
    provided for this Challenge only in the context of this Challenge.</p>\n"
  image_file: sbb_challenge.png
  featured_sequence: 10
  dynamic_content_flag: false
  dynamic_content: ''
  dynamic_content_tab: ''
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: crowdAI_GRADER_POOL
  online_submissions: true
  grader_logs: true
  require_registration: false
  grading_history: true
  post_challenge_submissions: false
  submissions_downloadable: true
  dataset_note_markdown: 
  dataset_note: 
challenge_28:
  id: 28
  organizer_id: 15
  challenge: 'NIPS 2018 : Adversarial Vision Challenge (Robust Model Track)'
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &8 2018-03-12 12:48:47.524358000 Z
    zone: *2
    time: *8
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &9 2018-10-04 11:05:59.700019000 Z
    zone: *2
    time: *9
  tagline: Pitting machine vision models against adversarial attacks.
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 33656
  participant_count: 284
  submission_count: 979
  score_title: Median L2 Distance
  score_secondary_title: Mean L2 Distance
  slug: nips-2018-adversarial-vision-challenge-robust-model-track
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: nips-2018-avc-robust-model-track
  online_grading: true
  vote_count: 140
  description_markdown: "Welcome to the _Adversarial Vision Challenge_, one of the
    official challenges in the [ NIPS 2018 ](https://nips.cc){:target='_blank'} competition
    track. In this competition you can take on the role of an attacker or a defender
    (or both). As a defender you are trying to build a visual object classifier that
    is as robust to image perturbations as possible. As an attacker, your task is
    to find the smallest possible image perturbations that will fool a classifier.\r\n\r\nThe
    overall goal of this challenge is to facilitate measurable progress towards robust
    machine vision models and more generally applicable adversarial attacks. As of
    right now, modern machine vision algorithms are extremely susceptible to small
    and almost imperceptible perturbations of their inputs (so-called _adversarial
    examples_). This property reveals an astonishing difference in the information
    processing of humans and machines and raises security concerns for many deployed
    machine vision systems like autonomous cars. Improving the robustness of vision
    algorithms is thus important to close the gap between human and machine perception
    and to enable safety-critical applications.\r\n\r\n<img src=\"https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png\"
    alt=\"Illustration Adversarial Examples\" style=\"width: 800px\" />\r\n\r\n##
    Competition tracks\r\n\r\nThere will be three tracks in which you and your team
    can compete: \r\n\r\n* [Robust Model Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track){:target='_blank'}\r\n*
    [Untargeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track){:target='_blank'}\r\n*
    [Targeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/){:target='_blank'}\r\n\r\n\r\nIn
    this track your task is to build and train a robust model on [tiny ImageNet](https://tiny-imagenet.herokuapp.com/){:target='_blank'}.
    The attacks will try to find small image perturbations that change the prediction
    of your model to the wrong class. The larger these perturbations are the better
    is your score (see below).\r\n\r\n## Evaluation criterion\r\n\r\nModels are scored
    as follows (higher is better):\r\n\r\n* Let M be the model and S be the set of
    samples.\r\n* We apply the five best untargeted attacks on M for each sample in
    S.\r\n* For each sample we record the minimum adversarial L2 distance (MAD) across
    the attacks.\r\n* If a model misclassifies a sample then the minimum adversarial
    distance is registered as zero for this sample.\r\n* The final model score is
    the median MAD across all samples.\r\n* The higher the score, the better.\r\n\r\nThe
    top-5 attacks against which submissions are evaluated are fixed for two weeks
    at a time after which we evaluate all current submissions to determine the new
    top-5 attacks for the upcoming two weeks.\r\n\r\n## Timeline\r\n\r\n_(tentative)_.
    \ \r\n* **June 25th, 2018**  : Challenge begins     \r\n* **November 1st** : Final
    submission date   \r\n* **November 15th** : Winners Announced     \r\n\r\n## Submissions\r\n\r\nTo
    make a submission, please follow the instructions in this GitLab repository:\r\n[
    https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template
    ](https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template){:target='_blank'}\r\n\r\nFork
    the above **template repository** in GitLab and follow the instructions stated
    in the README.md.\r\nYou need to have a crowdAI-account and sign in to GitLab
    using this account.\r\nIn the README you will also find links to multiple fully
    functional examples.\r\n\r\n\r\n## Organizing Team\r\n\r\nThe organizing team
    comes from multiple groups — [University of Tübingen](https://www.uni-tuebingen.de/en/university.html),
    [Google Brain](https://research.google.com/teams/brain/), [EPFL](https://www.epfl.ch/index.en.html)
    and [Pennsylvania State University](http://www.psu.edu/).\r\n\r\nThe Team consists
    of:    \r\n*  [ Wieland Brendel ](https://twitter.com/wielandbr){:target='_blank'}
    \  \r\n*  [ Jonas Rauber ](https://jonasrauber.de){:target='_blank'}   \r\n*  [
    Alexey Kurakin ](https://twitter.com/alexey2004){:target='_blank'}   \r\n*  [
    Nicolas Papernot ](https://twitter.com/NicolasPapernot){:target='_blank'}   \r\n*
    \ [ Behar Veliqi ](https://twitter.com/beveliqi){:target='_blank'}   \r\n*  [
    Sharada P. Mohanty ](https://twitter.com/MeMohanty){:target='_blank'}    \r\n*
    \ [ Marcel Salathé ](https://twitter.com/marcelsalathe){:target='_blank'}   \r\n*
    \ [ Matthias Bethge ](https://twitter.com/MatthiasBethge){:target='_blank'}   \r\n\r\n##
    Sponsors\r\n\r\n| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png\"
    alt=\"Amazon AWS\" style=\"width: 120px\" />     | <img src=\"https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png\"
    alt=\"Paperspace\"  style=\"width: 240px\"/>  |\r\n"
  description: |
    <p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href="https://nips.cc" target="_blank"> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

    <p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png" alt="Illustration Adversarial Examples" style="width: 800px" /></p>

    <h2 id="competition-tracks">Competition tracks</h2>

    <p>There will be three tracks in which you and your team can compete:</p>

    <ul>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track" target="_blank">Robust Model Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track" target="_blank">Untargeted Attacks Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/" target="_blank">Targeted Attacks Track</a></li>
    </ul>

    <p>In this track your task is to build and train a robust model on <a href="https://tiny-imagenet.herokuapp.com/" target="_blank">tiny ImageNet</a>. The attacks will try to find small image perturbations that change the prediction of your model to the wrong class. The larger these perturbations are the better is your score (see below).</p>

    <h2 id="evaluation-criterion">Evaluation criterion</h2>

    <p>Models are scored as follows (higher is better):</p>

    <ul>
      <li>Let M be the model and S be the set of samples.</li>
      <li>We apply the five best untargeted attacks on M for each sample in S.</li>
      <li>For each sample we record the minimum adversarial L2 distance (MAD) across the attacks.</li>
      <li>If a model misclassifies a sample then the minimum adversarial distance is registered as zero for this sample.</li>
      <li>The final model score is the median MAD across all samples.</li>
      <li>The higher the score, the better.</li>
    </ul>

    <p>The top-5 attacks against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 attacks for the upcoming two weeks.</p>

    <h2 id="timeline">Timeline</h2>

    <p><em>(tentative)</em>.<br />
    * <strong>June 25th, 2018</strong>  : Challenge begins   <br />
    * <strong>November 1st</strong> : Final submission date <br />
    * <strong>November 15th</strong> : Winners Announced</p>

    <h2 id="submissions">Submissions</h2>

    <p>To make a submission, please follow the instructions in this GitLab repository:
    <a href="https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template" target="_blank"> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template </a></p>

    <p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
    You need to have a crowdAI-account and sign in to GitLab using this account.
    In the README you will also find links to multiple fully functional examples.</p>

    <h2 id="organizing-team">Organizing Team</h2>

    <p>The organizing team comes from multiple groups — <a href="https://www.uni-tuebingen.de/en/university.html">University of Tübingen</a>, <a href="https://research.google.com/teams/brain/">Google Brain</a>, <a href="https://www.epfl.ch/index.en.html">EPFL</a> and <a href="http://www.psu.edu/">Pennsylvania State University</a>.</p>

    <p>The Team consists of:  <br />
    *  <a href="https://twitter.com/wielandbr" target="_blank"> Wieland Brendel </a> <br />
    *  <a href="https://jonasrauber.de" target="_blank"> Jonas Rauber </a> <br />
    *  <a href="https://twitter.com/alexey2004" target="_blank"> Alexey Kurakin </a> <br />
    *  <a href="https://twitter.com/NicolasPapernot" target="_blank"> Nicolas Papernot </a> <br />
    *  <a href="https://twitter.com/beveliqi" target="_blank"> Behar Veliqi </a> <br />
    *  <a href="https://twitter.com/MeMohanty" target="_blank"> Sharada P. Mohanty </a>  <br />
    *  <a href="https://twitter.com/marcelsalathe" target="_blank"> Marcel Salathé </a> <br />
    *  <a href="https://twitter.com/MatthiasBethge" target="_blank"> Matthias Bethge </a></p>

    <h2 id="sponsors">Sponsors</h2>

    <table>
      <tbody>
        <tr>
          <td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png" alt="Amazon AWS" style="width: 120px" /></td>
          <td><img src="https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png" alt="Paperspace" style="width: 240px" /></td>
        </tr>
      </tbody>
    </table>
  evaluation_markdown: ''
  evaluation: "\n"
  rules_markdown: "* Betghelab and Google Brain employees can participate but are
    ineligible for prizes\r\n* participants are required to release the code of their
    submissions as open source to be eligible for the final scoring\r\n* any legitimate
    input that is not classified by a model will be counted as an adversarial\r\n*
    if an attack fails to produce an adversarial, we will register a worst-case adversarial
    instead\r\n* all classifiers must be stateless and act on one image at a time\r\n*
    the decision of each classifier must be deterministic\r\n* attacks are allowed
    to query the model on self-defined inputs up to 1.000 times / sample\r\n* each
    model has to process one image within 40ms on a K80 GPU (excluding initialization
    and setup which may take up to 100s)\r\n* each attack has to process a batch of
    10 images within 900s on a K80 GPU"
  rules: |
    <ul>
      <li>Betghelab and Google Brain employees can participate but are ineligible for prizes</li>
      <li>participants are required to release the code of their submissions as open source to be eligible for the final scoring</li>
      <li>any legitimate input that is not classified by a model will be counted as an adversarial</li>
      <li>if an attack fails to produce an adversarial, we will register a worst-case adversarial instead</li>
      <li>all classifiers must be stateless and act on one image at a time</li>
      <li>the decision of each classifier must be deterministic</li>
      <li>attacks are allowed to query the model on self-defined inputs up to 1.000 times / sample</li>
      <li>each model has to process one image within 40ms on a K80 GPU (excluding initialization and setup which may take up to 100s)</li>
      <li>each attack has to process a batch of 10 images within 900s on a K80 GPU</li>
    </ul>
  prizes_markdown: "* **$15.000 worth of Paperspace cloud compute credits**: The top-20
    teams in each track (defense, untargeted attack, targeted attack) as of 28. September
    will receive 250$ each. "
  prizes: |
    <ul>
      <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
    </ul>
  resources_markdown: "## Contact Us   \r\n   \r\n* Gitter Channel : [crowdAI/nips-2018-adversarial-vision-challenge](https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics)\r\n
    \  \r\nWe strongly encourage you to use the public channels mentioned above for
    communications between the participants and the organizers. In extreme cases,
    if there are any queries or comments that you would like to make using a private
    communication channel, then you can send us an email at :\r\n\r\n*  [ wieland.brendel@bethgelab.org\r\n
    ](mailto:wieland.brendel@bethgelab.org){:target='_blank'}\r\n* [behar.veliqi@bethgelab.org](mailto:behar.veliqi@bethgelab.org){:target='_blank'}\r\n*
    \ [ sharada.mohanty@epfl.ch\r\n ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:wieland.brendel@bethgelab.org" target="_blank"> wieland.brendel@bethgelab.org
     </a></li>
      <li><a href="mailto:behar.veliqi@bethgelab.org" target="_blank">behar.veliqi@bethgelab.org</a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: challenge_logo_model.png
  featured_sequence: 9
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: nips-2018-avc-robust-model-track
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_33:
  id: 33
  organizer_id: 20
  challenge: 'MARLO 2018 '
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &10 2018-04-18 15:18:26.619504000 Z
    zone: *2
    time: *10
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &11 2018-10-04 10:51:00.712180000 Z
    zone: *2
    time: *11
  tagline: Multi-Agent Reinforcement Learning in Minecraft
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 29944
  participant_count: 91
  submission_count: 35
  score_title: Total Reward
  score_secondary_title: Mean Reward
  slug: marlo-2018
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: crowdai-marLo-2018
  online_grading: true
  vote_count: 89
  description_markdown: "> We are accepting submissions for a small warmup round for
    you to get acquainted with the submission system. Please feel free to make a submission
    using the [instructions here](https://github.com/crowdAI/marlo-single-agent-starter-kit/){:target='_blank'}.\r\n\r\n>
    **Due to the delay of opening submission system, we extended the Entry Period
    to November 12. Please take a look at the part of Winner Selection for detail.**\r\n\r\n##
    What is the Challenge?\r\n\r\nLearning to Play: The Multi-Agent Reinforcement
    Learning in MalmO Competition (“Challenge”) is a new challenge that proposes research
    on Multi-Agent Reinforcement Learning using multiple games. Participants would
    create learning agents that will be able to play multiple 3D games as deﬁned in
    the MalmO platform. The aim of the competition is to encourage AI research on
    more general approaches via multi-player games. For this, the Challenge will consist
    of not one but several games, each one of them with several tasks of varying difficulty
    and settings. Some of these tasks will be public and participants will be able
    to train on them. Others, however, will be private, only used to determine the
    ﬁnal rankings of the competition.\r\n\r\nOrganizer (Microsoft, Queen Mary University
    of London and crowdAI) will make the Challenge tasks and sample code available
    via GitHub on or before the Challenge start date. Entries will be judged by Organizer
    according to the criteria outlined in the “Winner Selection” section below.\r\n\r\nThe
    following prizes will be awarded: (1) the top 7 team will be awarded a MARLO Travel
    Grant with a maximum value of `$2,500` USD to join a relevant academic conference
    or workshop, additionally, the 1st winning team will be awarded a second MARLO
    Travel Grant with a maximum value of `$2,500` USD to join the [Applied Machine
    Learning Days 2019](https://www.appliedmldays.org/){:target='_blank'} and (2)
    three winning teams will be awarded Microsoft Azure Sponsorship with a maximum
    value of `$10,000` USD for the 1st place, `$5,000` USD for the 2nd place and `$3,000`
    USD for the 3rd place. Please see the “Challenge Prizes” section below for further
    details.\r\n\r\n## What are the start and end dates?\r\n\r\nThe Challenge starts
    at 00:01 Pacific Standard Time, on July 27, 2018, and the Qualify Round ends at
    23:59 Pacific Standard Time, on Nov 12, 2018 (“Entry Period”). Entries must be
    received within the Entry Period to be eligible.\r\n\r\nThe final tournament is
    held in [MARLO workshop](https://marlo-ai.github.io/){:target='_blank'} of AIIDE’18,
    the 14th [AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment](https://sites.google.com/ncsu.edu/aiide-2018/){:target='_blank'}
    held at the University of Alberta in Edmonton, AB, Canada on November 14, 2018.\r\n\r\n##
    Games and Tasks\r\n\r\nOne of the main features of this competition is that agents
    play in multiple games. Therefore, several tasks are proposed for this contest.
    For the purpose of this document and the competition itself, we deﬁne:\r\n\r\n*
    Game: each one of the diﬀerent scenarios in which agents play. \r\n\r\n* Task:
    each instance of a game. Tasks, within a game, may be diﬀerent to each other in
    terms of level layout, size, diﬃculty and other game-dependent settings.\r\n\r\n![Figure1.png](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ce3722de6fc358b980aab61893b6ffa0_Figure1.png)\r\nFigure
    1: Sketch of how games and tasks are organized in the Challenge.\r\n\r\nAs can
    be seen, tasks will be of public nature and accessible by the participants, while
    others are secret and will be used to evaluate the submitted entries at the end
    of the competition.\r\n\r\nTasks are distributed across sets:\r\n![Figure2.png](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1cf1be0ffe8090ed993adfcb63580163_Figure2.png)\r\nFigure
    2: On the left: Build Battle, where players need to recreate a structure (in this
    case, the structure is shown on the ground). On the right: Pig Chase, where players
    collaborate to corner the pig.\r\n  \r\n## Competition\r\n\r\nTo participate in
    the Challenge, you will first need to register as a user on crowdai.org and entry
    the Challenge. Then you start playing with the Challenge by simply cloning the
    [starter-kit of the Challenge on GitHub](https://github.com/crowdAI/marLo){:target='_blank'}.
    Your participation in the Challenge happens through a self-hosted GitLab instance
    of crowdAI. You need to create a \"private\" repository with your initial code
    (or simply code copied from the starter kit). You can make a submission by creating
    a new git tag and pushing the tag on your repository. crowdAI bot identifies your
    push tag, automatically clone your repository into a docker image and run an evaluation.
    After evaluation, the [leaderboard](https://www.crowdai.org/challenges/marlo-2018/leaderboards){:target='_blank'}
    is updated. You can find more detail about the actual submission process in the
    starter-kit.\r\n\r\nYour code (and also the corresponding issues for every submission)
    will stay private during the Challenge. You are encouraged to add an opensource
    license of your choice. At the end of the Challenge, you will be provided a time
    period, within which you can decide to keep your code private and not making it
    publicly available. Else your repository will be made public by default. If the
    participant has not already added a License to the repository, a MIT license will
    be automatically added to the repository before making it public. Those who object
    to having their code made public, will be excluded from the automated process
    of making all the Challenge specific repositories public.\r\n\r\nWe are not responsible
    for entries that we do not receive for any reason, or for entries that we receive
    but are not decipherable for any reason. We will automatically disqualify any
    incomplete or illegible entries."
  description: |
    <blockquote>
      <p>We are accepting submissions for a small warmup round for you to get acquainted with the submission system. Please feel free to make a submission using the <a href="https://github.com/crowdAI/marlo-single-agent-starter-kit/" target="_blank">instructions here</a>.</p>
    </blockquote>

    <blockquote>
      <p><strong>Due to the delay of opening submission system, we extended the Entry Period to November 12. Please take a look at the part of Winner Selection for detail.</strong></p>
    </blockquote>

    <h2 id="what-is-the-challenge">What is the Challenge?</h2>

    <p>Learning to Play: The Multi-Agent Reinforcement Learning in MalmO Competition (“Challenge”) is a new challenge that proposes research on Multi-Agent Reinforcement Learning using multiple games. Participants would create learning agents that will be able to play multiple 3D games as deﬁned in the MalmO platform. The aim of the competition is to encourage AI research on more general approaches via multi-player games. For this, the Challenge will consist of not one but several games, each one of them with several tasks of varying difficulty and settings. Some of these tasks will be public and participants will be able to train on them. Others, however, will be private, only used to determine the ﬁnal rankings of the competition.</p>

    <p>Organizer (Microsoft, Queen Mary University of London and crowdAI) will make the Challenge tasks and sample code available via GitHub on or before the Challenge start date. Entries will be judged by Organizer according to the criteria outlined in the “Winner Selection” section below.</p>

    <p>The following prizes will be awarded: (1) the top 7 team will be awarded a MARLO Travel Grant with a maximum value of <code class="highlighter-rouge">$2,500</code> USD to join a relevant academic conference or workshop, additionally, the 1st winning team will be awarded a second MARLO Travel Grant with a maximum value of <code class="highlighter-rouge">$2,500</code> USD to join the <a href="https://www.appliedmldays.org/" target="_blank">Applied Machine Learning Days 2019</a> and (2) three winning teams will be awarded Microsoft Azure Sponsorship with a maximum value of <code class="highlighter-rouge">$10,000</code> USD for the 1st place, <code class="highlighter-rouge">$5,000</code> USD for the 2nd place and <code class="highlighter-rouge">$3,000</code> USD for the 3rd place. Please see the “Challenge Prizes” section below for further details.</p>

    <h2 id="what-are-the-start-and-end-dates">What are the start and end dates?</h2>

    <p>The Challenge starts at 00:01 Pacific Standard Time, on July 27, 2018, and the Qualify Round ends at 23:59 Pacific Standard Time, on Nov 12, 2018 (“Entry Period”). Entries must be received within the Entry Period to be eligible.</p>

    <p>The final tournament is held in <a href="https://marlo-ai.github.io/" target="_blank">MARLO workshop</a> of AIIDE’18, the 14th <a href="https://sites.google.com/ncsu.edu/aiide-2018/" target="_blank">AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</a> held at the University of Alberta in Edmonton, AB, Canada on November 14, 2018.</p>

    <h2 id="games-and-tasks">Games and Tasks</h2>

    <p>One of the main features of this competition is that agents play in multiple games. Therefore, several tasks are proposed for this contest. For the purpose of this document and the competition itself, we deﬁne:</p>

    <ul>
      <li>
        <p>Game: each one of the diﬀerent scenarios in which agents play.</p>
      </li>
      <li>
        <p>Task: each instance of a game. Tasks, within a game, may be diﬀerent to each other in terms of level layout, size, diﬃculty and other game-dependent settings.</p>
      </li>
    </ul>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ce3722de6fc358b980aab61893b6ffa0_Figure1.png" alt="Figure1.png" />
    Figure 1: Sketch of how games and tasks are organized in the Challenge.</p>

    <p>As can be seen, tasks will be of public nature and accessible by the participants, while others are secret and will be used to evaluate the submitted entries at the end of the competition.</p>

    <p>Tasks are distributed across sets:
    <img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1cf1be0ffe8090ed993adfcb63580163_Figure2.png" alt="Figure2.png" />
    Figure 2: On the left: Build Battle, where players need to recreate a structure (in this case, the structure is shown on the ground). On the right: Pig Chase, where players collaborate to corner the pig.</p>

    <h2 id="competition">Competition</h2>

    <p>To participate in the Challenge, you will first need to register as a user on crowdai.org and entry the Challenge. Then you start playing with the Challenge by simply cloning the <a href="https://github.com/crowdAI/marLo" target="_blank">starter-kit of the Challenge on GitHub</a>. Your participation in the Challenge happens through a self-hosted GitLab instance of crowdAI. You need to create a “private” repository with your initial code (or simply code copied from the starter kit). You can make a submission by creating a new git tag and pushing the tag on your repository. crowdAI bot identifies your push tag, automatically clone your repository into a docker image and run an evaluation. After evaluation, the <a href="https://www.crowdai.org/challenges/marlo-2018/leaderboards" target="_blank">leaderboard</a> is updated. You can find more detail about the actual submission process in the starter-kit.</p>

    <p>Your code (and also the corresponding issues for every submission) will stay private during the Challenge. You are encouraged to add an opensource license of your choice. At the end of the Challenge, you will be provided a time period, within which you can decide to keep your code private and not making it publicly available. Else your repository will be made public by default. If the participant has not already added a License to the repository, a MIT license will be automatically added to the repository before making it public. Those who object to having their code made public, will be excluded from the automated process of making all the Challenge specific repositories public.</p>

    <p>We are not responsible for entries that we do not receive for any reason, or for entries that we receive but are not decipherable for any reason. We will automatically disqualify any incomplete or illegible entries.</p>
  evaluation_markdown: "## Winner Selection\r\n\r\n### Qualify Round\r\n\r\n**Round
    1 – until October 21, 2018**\r\n\r\n* Submitted agent to work with fixed random
    agent as its opponent.\r\n\r\n**Round 2 – October 21, 2018 – November 12, 2018**\r\n\r\n*
    Submitted agent to work with another agent by other participants.\r\n\r\nAt the
    close of the Entry Period (on Nov 12, 2018), the Organizer will select 8-32 qualifying
    teams from eligible entries based upon the overall performance on the Competition
    tasks (i.e. game score) to invite them to the Final Tournament.\r\n\r\nThe decisions
    of the Organizer are final and binding. If we do not receive a sufficient number
    of entries meeting the entry requirements, we may, at our discretion, select fewer
    qualifying teams to the tournament.\r\n\r\n### Final Tournament\r\n\r\nThe Final
    Tournament is organized in a form of live competition at the workshop in AIIDE’18.
    The 8-32 invited teams are broken into 8 groups. Teams on each group play among
    themselves (in which we here called a “league”) to determine a ranking, and the
    top 2 teams of these players progresses to the next round. Each league (P players
    in a group) is played across the same N games, with T repetitions played per game.
    Each game has its own leaderboard of agents ranked by the quality of the players.
    These leaderboards award ranking points to the entries, following a Formula 1-like
    scheme: 25 points for the 1st ranked entry, 18 for the 2nd, 15, 12, 10, 8, 6,
    4, 2 and 1 for positions in the ranking from 3rd to 10th respectively. No points
    are awarded for positions 11th and below. The winner of the league is determined
    by adding all ranking points obtained in the diﬀerent games of the challenge.\r\n![Figure3.png](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/495f17c83f2f2a4f5deb6d6663d6a4dc_Figure3.png)\r\nFigure
    3: MARLO Final Tournament\r\n\r\nIf any team member is a potential winner and
    is 18 years of age or older, but is considered a minor in their place of legal
    residence, we may require that person’s parent or legal guardian to sign all required
    forms on their behalf. If that person does not complete the required forms as
    instructed and/or return the required forms within the time period listed on the
    winner notification message, we may disqualify the relevant team and select a
    runner-up.\r\n\r\nIf your team is confirmed as a winner of the Challenge:\r\n\r\n*
    It may not designate another party as the winner. If your team is unable or unwilling
    to accept its prize, we may award it to a runner-up; and\r\n\r\n* Team members
    will be solely responsible for any taxes that may be payable in connection with
    the award of any prize."
  evaluation: |
    <h2 id="winner-selection">Winner Selection</h2>

    <h3 id="qualify-round">Qualify Round</h3>

    <p><strong>Round 1 – until October 21, 2018</strong></p>

    <ul>
      <li>Submitted agent to work with fixed random agent as its opponent.</li>
    </ul>

    <p><strong>Round 2 – October 21, 2018 – November 12, 2018</strong></p>

    <ul>
      <li>Submitted agent to work with another agent by other participants.</li>
    </ul>

    <p>At the close of the Entry Period (on Nov 12, 2018), the Organizer will select 8-32 qualifying teams from eligible entries based upon the overall performance on the Competition tasks (i.e. game score) to invite them to the Final Tournament.</p>

    <p>The decisions of the Organizer are final and binding. If we do not receive a sufficient number of entries meeting the entry requirements, we may, at our discretion, select fewer qualifying teams to the tournament.</p>

    <h3 id="final-tournament">Final Tournament</h3>

    <p>The Final Tournament is organized in a form of live competition at the workshop in AIIDE’18. The 8-32 invited teams are broken into 8 groups. Teams on each group play among themselves (in which we here called a “league”) to determine a ranking, and the top 2 teams of these players progresses to the next round. Each league (P players in a group) is played across the same N games, with T repetitions played per game. Each game has its own leaderboard of agents ranked by the quality of the players. These leaderboards award ranking points to the entries, following a Formula 1-like scheme: 25 points for the 1st ranked entry, 18 for the 2nd, 15, 12, 10, 8, 6, 4, 2 and 1 for positions in the ranking from 3rd to 10th respectively. No points are awarded for positions 11th and below. The winner of the league is determined by adding all ranking points obtained in the diﬀerent games of the challenge.
    <img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/495f17c83f2f2a4f5deb6d6663d6a4dc_Figure3.png" alt="Figure3.png" />
    Figure 3: MARLO Final Tournament</p>

    <p>If any team member is a potential winner and is 18 years of age or older, but is considered a minor in their place of legal residence, we may require that person’s parent or legal guardian to sign all required forms on their behalf. If that person does not complete the required forms as instructed and/or return the required forms within the time period listed on the winner notification message, we may disqualify the relevant team and select a runner-up.</p>

    <p>If your team is confirmed as a winner of the Challenge:</p>

    <ul>
      <li>
        <p>It may not designate another party as the winner. If your team is unable or unwilling to accept its prize, we may award it to a runner-up; and</p>
      </li>
      <li>
        <p>Team members will be solely responsible for any taxes that may be payable in connection with the award of any prize.</p>
      </li>
    </ul>
  rules_markdown: "### MARLO Travel Grant Prize, Microsoft Azure Sponsorship Prize\r\n\r\nTo
    be eligible to win the MARLO Travel Grant prize or Microsoft Azure Sponsorship
    Prize, your team must meet the following criteria at the time of entry and at
    the time of award of any prize:\r\n\r\nEach team member must be enrolled in undergraduate
    or post-graduate studies (that is, PhD or Masters level) at an accredited college
    or university and the team lead must be registered in a PhD program at an accredited
    college or university.\r\n\r\n### General Requirements\r\n\r\nTo participate in
    the Challenge and be eligible to win either prize, each team member must meet
    the following criteria at the time of entry and at the time of award of any prize:\r\n\r\n*
    18 years of age or older. If any team member is 18 years of age or older, but
    is considered a minor in their place of residence, they should ask their parent’s
    or legal guardian’s permission prior to submitting an entry into the Challenge;
    and\r\n\r\n* Not an employee of Microsoft Corporation or of any Microsoft subsidiary;
    and\r\n\r\n* Not an immediate family (parent, sibling, spouse/domestic partner,
    child) or household member of a Microsoft employee, an employee of a Microsoft
    subsidiary, or any person involved in any part of the administration and execution
    of this promotion.\r\n\r\nSubject to meeting the relevant eligibility criteria,
    teams are permitted to enter to the prizes. Your team lead must notify Organizer
    before the end of the Entry Period (Oct 7, 2018) by sending an email to [MalmoAdm@microsoft.com](mailto:malmoadm@microsoft.com){:target='_blank'},
    indicating your crowdAI team name and all every individual team members. Only
    emails actually received by Organizer before the end of the Entry Period will
    be considered.\r\n\r\nThis promotion is void where prohibited by law. No purchase
    necessary.\r\n\r\n### How will my team’s entry be potentially used?\r\n\r\nExcept
    as set out below, we are not claiming any ownership rights to your team’s entry.
    However, by submitting your entry, each team member: 1. Grants Organizer an irrevocable,
    non-exclusive, royalty-free, worldwide right and license to: (i) use, review,
    assess, test, and otherwise analyze your entry and all its content in connection
    with the Challenge; and (ii) feature your entry and all content in connection
    with the marketing, sale, or promotion of the Challenge (including but not limited
    to internal and external sales meetings, conference presentations, tradeshows,
    and screen shots of the Challenge entry in press releases) in all media (now known
    or later developed); and 2. agrees to sign any necessary documentation that may
    be required for us and our designees to make use of the rights you granted above;
    and 3. understands and acknowledges that Organizer may have developed or commissioned
    materials similar or identical to your team’s submission and you waive any claims
    you may have resulting from any similarities to your entry; and 4. understands
    that we cannot control the incoming information you will disclose to our representatives
    in the course of entering, or what our representatives will remember about your
    entry. You also understand that we will not restrict work assignments of representatives
    who have had access to your entry. By entering the Challenge, you agree that use
    of information in our representatives’ unaided memories in the development or
    deployment of our products or services does not create liability for us under
    this agreement or copyright or trade secret law; and 5. understands that you will
    not receive any compensation or credit for use of your entry, other than what
    is described in these Challenge Rules.\r\n\r\nIf your team does not want to grant
    us these rights to your entry, please do not enter the Challenge.\r\n\r\n### What
    if something unexpected happens and the Challenge can’t run as planned?\r\n\r\nIf
    someone cheats, or a virus, bug, bot, catastrophic event, or any other unforeseen
    or unexpected event that cannot be reasonably anticipated or controlled (also
    referred to as force majeure), affects the fairness and/or integrity of the Challenge,
    we reserve the right to cancel, change, or suspend the Challenge. This right is
    reserved whether the event is due to human or technical error. If a solution cannot
    be found to restore the integrity of the Challenge, we reserve the right to select
    winning teams from among all eligible entries received before we had to cancel,
    change, or suspend the Challenge.\r\nIf any team member attempts to, or we have
    strong reason to believe that a team member has, compromised the integrity or
    the legitimate operation of the Challenge by cheating, or through any other illegal,
    unlawful, or unfair activity, we may seek damages from that team member to the
    fullest extent permitted by law. Further, we may disqualify the team, and ban
    any team member from participating in any of our future Challenges, so please
    play fairly."
  rules: |
    <h3 id="marlo-travel-grant-prize-microsoft-azure-sponsorship-prize">MARLO Travel Grant Prize, Microsoft Azure Sponsorship Prize</h3>

    <p>To be eligible to win the MARLO Travel Grant prize or Microsoft Azure Sponsorship Prize, your team must meet the following criteria at the time of entry and at the time of award of any prize:</p>

    <p>Each team member must be enrolled in undergraduate or post-graduate studies (that is, PhD or Masters level) at an accredited college or university and the team lead must be registered in a PhD program at an accredited college or university.</p>

    <h3 id="general-requirements">General Requirements</h3>

    <p>To participate in the Challenge and be eligible to win either prize, each team member must meet the following criteria at the time of entry and at the time of award of any prize:</p>

    <ul>
      <li>
        <p>18 years of age or older. If any team member is 18 years of age or older, but is considered a minor in their place of residence, they should ask their parent’s or legal guardian’s permission prior to submitting an entry into the Challenge; and</p>
      </li>
      <li>
        <p>Not an employee of Microsoft Corporation or of any Microsoft subsidiary; and</p>
      </li>
      <li>
        <p>Not an immediate family (parent, sibling, spouse/domestic partner, child) or household member of a Microsoft employee, an employee of a Microsoft subsidiary, or any person involved in any part of the administration and execution of this promotion.</p>
      </li>
    </ul>

    <p>Subject to meeting the relevant eligibility criteria, teams are permitted to enter to the prizes. Your team lead must notify Organizer before the end of the Entry Period (Oct 7, 2018) by sending an email to <a href="mailto:malmoadm@microsoft.com" target="_blank">MalmoAdm@microsoft.com</a>, indicating your crowdAI team name and all every individual team members. Only emails actually received by Organizer before the end of the Entry Period will be considered.</p>

    <p>This promotion is void where prohibited by law. No purchase necessary.</p>

    <h3 id="how-will-my-teams-entry-be-potentially-used">How will my team’s entry be potentially used?</h3>

    <p>Except as set out below, we are not claiming any ownership rights to your team’s entry. However, by submitting your entry, each team member: 1. Grants Organizer an irrevocable, non-exclusive, royalty-free, worldwide right and license to: (i) use, review, assess, test, and otherwise analyze your entry and all its content in connection with the Challenge; and (ii) feature your entry and all content in connection with the marketing, sale, or promotion of the Challenge (including but not limited to internal and external sales meetings, conference presentations, tradeshows, and screen shots of the Challenge entry in press releases) in all media (now known or later developed); and 2. agrees to sign any necessary documentation that may be required for us and our designees to make use of the rights you granted above; and 3. understands and acknowledges that Organizer may have developed or commissioned materials similar or identical to your team’s submission and you waive any claims you may have resulting from any similarities to your entry; and 4. understands that we cannot control the incoming information you will disclose to our representatives in the course of entering, or what our representatives will remember about your entry. You also understand that we will not restrict work assignments of representatives who have had access to your entry. By entering the Challenge, you agree that use of information in our representatives’ unaided memories in the development or deployment of our products or services does not create liability for us under this agreement or copyright or trade secret law; and 5. understands that you will not receive any compensation or credit for use of your entry, other than what is described in these Challenge Rules.</p>

    <p>If your team does not want to grant us these rights to your entry, please do not enter the Challenge.</p>

    <h3 id="what-if-something-unexpected-happens-and-the-challenge-cant-run-as-planned">What if something unexpected happens and the Challenge can’t run as planned?</h3>

    <p>If someone cheats, or a virus, bug, bot, catastrophic event, or any other unforeseen or unexpected event that cannot be reasonably anticipated or controlled (also referred to as force majeure), affects the fairness and/or integrity of the Challenge, we reserve the right to cancel, change, or suspend the Challenge. This right is reserved whether the event is due to human or technical error. If a solution cannot be found to restore the integrity of the Challenge, we reserve the right to select winning teams from among all eligible entries received before we had to cancel, change, or suspend the Challenge.
    If any team member attempts to, or we have strong reason to believe that a team member has, compromised the integrity or the legitimate operation of the Challenge by cheating, or through any other illegal, unlawful, or unfair activity, we may seek damages from that team member to the fullest extent permitted by law. Further, we may disqualify the team, and ban any team member from participating in any of our future Challenges, so please play fairly.</p>
  prizes_markdown: "### MARLO Travel Grant Prize\r\n\r\nThe top 7 team will be awarded
    a MARLO Travel Grant with a maximum value of `$2,500` USD for the team members
    to join a relevant conference to publish their competition result. The conference
    is within a year after finishing the final tournament and it is decided later
    based on mutual agreement between Organizer and the winning team. Additionally,
    the 1st winning team will be awarded a second MARLO Travel Grant with a maximum
    value of `$2,500` USD to join the [Applied Machine Learning Days 2019](https://www.appliedmldays.org/){:target='_blank'}.
    Organizer will reimburse the following reasonably and necessarily incurred travel
    expenses of team members in attending the conference: (1) economy class roundtrip
    airfares from your nearest airport to the city of conference; and (2) accommodation
    up to 3 days. Organizer will not cover meals during your attendance in the conference.\r\n\r\n###
    Microsoft Azure Sponsorship Prize\r\n\r\nThree teams will each win a Microsoft
    Azure Sponsorship with a maximum value of `$10,000` USD for the 1st place, `$5,000`
    USD for the 2nd place and `$3,000` USD for the 3rd place. The Microsoft Azure
    Sponsorship will be awarded to the team lead nominated by the relevant team. Team
    leads will be solely responsible for allocation of the grant between the relevant
    team members. The award of a Microsoft Azure Sponsorship will be subject to each
    team member agreeing to comply with such terms and conditions of use or other
    requirements that Microsoft may impose.\r\n\r\n### Azure for Students\r\n\r\nAzure
    for Students gets you started with $100 in Azure credits to be used within the
    first 12 months plus select free services (subject to change) without requiring
    a credit card at sign-up. This is not mandatory for the Challenge. We recommend
    you take a look at the portal site if you would get additional computing resource:
    [https://azure.microsoft.com/en-us/free/students/](https://azure.microsoft.com/en-us/free/students/){:target='_blank'}.\r\n\r\n###
    Winner’s List and Sponsor\r\n\r\nWe will notify winning teams by November 17,
    2018. We will also post a leaderboard detailing the top 32 team entries online
    on crowdAI and under Microsoft Research website. Please note that we will only
    post team names and we will not post the names of any individual team members.
    This list will remain posted for a period of at least 12 calendar months.\r\n\r\nIf
    your team’s entry is in a public repository we may also post a link to that repository.
    Please check your GitLab and GitHub settings to ensure that this will not result
    in individual team members becoming identifiable, unless this is intended by the
    individual(s) in question.\r\n\r\nThis promotion is sponsored by Microsoft Corporation,
    One Microsoft Way, Redmond, WA 98052-6399, USA.\r\n\r\n## Timeline\r\n \r\n* July
    27th, 2018: Competition Open\r\n* October 21th, 2018 : Qualifying Round 1 submission
    deadline\r\n* November 12th, 2018 : Qualifying Round 2 submission deadline\r\n*
    November 14th, 2018: Final Round\r\n\r\n## Organizing Team\r\n \r\nThe organizing
    team comes from multiple groups — Queen Mary University of London, École Polytechnique
    Fédérale de Lausanne and Microsoft Research.\r\n\r\nThe organizing team consists
    of:\r\n\r\n* Diego Perez-Liebana (Queen Mary University of London)\r\n* Raluca
    D. Gaina (Queen Mary University of London)\r\n* Daniel Ionita (Queen Mary University
    of London)\r\n* Sharada Prasanna Mohanty (École polytechnique fédérale de Lausanne)\r\n*
    Sam Devlin (Microsoft Research)\r\n* Andre Kramer (Microsoft Research)\r\n* Sean
    Kuno (Microsoft Research)\r\n* Katja Hofmann (Microsoft Research)\r\n\r\n## Sponsors\r\n
    \r\n* Microsoft\r\n\r\n## Partners\r\n \r\n* Queen Mary University of London  \r\n*
    EPFL  \r\n"
  prizes: |
    <h3 id="marlo-travel-grant-prize">MARLO Travel Grant Prize</h3>

    <p>The top 7 team will be awarded a MARLO Travel Grant with a maximum value of <code class="highlighter-rouge">$2,500</code> USD for the team members to join a relevant conference to publish their competition result. The conference is within a year after finishing the final tournament and it is decided later based on mutual agreement between Organizer and the winning team. Additionally, the 1st winning team will be awarded a second MARLO Travel Grant with a maximum value of <code class="highlighter-rouge">$2,500</code> USD to join the <a href="https://www.appliedmldays.org/" target="_blank">Applied Machine Learning Days 2019</a>. Organizer will reimburse the following reasonably and necessarily incurred travel expenses of team members in attending the conference: (1) economy class roundtrip airfares from your nearest airport to the city of conference; and (2) accommodation up to 3 days. Organizer will not cover meals during your attendance in the conference.</p>

    <h3 id="microsoft-azure-sponsorship-prize">Microsoft Azure Sponsorship Prize</h3>

    <p>Three teams will each win a Microsoft Azure Sponsorship with a maximum value of <code class="highlighter-rouge">$10,000</code> USD for the 1st place, <code class="highlighter-rouge">$5,000</code> USD for the 2nd place and <code class="highlighter-rouge">$3,000</code> USD for the 3rd place. The Microsoft Azure Sponsorship will be awarded to the team lead nominated by the relevant team. Team leads will be solely responsible for allocation of the grant between the relevant team members. The award of a Microsoft Azure Sponsorship will be subject to each team member agreeing to comply with such terms and conditions of use or other requirements that Microsoft may impose.</p>

    <h3 id="azure-for-students">Azure for Students</h3>

    <p>Azure for Students gets you started with $100 in Azure credits to be used within the first 12 months plus select free services (subject to change) without requiring a credit card at sign-up. This is not mandatory for the Challenge. We recommend you take a look at the portal site if you would get additional computing resource: <a href="https://azure.microsoft.com/en-us/free/students/" target="_blank">https://azure.microsoft.com/en-us/free/students/</a>.</p>

    <h3 id="winners-list-and-sponsor">Winner’s List and Sponsor</h3>

    <p>We will notify winning teams by November 17, 2018. We will also post a leaderboard detailing the top 32 team entries online on crowdAI and under Microsoft Research website. Please note that we will only post team names and we will not post the names of any individual team members. This list will remain posted for a period of at least 12 calendar months.</p>

    <p>If your team’s entry is in a public repository we may also post a link to that repository. Please check your GitLab and GitHub settings to ensure that this will not result in individual team members becoming identifiable, unless this is intended by the individual(s) in question.</p>

    <p>This promotion is sponsored by Microsoft Corporation, One Microsoft Way, Redmond, WA 98052-6399, USA.</p>

    <h2 id="timeline">Timeline</h2>

    <ul>
      <li>July 27th, 2018: Competition Open</li>
      <li>October 21th, 2018 : Qualifying Round 1 submission deadline</li>
      <li>November 12th, 2018 : Qualifying Round 2 submission deadline</li>
      <li>November 14th, 2018: Final Round</li>
    </ul>

    <h2 id="organizing-team">Organizing Team</h2>

    <p>The organizing team comes from multiple groups — Queen Mary University of London, École Polytechnique Fédérale de Lausanne and Microsoft Research.</p>

    <p>The organizing team consists of:</p>

    <ul>
      <li>Diego Perez-Liebana (Queen Mary University of London)</li>
      <li>Raluca D. Gaina (Queen Mary University of London)</li>
      <li>Daniel Ionita (Queen Mary University of London)</li>
      <li>Sharada Prasanna Mohanty (École polytechnique fédérale de Lausanne)</li>
      <li>Sam Devlin (Microsoft Research)</li>
      <li>Andre Kramer (Microsoft Research)</li>
      <li>Sean Kuno (Microsoft Research)</li>
      <li>Katja Hofmann (Microsoft Research)</li>
    </ul>

    <h2 id="sponsors">Sponsors</h2>

    <ul>
      <li>Microsoft</li>
    </ul>

    <h2 id="partners">Partners</h2>

    <ul>
      <li>Queen Mary University of London</li>
      <li>EPFL</li>
    </ul>
  resources_markdown: "Please visit the starter-kit page first for our comprehensive
    instruction to start this competition.\r\n\r\n* Starter-kit: [https://github.com/crowdAI/marLo](https://github.com/crowdAI/marLo){:target='_blank'}\r\n\r\nThe
    original resource of Project Malmo platform is available on the GitHub.\r\n\r\n*
    GitHub: [https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo){:target='_blank'}\r\n*
    Project Malmo website: [https://www.microsoft.com/en-us/research/project/project-malmo/](https://www.microsoft.com/en-us/research/project/project-malmo/){:target='_blank'}\r\n\r\nFollow
    twitter for the latest information of Project Malmo\r\n\r\n* Twitter: [@Project_Malmo](https://twitter.com/Project_Malmo){:target='_blank'}\r\n\r\n##
    Contact Us\r\n \r\n* Gitter Channel : [ https://gitter.im/Microsoft/malmo ](https://gitter.im/Microsoft/malmo){:target='_blank'}
    \ \r\n* Discussion Forum : [ https://www.crowdai.org/challenges/marlo-2018/topics
    ](https://www.crowdai.org/challenges/marlo-2018/topics){:target='_blank'}\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at:  \r\n\r\n* [sharada.mohanty@epfl.ch](mailto:sharada.mohanty@epfl.ch){:target='_blank'}\r\n*
    [malmoadm@microsoft.com](mailto:malmoadm@microsoft.com){:target='_blank'}  \r\n"
  resources: |
    <p>Please visit the starter-kit page first for our comprehensive instruction to start this competition.</p>

    <ul>
      <li>Starter-kit: <a href="https://github.com/crowdAI/marLo" target="_blank">https://github.com/crowdAI/marLo</a></li>
    </ul>

    <p>The original resource of Project Malmo platform is available on the GitHub.</p>

    <ul>
      <li>GitHub: <a href="https://github.com/Microsoft/malmo" target="_blank">https://github.com/Microsoft/malmo</a></li>
      <li>Project Malmo website: <a href="https://www.microsoft.com/en-us/research/project/project-malmo/" target="_blank">https://www.microsoft.com/en-us/research/project/project-malmo/</a></li>
    </ul>

    <p>Follow twitter for the latest information of Project Malmo</p>

    <ul>
      <li>Twitter: <a href="https://twitter.com/Project_Malmo" target="_blank">@Project_Malmo</a></li>
    </ul>

    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/Microsoft/malmo" target="_blank"> https://gitter.im/Microsoft/malmo </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/marlo-2018/topics" target="_blank"> https://www.crowdai.org/challenges/marlo-2018/topics </a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

    <ul>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank">sharada.mohanty@epfl.ch</a></li>
      <li><a href="mailto:malmoadm@microsoft.com" target="_blank">malmoadm@microsoft.com</a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: Screen_Shot_2018-04-20_at_12.41.51.png
  featured_sequence: 9
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_40:
  id: 40
  organizer_id: 15
  challenge: 'NIPS 2018 : Adversarial Vision Challenge (Untargeted Attack Track)'
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &12 2018-06-22 14:14:01.474524000 Z
    zone: *2
    time: *12
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &13 2018-10-04 11:04:29.591956000 Z
    zone: *2
    time: *13
  tagline: ''
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 5224
  participant_count: 72
  submission_count: 421
  score_title: Median L2 Distance
  score_secondary_title: Mean L2 Distance
  slug: nips-2018-adversarial-vision-challenge-untargeted-attack-track
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: nips-2018-avc-untargeted-attack
  online_grading: true
  vote_count: 18
  description_markdown: "Welcome to the _Adversarial Vision Challenge_, one of the
    official challenges in the [ NIPS 2018 ](https://nips.cc){:target='_blank'} competition
    track. In this competition you can take on the role of an attacker or a defender
    (or both). As a defender you are trying to build a visual object classifier that
    is as robust to image perturbations as possible. As an attacker, your task is
    to find the smallest possible image perturbations that will fool a classifier.\r\n\r\nThe
    overall goal of this challenge is to facilitate measurable progress towards robust
    machine vision models and more generally applicable adversarial attacks. As of
    right now, modern machine vision algorithms are extremely susceptible to small
    and almost imperceptible perturbations of their inputs (so-called _adversarial
    examples_). This property reveals an astonishing difference in the information
    processing of humans and machines and raises security concerns for many deployed
    machine vision systems like autonomous cars. Improving the robustness of vision
    algorithms is thus important to close the gap between human and machine perception
    and to enable safety-critical applications.\r\n\r\n<img src=\"https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png\"
    alt=\"Illustration Adversarial Examples\" style=\"width: 800px\" />\r\n\r\n##
    Competition tracks\r\n\r\nThere will be three tracks in which you and your team
    can compete: \r\n\r\n\r\n* [Robust Model Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track){:target='_blank'}\r\n*
    [Untargeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track){:target='_blank'}\r\n*
    [Targeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/){:target='_blank'}\r\n\r\n\r\n\r\nIn
    this track you build an attack algorithm that breaks the defenses. For each model
    and each given image your attack tries to find the smallest perturbation that
    makes the model predict a wrong class label (so-called _adversarial perturbations_).
    Your attack will be able to craft model-specific adversarials by asking the model
    for its prediction on self-defined inputs (up to 1000 times / image). The smaller
    the adversarial perturbations are that your attack finds (on average), the better
    is your score (the exact scoring formula will be published soon).\r\n\r\n## Evaluation
    criterion\r\n\r\nAttacks are scored as follows (lower is better):\r\n\r\n* Let
    A be the attack and S be the set of samples.\r\n* We apply attack A against the
    best five models for each sample in S.\r\n* If an attack fails to produce a (targeted)
    adversarial for a given sample, then we register a worst case distance (distance
    of the sample to a uniform grey image).\r\n* The final attack score is the median
    L2 distance across samples.\r\n\r\nThe top-5 models against which submissions
    are evaluated are fixed for two weeks at a time after which we evaluate all current
    submissions to determine the new top-5 models for the upcoming two weeks.\r\n\r\n\r\n##
    Submissions\r\n\r\nTo make a submission, please follow the instructions in this
    GitLab repository:\r\n[ https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template
    ](https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template){:target='_blank'}\r\n\r\nFork
    the above **template repository** in GitLab and follow the instructions stated
    in the README.md.\r\nYou need to have a crowdAI-account and sign in to GitLab
    using this account.\r\nIn the README you will also find links to multiple fully
    functional examples.\r\n\r\n\r\n## Timeline\r\n\r\n_(tentative)_.  \r\n* **June
    25th, 2018**  : Challenge begins     \r\n* **November 1st** : Final submission
    date   \r\n* **November 15th** : Winners Announced     \r\n\r\n## Organizing Team\r\n\r\nThe
    organizing team comes from multiple groups — [University of Tübingen](https://www.uni-tuebingen.de/en/university.html),
    [Google Brain](https://research.google.com/teams/brain/), [EPFL](https://www.epfl.ch/index.en.html)
    and [Pennsylvania State University](http://www.psu.edu/).\r\n\r\nThe Team consists
    of:    \r\n*  [ Wieland Brendel ](https://twitter.com/wielandbr){:target='_blank'}
    \  \r\n*  [ Jonas Rauber ](https://jonasrauber.de){:target='_blank'}   \r\n*  [
    Alexey Kurakin ](https://twitter.com/alexey2004){:target='_blank'}   \r\n*  [
    Nicolas Papernot ](https://twitter.com/NicolasPapernot){:target='_blank'}   \r\n*
    \ [ Behar Veliqi ](https://twitter.com/beveliqi){:target='_blank'}   \r\n*  [
    Sharada P. Mohanty ](https://twitter.com/MeMohanty){:target='_blank'}    \r\n*
    \ [ Marcel Salathé ](https://twitter.com/marcelsalathe){:target='_blank'}   \r\n*
    \ [ Matthias Bethge ](https://twitter.com/MatthiasBethge){:target='_blank'}   \r\n\r\n##
    Sponsors\r\n\r\n| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png\"
    alt=\"Amazon AWS\" style=\"width: 120px\" />     | <img src=\"https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png\"
    alt=\"Paperspace\"  style=\"width: 240px\"/>  |\r\n\r\n"
  description: |+
    <p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href="https://nips.cc" target="_blank"> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

    <p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png" alt="Illustration Adversarial Examples" style="width: 800px" /></p>

    <h2 id="competition-tracks">Competition tracks</h2>

    <p>There will be three tracks in which you and your team can compete:</p>

    <ul>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track" target="_blank">Robust Model Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track" target="_blank">Untargeted Attacks Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/" target="_blank">Targeted Attacks Track</a></li>
    </ul>

    <p>In this track you build an attack algorithm that breaks the defenses. For each model and each given image your attack tries to find the smallest perturbation that makes the model predict a wrong class label (so-called <em>adversarial perturbations</em>). Your attack will be able to craft model-specific adversarials by asking the model for its prediction on self-defined inputs (up to 1000 times / image). The smaller the adversarial perturbations are that your attack finds (on average), the better is your score (the exact scoring formula will be published soon).</p>

    <h2 id="evaluation-criterion">Evaluation criterion</h2>

    <p>Attacks are scored as follows (lower is better):</p>

    <ul>
      <li>Let A be the attack and S be the set of samples.</li>
      <li>We apply attack A against the best five models for each sample in S.</li>
      <li>If an attack fails to produce a (targeted) adversarial for a given sample, then we register a worst case distance (distance of the sample to a uniform grey image).</li>
      <li>The final attack score is the median L2 distance across samples.</li>
    </ul>

    <p>The top-5 models against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 models for the upcoming two weeks.</p>

    <h2 id="submissions">Submissions</h2>

    <p>To make a submission, please follow the instructions in this GitLab repository:
    <a href="https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template" target="_blank"> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template </a></p>

    <p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
    You need to have a crowdAI-account and sign in to GitLab using this account.
    In the README you will also find links to multiple fully functional examples.</p>

    <h2 id="timeline">Timeline</h2>

    <p><em>(tentative)</em>.<br />
    * <strong>June 25th, 2018</strong>  : Challenge begins   <br />
    * <strong>November 1st</strong> : Final submission date <br />
    * <strong>November 15th</strong> : Winners Announced</p>

    <h2 id="organizing-team">Organizing Team</h2>

    <p>The organizing team comes from multiple groups — <a href="https://www.uni-tuebingen.de/en/university.html">University of Tübingen</a>, <a href="https://research.google.com/teams/brain/">Google Brain</a>, <a href="https://www.epfl.ch/index.en.html">EPFL</a> and <a href="http://www.psu.edu/">Pennsylvania State University</a>.</p>

    <p>The Team consists of:  <br />
    *  <a href="https://twitter.com/wielandbr" target="_blank"> Wieland Brendel </a> <br />
    *  <a href="https://jonasrauber.de" target="_blank"> Jonas Rauber </a> <br />
    *  <a href="https://twitter.com/alexey2004" target="_blank"> Alexey Kurakin </a> <br />
    *  <a href="https://twitter.com/NicolasPapernot" target="_blank"> Nicolas Papernot </a> <br />
    *  <a href="https://twitter.com/beveliqi" target="_blank"> Behar Veliqi </a> <br />
    *  <a href="https://twitter.com/MeMohanty" target="_blank"> Sharada P. Mohanty </a>  <br />
    *  <a href="https://twitter.com/marcelsalathe" target="_blank"> Marcel Salathé </a> <br />
    *  <a href="https://twitter.com/MatthiasBethge" target="_blank"> Matthias Bethge </a></p>

    <h2 id="sponsors">Sponsors</h2>

    <table>
      <tbody>
        <tr>
          <td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png" alt="Amazon AWS" style="width: 120px" /></td>
          <td><img src="https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png" alt="Paperspace" style="width: 240px" /></td>
        </tr>
      </tbody>
    </table>

  evaluation_markdown: ''
  evaluation: "\n"
  rules_markdown: "* Betghelab and Google Brain employees can participate but are
    ineligible for prizes\r\n* participants are required to release the code of their
    submissions as open source to be eligible for the final scoring\r\n* any legitimate
    input that is not classified by a model will be counted as an adversarial\r\n*
    if an attack fails to produce an adversarial, we will register a worst-case adversarial
    instead\r\n* all classifiers must be stateless and act on one image at a time\r\n*
    the decision of each classifier must be deterministic\r\n* attacks are allowed
    to query the model on self-defined inputs up to 1.000 times / sample\r\n* each
    model has to process one image within 40ms on a K80 GPU (excluding initialization
    and setup which may take up to 100s)\r\n* each attack has to process a batch of
    10 images within 900s on a K80 GPU"
  rules: |
    <ul>
      <li>Betghelab and Google Brain employees can participate but are ineligible for prizes</li>
      <li>participants are required to release the code of their submissions as open source to be eligible for the final scoring</li>
      <li>any legitimate input that is not classified by a model will be counted as an adversarial</li>
      <li>if an attack fails to produce an adversarial, we will register a worst-case adversarial instead</li>
      <li>all classifiers must be stateless and act on one image at a time</li>
      <li>the decision of each classifier must be deterministic</li>
      <li>attacks are allowed to query the model on self-defined inputs up to 1.000 times / sample</li>
      <li>each model has to process one image within 40ms on a K80 GPU (excluding initialization and setup which may take up to 100s)</li>
      <li>each attack has to process a batch of 10 images within 900s on a K80 GPU</li>
    </ul>
  prizes_markdown: "* **$15.000 worth of Paperspace cloud compute credits**: The top-20
    teams in each track (defense, untargeted attack, targeted attack) as of 28. September
    will receive 250$ each. "
  prizes: |
    <ul>
      <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
    </ul>
  resources_markdown: "## Contact Us   \r\n   \r\n* Gitter Channel : [crowdAI/nips-2018-adversarial-vision-challenge](https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics)\r\n
    \  \r\nWe strongly encourage you to use the public channels mentioned above for
    communications between the participants and the organizers. In extreme cases,
    if there are any queries or comments that you would like to make using a private
    communication channel, then you can send us an email at :\r\n\r\n*  [ wieland.brendel@bethgelab.org\r\n
    ](mailto:wieland.brendel@bethgelab.org){:target='_blank'}\r\n* [behar.veliqi@bethgelab.org](mailto:behar.veliqi@bethgelab.org){:target='_blank'}\r\n*
    \ [ sharada.mohanty@epfl.ch\r\n ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:wieland.brendel@bethgelab.org" target="_blank"> wieland.brendel@bethgelab.org
     </a></li>
      <li><a href="mailto:behar.veliqi@bethgelab.org" target="_blank">behar.veliqi@bethgelab.org</a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: 
  submission_instructions: 
  license_markdown: 
  license: 
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: challenge_logo_untargeted_attack.png
  featured_sequence: 9
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: ''
  winner_description: "\n"
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: nips-2018-avc-untargeted-attack
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_39:
  id: 39
  organizer_id: 15
  challenge: 'NIPS 2018 : Adversarial Vision Challenge (Targeted Attack Track)'
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &14 2018-06-22 14:12:31.543886000 Z
    zone: *2
    time: *14
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &15 2018-10-04 11:04:28.611030000 Z
    zone: *2
    time: *15
  tagline: ''
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 3369
  participant_count: 42
  submission_count: 129
  score_title: Median L2 Distance
  score_secondary_title: Mean L2 Distance
  slug: nips-2018-adversarial-vision-challenge-targeted-attack-track
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: nips-2018-avc-targeted-attack
  online_grading: true
  vote_count: 14
  description_markdown: "Welcome to the _Adversarial Vision Challenge_, one of the
    official challenges in the [ NIPS 2018 ](https://nips.cc){:target='_blank'} competition
    track. In this competition you can take on the role of an attacker or a defender
    (or both). As a defender you are trying to build a visual object classifier that
    is as robust to image perturbations as possible. As an attacker, your task is
    to find the smallest possible image perturbations that will fool a classifier.\r\n\r\nThe
    overall goal of this challenge is to facilitate measurable progress towards robust
    machine vision models and more generally applicable adversarial attacks. As of
    right now, modern machine vision algorithms are extremely susceptible to small
    and almost imperceptible perturbations of their inputs (so-called _adversarial
    examples_). This property reveals an astonishing difference in the information
    processing of humans and machines and raises security concerns for many deployed
    machine vision systems like autonomous cars. Improving the robustness of vision
    algorithms is thus important to close the gap between human and machine perception
    and to enable safety-critical applications.\r\n\r\n<img src=\"https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png\"
    alt=\"Illustration Adversarial Examples\" style=\"width: 800px\" />\r\n\r\n##
    Competition tracks\r\n\r\nThere will be three tracks in which you and your team
    can compete: \r\n\r\n* [Robust Model Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track){:target='_blank'}\r\n*
    [Untargeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track){:target='_blank'}\r\n*
    [Targeted Attacks Track](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/){:target='_blank'}\r\n\r\n\r\nThis
    track is very similar to the  [untargeted attacks](https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track){:target='_blank'}
    track. The only difference is that here an adversarial perturbation is not defined
    as making the model predict _any_ wrong label but it has to get the model to predict
    a _particular_ (wrong) label.\r\n\r\n## Evaluation criterion\r\n\r\nAttacks are
    scored as follows (lower is better):\r\n\r\n* Let A be the attack and S be the
    set of samples.\r\n* We apply attack A against the best five models for each sample
    in S.\r\n* If an attack fails to produce a (targeted) adversarial for a given
    sample, then we register a worst case distance (distance of the sample to a uniform
    grey image).\r\n* The final attack score is the median L2 distance across samples.\r\n\r\nThe
    top-5 models against which submissions are evaluated are fixed for two weeks at
    a time after which we evaluate all current submissions to determine the new top-5
    models for the upcoming two weeks.\r\n\r\n\r\n## Submissions\r\n\r\nTo make a
    submission, please follow the instructions in this GitLab repository:\r\n[ https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template
    ](https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template){:target='_blank'}\r\n\r\nFork
    the above **template repository** in GitLab and follow the instructions stated
    in the README.md.\r\nYou need to have a crowdAI-account and sign in to GitLab
    using this account.\r\nIn the README you will also find links to multiple fully
    functional examples.\r\n\r\n\r\n## Timeline\r\n\r\n_(tentative)_.  \r\n* **June
    25th, 2018**  : Challenge begins     \r\n* **November 1st** : Final submission
    date   \r\n* **November 15th** : Winners Announced     \r\n\r\n## Organizing Team\r\n\r\nThe
    organizing team comes from multiple groups — [University of Tübingen](https://www.uni-tuebingen.de/en/university.html),
    [Google Brain](https://research.google.com/teams/brain/), [EPFL](https://www.epfl.ch/index.en.html)
    and [Pennsylvania State University](http://www.psu.edu/).\r\n\r\nThe Team consists
    of:    \r\n*  [ Wieland Brendel ](https://twitter.com/wielandbr){:target='_blank'}
    \  \r\n*  [ Jonas Rauber ](https://jonasrauber.de){:target='_blank'}   \r\n*  [
    Alexey Kurakin ](https://twitter.com/alexey2004){:target='_blank'}   \r\n*  [
    Nicolas Papernot ](https://twitter.com/NicolasPapernot){:target='_blank'}   \r\n*
    \ [ Behar Veliqi ](https://twitter.com/beveliqi){:target='_blank'}   \r\n*  [
    Sharada P. Mohanty ](https://twitter.com/MeMohanty){:target='_blank'}    \r\n*
    \ [ Marcel Salathé ](https://twitter.com/marcelsalathe){:target='_blank'}   \r\n*
    \ [ Matthias Bethge ](https://twitter.com/MatthiasBethge){:target='_blank'}   \r\n\r\n##
    Sponsors\r\n\r\n| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png\"
    alt=\"Amazon AWS\" style=\"width: 120px\" />     | <img src=\"https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png\"
    alt=\"Paperspace\"  style=\"width: 240px\"/>  |\r\n"
  description: |
    <p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href="https://nips.cc" target="_blank"> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

    <p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png" alt="Illustration Adversarial Examples" style="width: 800px" /></p>

    <h2 id="competition-tracks">Competition tracks</h2>

    <p>There will be three tracks in which you and your team can compete:</p>

    <ul>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track" target="_blank">Robust Model Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track" target="_blank">Untargeted Attacks Track</a></li>
      <li><a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/" target="_blank">Targeted Attacks Track</a></li>
    </ul>

    <p>This track is very similar to the  <a href="https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track" target="_blank">untargeted attacks</a> track. The only difference is that here an adversarial perturbation is not defined as making the model predict <em>any</em> wrong label but it has to get the model to predict a <em>particular</em> (wrong) label.</p>

    <h2 id="evaluation-criterion">Evaluation criterion</h2>

    <p>Attacks are scored as follows (lower is better):</p>

    <ul>
      <li>Let A be the attack and S be the set of samples.</li>
      <li>We apply attack A against the best five models for each sample in S.</li>
      <li>If an attack fails to produce a (targeted) adversarial for a given sample, then we register a worst case distance (distance of the sample to a uniform grey image).</li>
      <li>The final attack score is the median L2 distance across samples.</li>
    </ul>

    <p>The top-5 models against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 models for the upcoming two weeks.</p>

    <h2 id="submissions">Submissions</h2>

    <p>To make a submission, please follow the instructions in this GitLab repository:
    <a href="https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template" target="_blank"> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template </a></p>

    <p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
    You need to have a crowdAI-account and sign in to GitLab using this account.
    In the README you will also find links to multiple fully functional examples.</p>

    <h2 id="timeline">Timeline</h2>

    <p><em>(tentative)</em>.<br />
    * <strong>June 25th, 2018</strong>  : Challenge begins   <br />
    * <strong>November 1st</strong> : Final submission date <br />
    * <strong>November 15th</strong> : Winners Announced</p>

    <h2 id="organizing-team">Organizing Team</h2>

    <p>The organizing team comes from multiple groups — <a href="https://www.uni-tuebingen.de/en/university.html">University of Tübingen</a>, <a href="https://research.google.com/teams/brain/">Google Brain</a>, <a href="https://www.epfl.ch/index.en.html">EPFL</a> and <a href="http://www.psu.edu/">Pennsylvania State University</a>.</p>

    <p>The Team consists of:  <br />
    *  <a href="https://twitter.com/wielandbr" target="_blank"> Wieland Brendel </a> <br />
    *  <a href="https://jonasrauber.de" target="_blank"> Jonas Rauber </a> <br />
    *  <a href="https://twitter.com/alexey2004" target="_blank"> Alexey Kurakin </a> <br />
    *  <a href="https://twitter.com/NicolasPapernot" target="_blank"> Nicolas Papernot </a> <br />
    *  <a href="https://twitter.com/beveliqi" target="_blank"> Behar Veliqi </a> <br />
    *  <a href="https://twitter.com/MeMohanty" target="_blank"> Sharada P. Mohanty </a>  <br />
    *  <a href="https://twitter.com/marcelsalathe" target="_blank"> Marcel Salathé </a> <br />
    *  <a href="https://twitter.com/MatthiasBethge" target="_blank"> Matthias Bethge </a></p>

    <h2 id="sponsors">Sponsors</h2>

    <table>
      <tbody>
        <tr>
          <td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png" alt="Amazon AWS" style="width: 120px" /></td>
          <td><img src="https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png" alt="Paperspace" style="width: 240px" /></td>
        </tr>
      </tbody>
    </table>
  evaluation_markdown: ''
  evaluation: "\n"
  rules_markdown: "* Betghelab and Google Brain employees can participate but are
    ineligible for prizes\r\n* participants are required to release the code of their
    submissions as open source to be eligible for the final scoring\r\n* any legitimate
    input that is not classified by a model will be counted as an adversarial\r\n*
    if an attack fails to produce an adversarial, we will register a worst-case adversarial
    instead\r\n* all classifiers must be stateless and act on one image at a time\r\n*
    the decision of each classifier must be deterministic\r\n* attacks are allowed
    to query the model on self-defined inputs up to 1.000 times / sample\r\n* each
    model has to process one image within 40ms on a K80 GPU (excluding initialization
    and setup which may take up to 100s)\r\n* each attack has to process a batch of
    10 images within 900s on a K80 GPU"
  rules: |
    <ul>
      <li>Betghelab and Google Brain employees can participate but are ineligible for prizes</li>
      <li>participants are required to release the code of their submissions as open source to be eligible for the final scoring</li>
      <li>any legitimate input that is not classified by a model will be counted as an adversarial</li>
      <li>if an attack fails to produce an adversarial, we will register a worst-case adversarial instead</li>
      <li>all classifiers must be stateless and act on one image at a time</li>
      <li>the decision of each classifier must be deterministic</li>
      <li>attacks are allowed to query the model on self-defined inputs up to 1.000 times / sample</li>
      <li>each model has to process one image within 40ms on a K80 GPU (excluding initialization and setup which may take up to 100s)</li>
      <li>each attack has to process a batch of 10 images within 900s on a K80 GPU</li>
    </ul>
  prizes_markdown: "* **$15.000 worth of Paperspace cloud compute credits**: The top-20
    teams in each track (defense, untargeted attack, targeted attack) as of 28. September
    will receive 250$ each. "
  prizes: |
    <ul>
      <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
    </ul>
  resources_markdown: "## Contact Us   \r\n   \r\n* Gitter Channel : [crowdAI/nips-2018-adversarial-vision-challenge](https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics)\r\n
    \  \r\nWe strongly encourage you to use the public channels mentioned above for
    communications between the participants and the organizers. In extreme cases,
    if there are any queries or comments that you would like to make using a private
    communication channel, then you can send us an email at :\r\n\r\n*  [ wieland.brendel@bethgelab.org\r\n
    ](mailto:wieland.brendel@bethgelab.org){:target='_blank'}\r\n* [behar.veliqi@bethgelab.org](mailto:behar.veliqi@bethgelab.org){:target='_blank'}\r\n*
    \ [ sharada.mohanty@epfl.ch\r\n ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:wieland.brendel@bethgelab.org" target="_blank"> wieland.brendel@bethgelab.org
     </a></li>
      <li><a href="mailto:behar.veliqi@bethgelab.org" target="_blank">behar.veliqi@bethgelab.org</a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: 
  submission_instructions: 
  license_markdown: 
  license: 
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: challenge_logo_targeted_attack.png
  featured_sequence: 9
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: ''
  winner_description: "\n"
  winners_tab_active: false
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: nips-2018-avc-targeted-attack
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_12:
  id: 12
  organizer_id: 6
  challenge: AI-generated music challenge
  status_cd: running
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &16 2017-11-02 18:03:50.837283000 Z
    zone: *2
    time: *16
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &17 2018-10-04 10:06:46.160852000 Z
    zone: *2
    time: *17
  tagline: 'Music generated by AI '
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 15246
  participant_count: 124
  submission_count: 108
  score_title: "$ \\mu - k * \\sigma $"
  score_secondary_title: "$ \\mu $"
  slug: ai-generated-music-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: AIGeneratedMusicChallenge
  online_grading: true
  vote_count: 81
  description_markdown: "Can AI generate music that we humans find beautiful, perhaps
    even moving? Let's find out!\r\n\r\nIn this challenge, participants are tasked
    to generate an AI model that learns on a large data set of music (in the form
    of MIDI files), and is then capable of producing its own music. Concretely, the
    model must produce a music piece in response to a short \"seed\" MIDI file that
    is given as input.\r\n\r\nThere are two special aspects of this challenge, apart
    from the extremely interesting application. First, the results of the models will
    be evaluated by humans, with an ELO-style system where volunteers are given two
    randomly paired pieces of generated music, and choose the one they like better.
    Second, **the top five models will at the end each generate a piece of music that
    will be performed live on stage at the Applied Machine Learning Days**!"
  description: |
    <p>Can AI generate music that we humans find beautiful, perhaps even moving? Let’s find out!</p>

    <p>In this challenge, participants are tasked to generate an AI model that learns on a large data set of music (in the form of MIDI files), and is then capable of producing its own music. Concretely, the model must produce a music piece in response to a short “seed” MIDI file that is given as input.</p>

    <p>There are two special aspects of this challenge, apart from the extremely interesting application. First, the results of the models will be evaluated by humans, with an ELO-style system where volunteers are given two randomly paired pieces of generated music, and choose the one they like better. Second, <strong>the top five models will at the end each generate a piece of music that will be performed live on stage at the Applied Machine Learning Days</strong>!</p>
  evaluation_markdown: "The grader expects a MIDI file of a total length of `3600`
    seconds (when played at `120 bpm`). The MIDI file has to be a `type 0` MIDI file
    (a maximum of 1 track), and in the case of multiple tracks, only the first track
    will be considered. There are no challenge-specific restrictions on the number
    of channels being used in the MIDI file.\r\nThe grader splits the MIDI file into
    `120 chunks` of approximately `30 seconds` each, and each submission is represented
    by this pool of 120 chunks.\r\nDuring this post-processing step, all meta events
    from the MIDI file will be removed except the PPQ meta event (or `ticks per beat`),
    hence the officially supported MIDI events will only be the `note_on` and `note_off`
    events; where `note_off` event can be optionally replaced by a `note_on` event
    with a _velocity_ of `0`. All the MIDI parsing is done using the [MIDO](https://mido.readthedocs.io/en/latest/)
    library; and you are requested to ensure that your submitted file is estimated
    to be of `3600 +/- 10 seconds` by `mido.MidiFile('your_file_path').length`.\r\n\r\nA
    separate [evaluation interface](https://www.crowdai.org/challenges/ai-generated-music-challenge/dynamic_contents)
    is made available, where all the participants (and other external volunteers)
    can hear two randomly sampled chunks and then vote for the one they like better
    (more details on the sampling mechanism is provided in the following sections).
    These randomly sampled chunks will be played with the [SoundFont of an acoustic
    grand piano](https://github.com/mudcube/MIDI.js/blob/master/examples/soundfont/acoustic_grand_piano-mp3.js)
    at `120 bpm`.\r\n\r\nThese binary comparisons will be used to compute an individual
    score for every submission, which evolves over time as it gets more and more evaluations
    in the evaluation interface. The scoring mechanism follows the [TrueSkill](https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/)
    ranking system, and hence is modeled by $ \\mu $ (a quantitative estimate of the
    preference of a general population towards a particular song) and $ \\sigma $
    (the confidence of the system in this estimate). The actual score on the leaderboard
    is computed by taking a conservative estimate of the modeled score, and hence
    is represented by :\r\n$ \\mu - k * \\sigma $\r\nwhere $k$ is the ratio of the
    default $\\mu$ and $\\sigma$ values and is represented by:  $(\\mu=25) / (\\sigma=8.334)$.\r\n\r\nThe
    [submissions tab](https://www.crowdai.org/challenges/ai-generated-music-challenge/submissions)
    will report the values for $\\mu$, $\\sigma$ and the number of evaluations completed
    for every submission; and the leaderboard will use the conservative estimate of
    $\\mu - k * \\sigma$ as the primary score, and $\\mu$ as the secondary score.\r\n\r\nTo
    ensure that the top-10 selected participants are not overfitting on the training
    set; **the top-10**  submissions at the end of the challenge, will be divided
    into quantized chunks of $\\tau(=5)$ seconds each (at 120 bpm) with a sliding
    window of stride $s$, and a normalised dynamic time warp (DTW) distance will be
    computed against $\\tau(=5)$  second chunks from all the MIDI files listed in
    the [Datasets](https://www.crowdai.org/challenges/ai-generated-music-challenge/dataset_files).
    \r\nWith $DTW(x, y)$ representing the DTW between two $\\tau(=5)$ second quantized
    chunks, the normalized DTW will be computed by :\r\n\r\n$NDTW(x,y) = \\frac{127
    \\times T(\\tau=5) - DTW(x,y) }{127 \\times T(\\tau=5)}$\r\n\r\nwhere,\r\n$T(\\tau)$
    represents the number of ticks in a time period of $\\tau$ seconds.\r\n\r\nAll
    matching chunks pairs with $NTDW < 0.3$ will be manually verified, and in case
    the chunks are found to be similar, then the submissions will be disqualified.\r\nGiven
    the subjective nature of the evaluation, the organisers will reserve the right
    to both adjust the threshold of $0.3$ and also to decide if the flagged chunks
    are indeed similar because of the model overfitting, or because of the said participant
    trying to cheat by stitching together MIDI snippets from the training data.\r\n\r\n\r\n**Starter
    Kit** : A starter kit to help you get started on the submission procedure is made
    available at: [https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit](https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit).
    \  \r\n   \r\n**Comin Soon** : A Getting Started guide on music generation from
    MIDI files using LSTMs."
  evaluation: "<p>The grader expects a MIDI file of a total length of <code>3600</code>
    seconds (when played at <code>120 bpm</code>). The MIDI file has to be a <code>type
    0</code> MIDI file (a maximum of 1 track), and in the case of multiple tracks,
    only the first track will be considered. There are no challenge-specific restrictions
    on the number of channels being used in the MIDI file.\nThe grader splits the
    MIDI file into <code>120 chunks</code> of approximately <code>30 seconds</code>
    each, and each submission is represented by this pool of 120 chunks.\nDuring this
    post-processing step, all meta events from the MIDI file will be removed except
    the PPQ meta event (or <code>ticks per beat</code>), hence the officially supported
    MIDI events will only be the <code>note_on</code> and <code>note_off</code> events;
    where <code>note_off</code> event can be optionally replaced by a <code>note_on</code>
    event with a <em>velocity</em> of <code>0</code>. All the MIDI parsing is done
    using the <a href=\"https://mido.readthedocs.io/en/latest/\">MIDO</a> library;
    and you are requested to ensure that your submitted file is estimated to be of
    <code>3600 +/- 10 seconds</code> by <code>mido.MidiFile('your_file_path').length</code>.</p>\n\n<p>A
    separate <a href=\"https://www.crowdai.org/challenges/ai-generated-music-challenge/dynamic_contents\">evaluation
    interface</a> is made available, where all the participants (and other external
    volunteers) can hear two randomly sampled chunks and then vote for the one they
    like better (more details on the sampling mechanism is provided in the following
    sections). These randomly sampled chunks will be played with the <a href=\"https://github.com/mudcube/MIDI.js/blob/master/examples/soundfont/acoustic_grand_piano-mp3.js\">SoundFont
    of an acoustic grand piano</a> at <code>120 bpm</code>.</p>\n\n<p>These binary
    comparisons will be used to compute an individual score for every submission,
    which evolves over time as it gets more and more evaluations in the evaluation
    interface. The scoring mechanism follows the <a href=\"https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/\">TrueSkill</a>
    ranking system, and hence is modeled by $ \\mu $ (a quantitative estimate of the
    preference of a general population towards a particular song) and $ \\sigma $
    (the confidence of the system in this estimate). The actual score on the leaderboard
    is computed by taking a conservative estimate of the modeled score, and hence
    is represented by :\n$ \\mu - k * \\sigma $\nwhere $k$ is the ratio of the default
    $\\mu$ and $\\sigma$ values and is represented by:  $(\\mu=25) / (\\sigma=8.334)$.</p>\n\n<p>The
    <a href=\"https://www.crowdai.org/challenges/ai-generated-music-challenge/submissions\">submissions
    tab</a> will report the values for $\\mu$, $\\sigma$ and the number of evaluations
    completed for every submission; and the leaderboard will use the conservative
    estimate of $\\mu - k * \\sigma$ as the primary score, and $\\mu$ as the secondary
    score.</p>\n\n<p>To ensure that the top-10 selected participants are not overfitting
    on the training set; <strong>the top-10</strong>  submissions at the end of the
    challenge, will be divided into quantized chunks of $\\tau(=5)$ seconds each (at
    120 bpm) with a sliding window of stride $s$, and a normalised dynamic time warp
    (DTW) distance will be computed against $\\tau(=5)$  second chunks from all the
    MIDI files listed in the <a href=\"https://www.crowdai.org/challenges/ai-generated-music-challenge/dataset_files\">Datasets</a>.
    \nWith $DTW(x, y)$ representing the DTW between two $\\tau(=5)$ second quantized
    chunks, the normalized DTW will be computed by :</p>\n\n<p>$NDTW(x,y) = \\frac{127
    \\times T(\\tau=5) - DTW(x,y) }{127 \\times T(\\tau=5)}$</p>\n\n<p>where,\n$T(\\tau)$
    represents the number of ticks in a time period of $\\tau$ seconds.</p>\n\n<p>All
    matching chunks pairs with $NTDW &lt; 0.3$ will be manually verified, and in case
    the chunks are found to be similar, then the submissions will be disqualified.\nGiven
    the subjective nature of the evaluation, the organisers will reserve the right
    to both adjust the threshold of $0.3$ and also to decide if the flagged chunks
    are indeed similar because of the model overfitting, or because of the said participant
    trying to cheat by stitching together MIDI snippets from the training data.</p>\n\n<p><strong>Starter
    Kit</strong> : A starter kit to help you get started on the submission procedure
    is made available at: <a href=\"https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit\">https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit</a>.</p>\n\n<p><strong>Comin
    Soon</strong> : A Getting Started guide on music generation from MIDI files using
    LSTMs.</p>\n"
  rules_markdown: "* Participants are allowed at most 2 submissions per day.\r\n*
    By uploading a submission, participants provide crowdai the right to host and
    play short clips of the submitted midi files publicly to human evaluators who
    may or may not be affiliated with crowdai.\r\n* Participants are not allowed to
    make submissions which are hand written, generated using custom rules, or recorded.\r\n*
    \ Participants are expected to release their final code using any [Open Source
    license](https://opensource.org/licenses) of their choice to be eligible for the
    prizes.\r\n* Organizers reserve the right to make changes to the rules"
  rules: |
    <ul>
      <li>Participants are allowed at most 2 submissions per day.</li>
      <li>By uploading a submission, participants provide crowdai the right to host and play short clips of the submitted midi files publicly to human evaluators who may or may not be affiliated with crowdai.</li>
      <li>Participants are not allowed to make submissions which are hand written, generated using custom rules, or recorded.</li>
      <li>Participants are expected to release their final code using any <a href="https://opensource.org/licenses">Open Source license</a> of their choice to be eligible for the prizes.</li>
      <li>Organizers reserve the right to make changes to the rules</li>
    </ul>
  prizes_markdown: The winner will be invited to the 3rd Applied Machine Learning
    Days at EPFL in Switzerland in January 2019, with travel and accommodation covered
    (up to $2000).
  prizes: "<p>The winner will be invited to the 3rd Applied Machine Learning Days
    at EPFL in Switzerland in January 2019, with travel and accommodation covered
    (up to $2000).</p>\n"
  resources_markdown: "**Starter Kit** : A starter kit to help you get started on
    the submission procedure is made available at : [https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit](https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit).
    \  \r\n   \r\nSome other projects to help you quickly get started on MIDI composition:
    \  \r\n\r\n* [https://github.com/brannondorsey/midi-rnn](https://github.com/brannondorsey/midi-rnn)\r\n*
    [https://github.com/jisungk/deepjazz](https://github.com/jisungk/deepjazz)\r\n*
    [Google Magenta : Performance RNN](https://magenta.tensorflow.org/performance-rnn)\r\n*
    [MIDINet](https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html)\r\n\r\n###
    Contact:\r\n\r\n* Technical issues : [https://gitter.im/crowdAI/AI-Generated-Music-Challenge](https://gitter.im/crowdAI/AI-Generated-Music-Challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/ai-generated-music-challenge/topics](https://www.crowdai.org/challenges/ai-generated-music-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  Sharada Prasanna Mohanty
    [sharada.mohanty@epfl.ch](mailto:sharada.mohanty@epfl.ch){:target='_blank'} \r\n*
    \ Florian Colombo [florian.colombo@epfl.ch](mailto:florian.colombo@epfl.ch){:target='_blank'}\r\n\r\n"
  resources: |+
    <p><strong>Starter Kit</strong> : A starter kit to help you get started on the submission procedure is made available at : <a href="https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit">https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit</a>.</p>

    <p>Some other projects to help you quickly get started on MIDI composition:</p>

    <ul>
      <li><a href="https://github.com/brannondorsey/midi-rnn">https://github.com/brannondorsey/midi-rnn</a></li>
      <li><a href="https://github.com/jisungk/deepjazz">https://github.com/jisungk/deepjazz</a></li>
      <li><a href="https://magenta.tensorflow.org/performance-rnn">Google Magenta : Performance RNN</a></li>
      <li><a href="https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html">MIDINet</a></li>
    </ul>

    <h3 id="contact">Contact:</h3>

    <ul>
      <li>Technical issues : <a href="https://gitter.im/crowdAI/AI-Generated-Music-Challenge">https://gitter.im/crowdAI/AI-Generated-Music-Challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/ai-generated-music-challenge/topics">https://www.crowdai.org/challenges/ai-generated-music-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Sharada Prasanna Mohanty <a href="mailto:sharada.mohanty@epfl.ch" target="_blank">sharada.mohanty@epfl.ch</a></li>
      <li>Florian Colombo <a href="mailto:florian.colombo@epfl.ch" target="_blank">florian.colombo@epfl.ch</a></li>
    </ul>

  submission_instructions_markdown: abcd
  submission_instructions: "<p>abcd</p>\n"
  license_markdown: abcd
  license: "<p>abcd</p>\n"
  dataset_description_markdown: abcd
  dataset_description: "<p>abcd</p>\n"
  image_file: Screen_Shot_2017-11-04_at_10.32.55.png
  featured_sequence: 4
  dynamic_content_flag: false
  dynamic_content: ''
  dynamic_content_tab: Evaluate Music
  winner_description_markdown: "### AI-generated music challenge 2017 Results:\r\n<br>\r\n\r\n8
    submitted music pieces were [played live on stage](https://www.youtube.com/watch?v=c_gj3VV0VbY&feature=youtu.be)
    at [AMLD2018](https://www.appliedmldays.org/) by the [EPFL and Unil Student Chamber
    Orchestra](https://oche.epfl.ch/). The most popular (by public vote) music piece
    was a generation from [Qin_Yongliang](https://www.crowdai.org/participants/qin_yongliang)
    algorithm. He has won an invitation to Applied Machine Learning Days 2019."
  winner_description: |
    <h3 id="ai-generated-music-challenge-2017-results">AI-generated music challenge 2017 Results:</h3>
    <p><br /></p>

    <p>8 submitted music pieces were <a href="https://www.youtube.com/watch?v=c_gj3VV0VbY&amp;feature=youtu.be">played live on stage</a> at <a href="https://www.appliedmldays.org/">AMLD2018</a> by the <a href="https://oche.epfl.ch/">EPFL and Unil Student Chamber Orchestra</a>. The most popular (by public vote) music piece was a generation from <a href="https://www.crowdai.org/participants/qin_yongliang">Qin_Yongliang</a> algorithm. He has won an invitation to Applied Machine Learning Days 2019.</p>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_8:
  id: 8
  organizer_id: 4
  challenge: 'NIPS 2017: Learning to Run'
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &18 2017-04-06 09:47:11.933323000 Z
    zone: *2
    time: *18
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &19 2018-10-04 11:06:30.149970000 Z
    zone: *2
    time: *19
  tagline: Reinforcement learning environments with musculoskeletal models
  primary_sort_order_cd: descending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 81149
  participant_count: 607
  submission_count: 2154
  score_title: Mean Reward per Simulation
  score_secondary_title: ''
  slug: nips-2017-learning-to-run
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: true
  challenge_client_name: Learning2RunChallengeNIPS2017
  online_grading: true
  vote_count: 225
  description_markdown: "**THIS CHALLENGE IS OVER, BUT THERE IS [A NEW ONE AT NIPS
    2018](https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge)**\r\n\r\nUpdates
    for participants: **Please read about the latest changes and the logistics of
    the second round [here](https://github.com/stanfordnmbl/osim-rl/tree/master/docs)
    and [here](https://www.crowdai.org/topics/important-announcement-round-2-submission/discussion)
    and [here](https://www.crowdai.org/topics/important-announcement-round-2-updates-submission-screencast/discussion)(last
    update November 6th).**\r\n\r\nWelcome to **Learning to Run**, one of the 5 official
    challenges in the [NIPS 2017 Competition Track](https://nips.cc/Conferences/2017/CompetitionTrack).
    In this competition, you are tasked with developing a controller to enable a physiologically-based
    human model to navigate a complex obstacle course as quickly as possible. You
    are provided with a human musculoskeletal model and a physics-based simulation
    environment where you can synthesize physically and physiologically accurate motion.
    Potential obstacles include external obstacles like steps, or a slippery floor,
    along with internal obstacles like muscle weakness or motor noise. You are scored
    based on the distance you travel through the obstacle course in a set amount of
    time.\r\n\r\nOur objectives are to:\r\n\r\n* bring Deep Reinforcement Learning
    to solve problems in medicine,\r\n* promote open-source tools in RL research (the
    physics simulator, the RL environment, and the competition platform are all open-source),\r\n*
    encourage RL research in computationally complex environments, with stochasticity
    and highly-dimensional action spaces.\r\n\r\nFollow the instructions in the [Getting
    Started guide in the Dataset section of the challenge](https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files)
    and visit our [github repo](https://github.com/stanfordnmbl/osim-rl) to get started!\r\n\r\n**First
    Prize -- NVIDIA DGX Station™**\r\n\r\n[![NVIDIA Station](https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png){:width=\"240px\"}](https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png)\r\n\r\nNVIDIA
    DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists.\r\n\r\n**Computing
    support -- Amazon AWS cloud credits**\r\n\r\n**Amazon AWS has generously agreed
    to support participants of the challenge with $30,000 worth of cloud credits.**
    The top 100 performers as per the leaderboard on August 13th, 2017, 23:59:59 UTC,
    received $300 AWS cloud credits.\r\n\r\n## Partners\r\n\r\n![stanford](https://s3.amazonaws.com/salathegroup-static/nips/logos/Stanford.png){:class='img-logo'}\r\n![epfl](https://s3.amazonaws.com/salathegroup-static/nips/logos/epfl.png){:class='img-logo'}\r\n![berkley](https://s3.amazonaws.com/salathegroup-static/nips/logos/Berkeley.png){:class='img-logo'}\r\n[![stanford
    mobilize](https://s3.amazonaws.com/salathegroup-static/nips/logos/mobilize.png){:class='img-logo'}](http://mobilize.stanford.edu/)\r\n\r\n\r\n##
    Sponsors\r\n\r\n[![Amazon AWS](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/AmazonWebservices_Logo.svg/2000px-AmazonWebservices_Logo.svg.png){:class='img-logo'}](https://aws.amazon.com/)\r\n[![NVIDIA](https://vignette1.wikia.nocookie.net/logopedia/images/3/38/Nvidia_logo.png/revision/latest?cb=20120829072950){:class='img-logo'}](https://nvidia.com/)\r\n[![TRI](https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/tri1.png){:class='img-logo'}](http://www.tri.global/)\r\n\r\n\r\n##
    Media\r\n\r\n[![TechCrunch](https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png){:class='img-logo'}](https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/)\r\n[![Stanford
    News](https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW){:class='img-logo'}](http://news.stanford.edu/2017/08/07/virtual-competitors-vie-different-kind-athletic-title/)\r\n[![IEEE](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png){:class='img-logo'}](http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy)\r\n"
  description: |
    <p><strong>THIS CHALLENGE IS OVER, BUT THERE IS <a href="https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge">A NEW ONE AT NIPS 2018</a></strong></p>

    <p>Updates for participants: <strong>Please read about the latest changes and the logistics of the second round <a href="https://github.com/stanfordnmbl/osim-rl/tree/master/docs">here</a> and <a href="https://www.crowdai.org/topics/important-announcement-round-2-submission/discussion">here</a> and <a href="https://www.crowdai.org/topics/important-announcement-round-2-updates-submission-screencast/discussion">here</a>(last update November 6th).</strong></p>

    <p>Welcome to <strong>Learning to Run</strong>, one of the 5 official challenges in the <a href="https://nips.cc/Conferences/2017/CompetitionTrack">NIPS 2017 Competition Track</a>. In this competition, you are tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible. You are provided with a human musculoskeletal model and a physics-based simulation environment where you can synthesize physically and physiologically accurate motion. Potential obstacles include external obstacles like steps, or a slippery floor, along with internal obstacles like muscle weakness or motor noise. You are scored based on the distance you travel through the obstacle course in a set amount of time.</p>

    <p>Our objectives are to:</p>

    <ul>
      <li>bring Deep Reinforcement Learning to solve problems in medicine,</li>
      <li>promote open-source tools in RL research (the physics simulator, the RL environment, and the competition platform are all open-source),</li>
      <li>encourage RL research in computationally complex environments, with stochasticity and highly-dimensional action spaces.</li>
    </ul>

    <p>Follow the instructions in the <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files">Getting Started guide in the Dataset section of the challenge</a> and visit our <a href="https://github.com/stanfordnmbl/osim-rl">github repo</a> to get started!</p>

    <p><strong>First Prize – NVIDIA DGX Station™</strong></p>

    <p><a href="https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png"><img src="https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png" alt="NVIDIA Station" width="240px" /></a></p>

    <p>NVIDIA DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists.</p>

    <p><strong>Computing support – Amazon AWS cloud credits</strong></p>

    <p><strong>Amazon AWS has generously agreed to support participants of the challenge with $30,000 worth of cloud credits.</strong> The top 100 performers as per the leaderboard on August 13th, 2017, 23:59:59 UTC, received $300 AWS cloud credits.</p>

    <h2 id="partners">Partners</h2>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/nips/logos/Stanford.png" alt="stanford" class="img-logo" />
    <img src="https://s3.amazonaws.com/salathegroup-static/nips/logos/epfl.png" alt="epfl" class="img-logo" />
    <img src="https://s3.amazonaws.com/salathegroup-static/nips/logos/Berkeley.png" alt="berkley" class="img-logo" />
    <a href="http://mobilize.stanford.edu/"><img src="https://s3.amazonaws.com/salathegroup-static/nips/logos/mobilize.png" alt="stanford mobilize" class="img-logo" /></a></p>

    <h2 id="sponsors">Sponsors</h2>

    <p><a href="https://aws.amazon.com/"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/AmazonWebservices_Logo.svg/2000px-AmazonWebservices_Logo.svg.png" alt="Amazon AWS" class="img-logo" /></a>
    <a href="https://nvidia.com/"><img src="https://vignette1.wikia.nocookie.net/logopedia/images/3/38/Nvidia_logo.png/revision/latest?cb=20120829072950" alt="NVIDIA" class="img-logo" /></a>
    <a href="http://www.tri.global/"><img src="https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/tri1.png" alt="TRI" class="img-logo" /></a></p>

    <h2 id="media">Media</h2>

    <p><a href="https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/"><img src="https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png" alt="TechCrunch" class="img-logo" /></a>
    <a href="http://news.stanford.edu/2017/08/07/virtual-competitors-vie-different-kind-athletic-title/"><img src="https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW" alt="Stanford News" class="img-logo" /></a>
    <a href="http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy"><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png" alt="IEEE" class="img-logo" /></a></p>
  evaluation_markdown: Your task is to build a function f which takes the current
    state observation (a 41 dimensional vector) and returns the muscle excitations
    action (18 dimensional vector) in a way that maximizes the reward. Your total
    reward is the position of the pelvis on the x axis after the last iteration minus
    a penalty for using ligament forces. Ligaments are tissues which prevent your
    joints from bending too much - overusing these tissues leads to injuries, so we
    want to avoid it. The penalty in the total reward is equal to the sum of forces
    generated by ligaments over the trial, divided by 1000. For details on evaluation
    please refer to the [Getting Started guide in the Dataset section of the challenge](https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files).
  evaluation: '<p>Your task is to build a function f which takes the current state
    observation (a 41 dimensional vector) and returns the muscle excitations action
    (18 dimensional vector) in a way that maximizes the reward. Your total reward
    is the position of the pelvis on the x axis after the last iteration minus a penalty
    for using ligament forces. Ligaments are tissues which prevent your joints from
    bending too much - overusing these tissues leads to injuries, so we want to avoid
    it. The penalty in the total reward is equal to the sum of forces generated by
    ligaments over the trial, divided by 1000. For details on evaluation please refer
    to the <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files">Getting
    Started guide in the Dataset section of the challenge</a>.</p>

'
  rules_markdown: "In order to avoid overfitting to the training environment, the
    participants with score > 15 will be asked to resubmit their solutions in the
    second round of the challenge. The final ranking will be based on results from
    the second round.\r\n\r\nRound 2 Rules:\r\n\r\n* **1)** All eligible participants/teams
    are allowed **5 successful submissions** (instead of 3) and upto **2 failed submissions**\r\n*
    **2)** The submitted container will not have access to external network when being
    graded\r\n* **3)** Each submission container is allowed to use a maximum memory
    of **5GB**\r\n* **4)** Each submission will be evaluated for atleast `N=10` simulations.
    \r\n* **4.5)** In case of a tie between the top-2 participants, we will re-run
    their submissions with `N=20` and the new scores will be used as a tie breaker.
    \r\n* **5)** Timeout for a submission is **8hours**. In case of `N > 10`, the
    timeout will be proportionally increased.\r\n* **6)** Each team can use only one
    account in the second round\r\n* **7)** A team with two or more accounts accepted
    in the second round is obliged to report this issue to the organizers immediately,
    before submitting any solution\r\n* **7.5)** To be eligible for the prize as a
    team, the combined submissions from the accounts of all team members in round
    2 has to be less than or equal to upto 5 successful submissions (+2 failed submissions).\r\n*
    **8)** The winners will be asked to release the code and the trained models of
    the solution\r\n* **9)** Violation of the rules or other unfair activity may result
    in disqualification for the prizes\r\n\r\n\r\nAdditional rules:\r\n\r\n* You are
    not allowed to use external datasets (e.g., kinematics of people walking),\r\n*
    NVIDIA teams are not elligible for the first prize,\r\n* Organizers reserve the
    right to modify challenge rules as required."
  rules: |
    <p>In order to avoid overfitting to the training environment, the participants with score &gt; 15 will be asked to resubmit their solutions in the second round of the challenge. The final ranking will be based on results from the second round.</p>

    <p>Round 2 Rules:</p>

    <ul>
      <li><strong>1)</strong> All eligible participants/teams are allowed <strong>5 successful submissions</strong> (instead of 3) and upto <strong>2 failed submissions</strong></li>
      <li><strong>2)</strong> The submitted container will not have access to external network when being graded</li>
      <li><strong>3)</strong> Each submission container is allowed to use a maximum memory of <strong>5GB</strong></li>
      <li><strong>4)</strong> Each submission will be evaluated for atleast <code>N=10</code> simulations.</li>
      <li><strong>4.5)</strong> In case of a tie between the top-2 participants, we will re-run their submissions with <code>N=20</code> and the new scores will be used as a tie breaker.</li>
      <li><strong>5)</strong> Timeout for a submission is <strong>8hours</strong>. In case of <code>N &gt; 10</code>, the timeout will be proportionally increased.</li>
      <li><strong>6)</strong> Each team can use only one account in the second round</li>
      <li><strong>7)</strong> A team with two or more accounts accepted in the second round is obliged to report this issue to the organizers immediately, before submitting any solution</li>
      <li><strong>7.5)</strong> To be eligible for the prize as a team, the combined submissions from the accounts of all team members in round 2 has to be less than or equal to upto 5 successful submissions (+2 failed submissions).</li>
      <li><strong>8)</strong> The winners will be asked to release the code and the trained models of the solution</li>
      <li><strong>9)</strong> Violation of the rules or other unfair activity may result in disqualification for the prizes</li>
    </ul>

    <p>Additional rules:</p>

    <ul>
      <li>You are not allowed to use external datasets (e.g., kinematics of people walking),</li>
      <li>NVIDIA teams are not elligible for the first prize,</li>
      <li>Organizers reserve the right to modify challenge rules as required.</li>
    </ul>
  prizes_markdown: "1st - NVIDIA DGX Station™\r\n\r\n2nd - NVIDIA Titan Xp\r\n\r\n3rd
    - NVIDIA Titan Xp\r\n\r\nAdditionally:\r\n\r\n* Invitation to publish articles
    in the NIPS competition book.\r\n* Invitation to the 2nd [Applied Machine Learning
    Days](https://www.appliedmldays.org) at EPFL in Switzerland on January 29 & 30,
    2018, with travel and accommodation covered.\r\n* Invitation to give a research
    talk at Stanford, with travel and accommodation covered.\r\n* Reimbursement of
    travel and accommodation at NIPS 2017\r\n\r\n[![NVIDIA Station](https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png){:width=\"240px\"}]\r\n\r\nNVIDIA
    DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists”
    with the following benefits:\r\n\r\n* Revolutionary form factor - designed for
    the desk, whisper-quiet\r\n* Start experimenting in hours, not weeks, powered
    by DGX Stack\r\n* Productivity that goes from desk  to data center to cloud\r\n*
    Breakthrough performance and precision – powered by Volta"
  prizes: |
    <p>1st - NVIDIA DGX Station™</p>

    <p>2nd - NVIDIA Titan Xp</p>

    <p>3rd - NVIDIA Titan Xp</p>

    <p>Additionally:</p>

    <ul>
      <li>Invitation to publish articles in the NIPS competition book.</li>
      <li>Invitation to the 2nd <a href="https://www.appliedmldays.org">Applied Machine Learning Days</a> at EPFL in Switzerland on January 29 &amp; 30, 2018, with travel and accommodation covered.</li>
      <li>Invitation to give a research talk at Stanford, with travel and accommodation covered.</li>
      <li>Reimbursement of travel and accommodation at NIPS 2017</li>
    </ul>

    <p>[<img src="https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png" alt="NVIDIA Station" width="240px" />]</p>

    <p>NVIDIA DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists” with the following benefits:</p>

    <ul>
      <li>Revolutionary form factor - designed for the desk, whisper-quiet</li>
      <li>Start experimenting in hours, not weeks, powered by DGX Stack</li>
      <li>Productivity that goes from desk  to data center to cloud</li>
      <li>Breakthrough performance and precision – powered by Volta</li>
    </ul>
  resources_markdown: "Please refer to the _Getting Started_ guide in the Dataset
    section of the challenge, for more details on how to access the challenge environments,
    and also for a basic tutorial on how to make your first submission. \r\n\r\nWe
    are in the process of compiling the book chapter for the Book on the NIPS Challenge
    Track this year. But in the meantime, here are some interesting articles and blog
    posts written by participants : \r\n\r\n* [https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5](https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5)\r\n*
    [https://arxiv.org/abs/1711.06922](https://arxiv.org/abs/1711.06922)\r\n* [https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8](https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8)\r\n\r\n\r\n\r\n##
    Contact Us\r\n\r\n* Gitter Channel : [crowdAI/NIPS-Learning-To-Run-Challenge](https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge)\r\n*
    Technical issues : [https://github.com/stanfordnmbl/osim-rl/issues ](https://github.com/stanfordnmbl/osim-rl/issues){:target='_blank'}\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics](https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organisers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  [ lukasz.kidzinski@stanford.edu\r\n
    ](mailto:lukasz.kidzinski@stanford.edu\r\n){:target='_blank'}\r\n*  [ sharada.mohanty@epfl.ch\r\n
    ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <p>Please refer to the <em>Getting Started</em> guide in the Dataset section of the challenge, for more details on how to access the challenge environments, and also for a basic tutorial on how to make your first submission.</p>

    <p>We are in the process of compiling the book chapter for the Book on the NIPS Challenge Track this year. But in the meantime, here are some interesting articles and blog posts written by participants :</p>

    <ul>
      <li><a href="https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5">https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5</a></li>
      <li><a href="https://arxiv.org/abs/1711.06922">https://arxiv.org/abs/1711.06922</a></li>
      <li><a href="https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8">https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8</a></li>
    </ul>

    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge">crowdAI/NIPS-Learning-To-Run-Challenge</a></li>
      <li>Technical issues : <a href="https://github.com/stanfordnmbl/osim-rl/issues" target="_blank">https://github.com/stanfordnmbl/osim-rl/issues </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics">https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:lukasz.kidzinski@stanford.edu" target="_blank"> lukasz.kidzinski@stanford.edu
     </a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: ''
  license_markdown: ''
  license: ''
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: 8.Screen_Shot_2017-04-10_at_1.23.56_PM.png
  featured_sequence: 4
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### NIPS 2017: Learning to Run Challenge Results\r\n<br><br>\r\n\r\n|
    Rank | Participant | Prize |\r\n| **1.** | [NNAISENSE](https://www.crowdai.org/participants/nnaisense)
    | Invitation to [AMLD2018](https://www.appliedmldays.org/)<br>Invitation to [NIPS2017
    Challenge Track workshop](https://nips.cc/Conferences/2017/Schedule?showEvent=8748)<br>1
    [NIVIDIA DGX Station](https://www.nvidia.com/en-us/data-center/dgx-station/)<br><br>![NVIDIA
    Station](https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png){:width=\"300px\"}
    |\r\n| **2.** | [Megvii-hzwer](https://www.crowdai.org/participants/pku-hzwer)
    | 1 [NIVIDIA TITAN Xp](https://www.nvidia.com/en-us/titan/titan-xp/) |\r\n| **3.**
    | [reason8.ai](https://www.crowdai.org/participants/reason8-ai) | 1 [NIVIDIA TITAN
    Xp](https://www.nvidia.com/en-us/titan/titan-xp/) |\r\n\r\n<br>\r\nSpecial mention
    for [Qin_Yongliang](https://www.crowdai.org/participants/qin-yongliang) for helping
    other participants getting started in the challenge (introductory blog posts,
    release his own code). As recognition, he received an invitation to [AMLD2018](https://www.appliedmldays.org/)."
  winner_description: |
    <h3 id="nips-2017-learning-to-run-challenge-results">NIPS 2017: Learning to Run Challenge Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/nnaisense">NNAISENSE</a></td>
          <td>Invitation to <a href="https://www.appliedmldays.org/">AMLD2018</a><br />Invitation to <a href="https://nips.cc/Conferences/2017/Schedule?showEvent=8748">NIPS2017 Challenge Track workshop</a><br />1 <a href="https://www.nvidia.com/en-us/data-center/dgx-station/">NIVIDIA DGX Station</a><br /><br /><img src="https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png" alt="NVIDIA Station" width="300px" /></td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/pku-hzwer">Megvii-hzwer</a></td>
          <td>1 <a href="https://www.nvidia.com/en-us/titan/titan-xp/">NIVIDIA TITAN Xp</a></td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/reason8-ai">reason8.ai</a></td>
          <td>1 <a href="https://www.nvidia.com/en-us/titan/titan-xp/">NIVIDIA TITAN Xp</a></td>
        </tr>
      </tbody>
    </table>

    <p><br />
    Special mention for <a href="https://www.crowdai.org/participants/qin-yongliang">Qin_Yongliang</a> for helping other participants getting started in the challenge (introductory blog posts, release his own code). As recognition, he received an invitation to <a href="https://www.appliedmldays.org/">AMLD2018</a>.</p>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_2:
  id: 2
  organizer_id: 2
  challenge: Dark Skies - Classification of Nighttime Images
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &20 2016-05-27 10:26:12.175885000 Z
    zone: *2
    time: *20
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &21 2018-10-04 09:10:05.039360000 Z
    zone: *2
    time: *21
  tagline: Classification of Nighttime Images from the International Space Station
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: answer_files/dark_skies_solution_file.csv
  page_views: 6214
  participant_count: 103
  submission_count: 22
  score_title: Mean F1
  score_secondary_title: Mean Log Loss
  slug: dark-skies-classification-of-nighttime-images
  submission_license: ''
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: dark-skies-classification-of-nighttime-images
  online_grading: true
  vote_count: 4
  description_markdown: "The International Space Station (ISS) has taken millions
    of images of Earth. About 30% of those images were taken at night. These  photograph
    are the highest-resolution night imagery available from orbit. [For technical
    reasons](http://www.nasa.gov/mission_pages/station/research/news/crowdsourcing_night_images),
    we don't precisely know what the camera was pointing at when taking the images.
    However, if we knew exactly what these images were showing (stars, cities, auroras,
    etc.), we could use this wealth of information to address issues such as night
    pollution, population movement, energy usage, etc. \r\n\r\nUntil recently, it
    has been thought that algorithms cannot distinguish between stars, cities, and
    other objects. This challenge tries to prove otherwise. Thousands of volunteers
    have hand-labeled tens of thousands of images as part of a [citizen science project](http://crowdcrafting.org/project/darkskies/).
    The goal of this challenge is to build on this manually evaluated data set, and
    develop an image classification algorithm that can correctly identify whether
    an image shows stars, cities, or other objects.\r\n\r\n![]( https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/783da80339b677a621a28fc9c8649b64_spain_portugalpsp.jpg)\r\n_The
    Iberian Peninsula at night, showing Spain and Portugal. Madrid is the bright spot
    just above the center. Credits: NASA_"
  description: |
    <p>The International Space Station (ISS) has taken millions of images of Earth. About 30% of those images were taken at night. These  photograph are the highest-resolution night imagery available from orbit. <a href="http://www.nasa.gov/mission_pages/station/research/news/crowdsourcing_night_images">For technical reasons</a>, we don’t precisely know what the camera was pointing at when taking the images. However, if we knew exactly what these images were showing (stars, cities, auroras, etc.), we could use this wealth of information to address issues such as night pollution, population movement, energy usage, etc.</p>

    <p>Until recently, it has been thought that algorithms cannot distinguish between stars, cities, and other objects. This challenge tries to prove otherwise. Thousands of volunteers have hand-labeled tens of thousands of images as part of a <a href="http://crowdcrafting.org/project/darkskies/">citizen science project</a>. The goal of this challenge is to build on this manually evaluated data set, and develop an image classification algorithm that can correctly identify whether an image shows stars, cities, or other objects.</p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/783da80339b677a621a28fc9c8649b64_spain_portugalpsp.jpg" alt="" />
    <em>The Iberian Peninsula at night, showing Spain and Portugal. Madrid is the bright spot just above the center. Credits: NASA</em></p>
  evaluation_markdown: "Submissions will be evaluated using a Multi Class Log Loss
    evaluation function, which are defined as :\r\n\r\n### Mean F1 score   \r\n      \r\nThe
    F1 score is computed separately for all classes by using:\r\n\r\n![](https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png){:class='img-medium'}\r\n\r\n*
    **_p_** refers to the precision\r\n* **_r_** refers to the recall\r\n* **_tp_**
    refers to the number of true positives, \r\n* **_fp_** refers to the number of
    false positives\r\n* **_fn_** refers to the number of false negatives\r\n\r\nThen
    finally the mean of all the F1 scores across all the classes is used for come
    up with the combined mean F1 score.   \r\n   \r\n\r\n### Mean Log Loss   \r\n\r\n![](https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png){:class='img-medium'}\r\n\r\n\r\n*
    **_N_** is the total number of examples in the test set\r\n* **_M_** is the total
    number of class labels (7 for  this challenge)\r\n* **_y ij_** is a boolean value
    representing if the i-th instance in the test set belongs to the j-th label.\r\n*
    **_p ij_** is the probability according to your submission that the i-th instance
    may belong to the j-th label.\r\n* **_Ln_** is the natural logarithmic function.\r\n\r\n\r\nAll
    submissions will be evaluated on the test dataset in the docker containers referenced
    in the Resources section. The code archive will be uncompressed into the ```/darkskies```
    path, and every code archive is expected to contain a ```main.sh``` script which
    takes path to a folder containing images as its first parameter. So to test your
    code submission, we will finally execute : \r\n\r\n```/darkskies/main.sh pathToFolderContainingTestImages```\r\n\r\nThis
    is expected to output a CSV file containing the name of the file, and the associated
    probabilities for all the classes at the location :\r\n\r\n```/darkskies/classification.csv```\r\n"
  evaluation: |
    <p>Submissions will be evaluated using a Multi Class Log Loss evaluation function, which are defined as :</p>

    <h3 id="mean-f1-score">Mean F1 score</h3>

    <p>The F1 score is computed separately for all classes by using:</p>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png" alt="" class="img-medium" /></p>

    <ul>
      <li><strong><em>p</em></strong> refers to the precision</li>
      <li><strong><em>r</em></strong> refers to the recall</li>
      <li><strong><em>tp</em></strong> refers to the number of true positives,</li>
      <li><strong><em>fp</em></strong> refers to the number of false positives</li>
      <li><strong><em>fn</em></strong> refers to the number of false negatives</li>
    </ul>

    <p>Then finally the mean of all the F1 scores across all the classes is used for come up with the combined mean F1 score.</p>

    <h3 id="mean-log-loss">Mean Log Loss</h3>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png" alt="" class="img-medium" /></p>

    <ul>
      <li><strong><em>N</em></strong> is the total number of examples in the test set</li>
      <li><strong><em>M</em></strong> is the total number of class labels (7 for  this challenge)</li>
      <li><strong><em>y ij</em></strong> is a boolean value representing if the i-th instance in the test set belongs to the j-th label.</li>
      <li><strong><em>p ij</em></strong> is the probability according to your submission that the i-th instance may belong to the j-th label.</li>
      <li><strong><em>Ln</em></strong> is the natural logarithmic function.</li>
    </ul>

    <p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code>/darkskies</code> path, and every code archive is expected to contain a <code>main.sh</code> script which takes path to a folder containing images as its first parameter. So to test your code submission, we will finally execute :</p>

    <p><code>/darkskies/main.sh pathToFolderContainingTestImages</code></p>

    <p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

    <p><code>/darkskies/classification.csv</code></p>
  rules_markdown: "* Participants are allowed a maximum of five submissions each 24
    hours.\r\n* crowdAI reserves the right to modify challenge rules as required."
  rules: |
    <ul>
    <li>Participants are allowed a maximum of five submissions each 24 hours.</li>
    <li>crowdAI reserves the right to modify challenge rules as required.</li>
    </ul>
  prizes_markdown: The author of the most highly ranked submission will be invited
    to the **crowdAI winner's symposium** at EPFL in Switzerland on January 30/31,
    2017. The educational award is given to the participant with the either the most
    insightful submission posts, or the best tutorial - the recipient of this award
    will also be invited to the symposium (the crowdAI team will pick the recipient
    of this award). Expenses for travel and accommodation are covered by crowdAI.
  prizes: "<p>The author of the most highly ranked submission will be invited to the
    <strong>crowdAI winner&#39;s symposium</strong> at EPFL in Switzerland on January
    30/31, 2017. The educational award is given to the participant with the either
    the most insightful submission posts, or the best tutorial - the recipient of
    this award will also be invited to the symposium (the crowdAI team will pick the
    recipient of this award). Expenses for travel and accommodation are covered by
    crowdAI.</p>\n"
  resources_markdown: "The challenge entries may be made using any of the frameworks
    listed below. Selected entries will be tested using the linked Docker containers,
    which are the standard development environments for the challenge.\r\n\r\n**Caffe**
    \ : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/\r\n**Tensorflow** :
    https://hub.docker.com/r/tensorflow/tensorflow/\r\n**Torch7** : https://hub.docker.com/r/kaixhin/cuda-torch/\r\n**Scikit-Learn**
    :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2\r\n**Scikit-Learn**
    : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3\r\n**Octave**
    : https://hub.docker.com/r/schickling/octave/\r\n**Keras** :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/\r\n\r\nYou
    can submit a question in the [forum](https://www.crowdai.org/challenges/2/topics)
    should you need help with any of these containers or frameworks. We are here to
    help you learn!"
  resources: "The challenge entries may be made using any of the frameworks listed
    below. Selected entries will be tested using the linked Docker containers, which
    are the standard development environments for the challenge.\r\n\r\n**Caffe**
    \ : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/\r\n**Tensorflow** :
    https://hub.docker.com/r/tensorflow/tensorflow/\r\n**Torch7** : https://hub.docker.com/r/kaixhin/cuda-torch/\r\n**Scikit-Learn**
    :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2\r\n**Scikit-Learn**
    : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3\r\n**Octave**
    : https://hub.docker.com/r/schickling/octave/\r\n**Keras** :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/\r\n\r\nYou
    can submit a question in the [forum](https://www.crowdai.org/challenges/2/topics)
    should you need help with any of these containers or frameworks. We are here to
    help you learn!"
  submission_instructions_markdown: ''
  submission_instructions: ''
  license_markdown: "\r\nThe images used in this challenge have been made available
    by the **Earth Science and Remote Sensing Unit, NASA Johnson Space Center**, and
    the original files can be found on the NASA website [link](http://eol.jsc.nasa.gov).\r\nThe
    images used in this challenge were labelled by a hugely successful [crowdsourcing
    challenge](http://crowdcrafting.org/project/darkskies/) by the [Cities@Night](www.citiesatnight.org)
    project on [CrowdCrafting](http://crowdcrafting.org/), where ~19000 people from
    all over the world came together to manually classify ~170000 images.\r\n\r\nFurther
    conditions surrounding the use of these images may be found [here](http://eol.jsc.nasa.gov/FAQ/default.htm#terms).\r\n\r\n"
  license: |
    <p>The images used in this challenge have been made available by the <strong>Earth Science and Remote Sensing Unit, NASA Johnson Space Center</strong>, and the original files can be found on the NASA website <a href="http://eol.jsc.nasa.gov" rel="nofollow" target="_blank">link</a>.<br>
    The images used in this challenge were labelled by a hugely successful <a href="http://crowdcrafting.org/project/darkskies/" rel="nofollow" target="_blank">crowdsourcing challenge</a> by the <a href="www.citiesatnight.org" rel="nofollow" target="_blank">Cities@Night</a> project on <a href="http://crowdcrafting.org/" rel="nofollow" target="_blank">CrowdCrafting</a>, where ~19000 people from all over the world came together to manually classify ~170000 images.</p>

    <p>Further conditions surrounding the use of these images may be found <a href="http://eol.jsc.nasa.gov/FAQ/default.htm#terms" rel="nofollow" target="_blank">here</a>.</p>
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: 2.Screen_Shot_2016-05-30_at_8.37.39_PM.png
  featured_sequence: 1
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### Dark Skies - Classification of Nighttime Images
    Results\r\n<br><br>\r\n\r\n| Rank | Participant | Prize |\r\n| **1.** | [Sebastien_Boyer](https://www.crowdai.org/participants/sebastien_boyer)
    | Invitation to [AMLD2017](https://www.appliedmldays.org/2017) |\r\n| **2.** |
    [preethamsp](https://www.crowdai.org/participants/preethamsp) |  |\r\n| **3.**
    | [LiberiFatali](https://www.crowdai.org/participants/liberifatali) |  |"
  winner_description: |
    <h3 id="dark-skies---classification-of-nighttime-images-results">Dark Skies - Classification of Nighttime Images Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/sebastien_boyer">Sebastien_Boyer</a></td>
          <td>Invitation to <a href="https://www.appliedmldays.org/2017">AMLD2017</a></td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/preethamsp">preethamsp</a></td>
          <td> </td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/liberifatali">LiberiFatali</a></td>
          <td> </td>
        </tr>
      </tbody>
    </table>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: 
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_1:
  id: 1
  organizer_id: 1
  challenge: PlantVillage Disease Classification Challenge
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &22 2016-04-11 16:51:55.291585000 Z
    zone: *2
    time: *22
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &23 2018-10-04 11:07:30.969458000 Z
    zone: *2
    time: *23
  tagline: PlantVillage is built on the premise that all knowledge that helps people
    grow food should be openly accessible to anyone on the planet.
  primary_sort_order_cd: descending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: answer_files/plant_village_answers.csv
  page_views: 26643
  participant_count: 951
  submission_count: 36
  score_title: Mean F1
  score_secondary_title: Mean Log Loss
  slug: plantvillage-disease-classification-challenge
  submission_license: 
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: plantvillage-disease-classification-challenge
  online_grading: true
  vote_count: 26
  description_markdown: "We depend on edible plants just as we depend on oxygen. Without
    crops, there is no food, and without food, there is no life. It's no accident
    that human civilization began to thrive with the invention of agriculture.\r\n\r\nToday,
    modern technology allows us to grow crops in quantities necessary for a steady
    food supply for billions of people. But diseases remain a major threat to this
    supply, and a large fraction of crops are lost each year to diseases. The situation
    is particularly dire for the 500 million smallholder farmers around the globe,
    whose livelihoods depend on their crops doing well. In Africa alone, 80% of the
    agricultural output comes from smallholder farmers.\r\n\r\nWith billions of smartphones
    around the globe, wouldn't it be great if the smartphone could be turned into
    a disease diagnostics tool, recognizing diseases from images it captures with
    its camera? This challenge is the first of many steps turning this vision into
    a reality. PlantVillage is a not-for-profit project by Penn State University in
    the US and EPFL in Switzerland. We have collected - and continue to collect -
    tens of thousands of images of diseased and healthy crops. The goal of this challenge
    is to develop algorithms than can accurately diagnose a disease based on an image.\r\n\r\nHere
    are the 38 classes of crop disease pairs that the dataset is offering:\r\n\r\n![](https://s3.amazonaws.com/salathegroup-static/plantvillage/plantvillage-min.png)\r\n\r\nTo
    learn more about the background of the dataset, please refer to the following
    paper: http://arxiv.org/abs/1511.08060. You must cite this paper if you use the
    dataset. "
  description: |
    <p>We depend on edible plants just as we depend on oxygen. Without crops, there is no food, and without food, there is no life. It&#39;s no accident that human civilization began to thrive with the invention of agriculture.</p>

    <p>Today, modern technology allows us to grow crops in quantities necessary for a steady food supply for billions of people. But diseases remain a major threat to this supply, and a large fraction of crops are lost each year to diseases. The situation is particularly dire for the 500 million smallholder farmers around the globe, whose livelihoods depend on their crops doing well. In Africa alone, 80% of the agricultural output comes from smallholder farmers.</p>

    <p>With billions of smartphones around the globe, wouldn&#39;t it be great if the smartphone could be turned into a disease diagnostics tool, recognizing diseases from images it captures with its camera? This challenge is the first of many steps turning this vision into a reality. PlantVillage is a not-for-profit project by Penn State University in the US and EPFL in Switzerland. We have collected - and continue to collect - tens of thousands of images of diseased and healthy crops. The goal of this challenge is to develop algorithms than can accurately diagnose a disease based on an image.</p>

    <p>Here are the 38 classes of crop disease pairs that the dataset is offering:</p>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/plantvillage/plantvillage-min.png" alt=""></p>

    <p>To learn more about the background of the dataset, please refer to the following paper: <a href="http://arxiv.org/abs/1511.08060" rel="nofollow" target="_blank">http://arxiv.org/abs/1511.08060</a>. You must cite this paper if you use the dataset. </p>
  evaluation_markdown: "Submissions will be evaluated using a Multi Class Log Loss
    evaluation function, which are defined as :\r\n\r\n### Mean F1 score   \r\n      \r\nThe
    F1 score is computed separately for all classes by using:\r\n\r\n![](https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png){:class='img-medium'}\r\n\r\n*
    **_p_** refers to the precision\r\n* **_r_** refers to the recall\r\n* **_tp_**
    refers to the number of True Positives, \r\n* **_fp_** refers to the number of
    False Positives\r\n* **_fn_** refers to the number of False Negatives\r\n\r\nThen
    finally the Mean of all the F1 scores across all the classes is used for come
    up with the combined Mean F1 score.   \r\n   \r\n\r\n### Mean Log Loss   \r\n\r\n![](https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png){:class='img-medium'}\r\n\r\n\r\n*
    **_N_** is the total number of examples in the test set\r\n* **_M_** is the total
    number of class labels (38 for  this challenge)\r\n* **_y ij_** is a boolean value
    representing if the i-th instance in the test set belongs to the j-th label.\r\n*
    **_p ij_** is the probability according to your submission that the i-th instance
    may belong to the j-th label.\r\n* **_Ln_** is the natural logarithmic function.\r\n\r\n\r\nAll
    submissions will be evaluated on the test dataset in the docker containers referenced
    in the Resources section. The code archive will be uncompressed into the ```/plantvillage```
    path, and every code archive is expected to contain a ```main.sh``` script which
    takes path to a folder containing images as its first parameter. So to test your
    code submission, we will finally execute : \r\n\r\n```/plantvillage/main.sh pathToFolderContainingTestImages```\r\n\r\nThis
    is expected to output a CSV file containing the name of the file, and the associated
    probabilities for all the classes at the location :\r\n\r\n```/plantvillage/classification.csv```\r\n\r\n"
  evaluation: |+
    <p>Submissions will be evaluated using a Multi Class Log Loss evaluation function, which are defined as :</p>

    <h3 id="mean-f1-score">Mean F1 score</h3>

    <p>The F1 score is computed separately for all classes by using:</p>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png" alt="" class="img-medium" /></p>

    <ul>
      <li><strong><em>p</em></strong> refers to the precision</li>
      <li><strong><em>r</em></strong> refers to the recall</li>
      <li><strong><em>tp</em></strong> refers to the number of True Positives,</li>
      <li><strong><em>fp</em></strong> refers to the number of False Positives</li>
      <li><strong><em>fn</em></strong> refers to the number of False Negatives</li>
    </ul>

    <p>Then finally the Mean of all the F1 scores across all the classes is used for come up with the combined Mean F1 score.</p>

    <h3 id="mean-log-loss">Mean Log Loss</h3>

    <p><img src="https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png" alt="" class="img-medium" /></p>

    <ul>
      <li><strong><em>N</em></strong> is the total number of examples in the test set</li>
      <li><strong><em>M</em></strong> is the total number of class labels (38 for  this challenge)</li>
      <li><strong><em>y ij</em></strong> is a boolean value representing if the i-th instance in the test set belongs to the j-th label.</li>
      <li><strong><em>p ij</em></strong> is the probability according to your submission that the i-th instance may belong to the j-th label.</li>
      <li><strong><em>Ln</em></strong> is the natural logarithmic function.</li>
    </ul>

    <p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code>/plantvillage</code> path, and every code archive is expected to contain a <code>main.sh</code> script which takes path to a folder containing images as its first parameter. So to test your code submission, we will finally execute :</p>

    <p><code>/plantvillage/main.sh pathToFolderContainingTestImages</code></p>

    <p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

    <p><code>/plantvillage/classification.csv</code></p>

  rules_markdown: "* Participants are allowed a maximum of five submissions each 24
    hours.\r\n\r\n* Use of any external datasets (or any pre-trained trained models)
    in any form is not allowed.\r\n\r\n* All images are released under the Creative
    Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) license, with the clarification
    that algorithms trained on the data fall under the same license. \r\n\r\n* In
    order to be eligible for the winner's prize, you must release the source code
    used to generate the winning submission on a public GitHub repository, licensed
    under the Creative Commons Attribution-ShareAlike 3.0 Unported license.\r\n\r\n*
    crowdAI reserves the right to modify challenge rules as required."
  rules: |
    <ul>
    <li><p>Participants are allowed a maximum of five submissions each 24 hours.</p></li>
    <li><p>Use of any external datasets (or any pre-trained trained models) in any form is not allowed.</p></li>
    <li><p>All images are released under the Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) license, with the clarification that algorithms trained on the data fall under the same license. </p></li>
    <li><p>In order to be eligible for the winner&#39;s prize, you must release the source code used to generate the winning submission on a public GitHub repository, licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported license.</p></li>
    <li><p>crowdAI reserves the right to modify challenge rules as required.</p></li>
    </ul>
  prizes_markdown: The author of the most highly ranked submission will be invited
    to the **crowdAI winner's symposium** at EPFL in Switzerland on January 30/31,
    2017. The educational award is given to the participant with the either the most
    insightful submission posts, or the best tutorial - the recipient of this award
    will also be invited to the symposium (the crowdAI team will pick the recipient
    of this award). Expenses for travel and accommodation are covered by crowdAI.
  prizes: "<p>The author of the most highly ranked submission will be invited to the
    <strong>crowdAI winner&#39;s symposium</strong> at EPFL in Switzerland on January
    30/31, 2017. The educational award is given to the participant with the either
    the most insightful submission posts, or the best tutorial - the recipient of
    this award will also be invited to the symposium (the crowdAI team will pick the
    recipient of this award). Expenses for travel and accommodation are covered by
    crowdAI.</p>\n"
  resources_markdown: "References to Docker Containers where the submissions will
    be tested :: \r\n\r\n**Caffe**  : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/\r\n**Tensorflow**
    : https://hub.docker.com/r/tensorflow/tensorflow/\r\n**Torch7** : https://hub.docker.com/r/kaixhin/cuda-torch/\r\n**Scikit-Learn**
    :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2\r\n**Scikit-Learn**
    : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3\r\n**Octave**
    : https://hub.docker.com/r/schickling/octave/\r\n**Keras** :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/
    \r\n\r\nFeel free to shoot us an email if you want to be able to submit code in
    your favourite language or framework :D We would be happy to help :)"
  resources: |
    <p>References to Docker Containers where the submissions will be tested :: </p>

    <p><strong>Caffe</strong>  : <a href="https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/" rel="nofollow" target="_blank">https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/</a><br>
    <strong>Tensorflow</strong> : <a href="https://hub.docker.com/r/tensorflow/tensorflow/" rel="nofollow" target="_blank">https://hub.docker.com/r/tensorflow/tensorflow/</a><br>
    <strong>Torch7</strong> : <a href="https://hub.docker.com/r/kaixhin/cuda-torch/" rel="nofollow" target="_blank">https://hub.docker.com/r/kaixhin/cuda-torch/</a><br>
    <strong>Scikit-Learn</strong> :(Python-2): <a href="https://github.com/dataquestio/ds-containers/tree/master/python2" rel="nofollow" target="_blank">https://github.com/dataquestio/ds-containers/tree/master/python2</a><br>
    <strong>Scikit-Learn</strong> : (Python-3): <a href="https://github.com/dataquestio/ds-containers/tree/master/python3" rel="nofollow" target="_blank">https://github.com/dataquestio/ds-containers/tree/master/python3</a><br>
    <strong>Octave</strong> : <a href="https://hub.docker.com/r/schickling/octave/" rel="nofollow" target="_blank">https://hub.docker.com/r/schickling/octave/</a><br>
    <strong>Keras</strong> :  <a href="https://hub.docker.com/r/patdiscvrd/keras/%7E/dockerfile/" rel="nofollow" target="_blank">https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/</a> </p>

    <p>Feel free to shoot us an email if you want to be able to submit code in your favourite language or framework :D We would be happy to help :)</p>
  submission_instructions_markdown: "**This challenge has now ended. You may continue
    to make submissions for educational purposes, and they will be indicated separately
    on the leaderboard.**\r\n\r\nThe participants are required to submit a CSV file
    containing the probabilities across all classes for all the images in the test
    set. The dataset has a total of 38 classes, which are labelled as c0 through c37.\r\nThe
    first column of the said CSV file has to be the name of the file from the test
    set, and the subsequent 38 columns have to be the probability that the image belongs
    to the corresponding class.\r\nThe first row of the CSV file should look like
    :\r\n\"\r\n**filename,c_0,c_1,c_2,c_3,c_4,c_5,c_6,c_7,c_8,c_9,c_10,c_11,c_12,c_13,c_14,c_15,c_16,c_17,c_18,c_19,c_20,c_21,c_22,c_23,c_24,c_25,c_26,c_27,c_28,c_29,c_30,c_31,c_32,c_33,c_34,c_35,c_36,c_37**\r\n\"\r\nAlong
    with the CSV file, the participants also have to submit a prediction script (along
    with associated models, etc) as described here.\r\nA sample submission can be
    downloaded from here."
  submission_instructions: |
    <p><strong>This challenge has now ended. You may continue to make submissions for educational purposes, and they will be indicated separately on the leaderboard.</strong></p>

    <p>The participants are required to submit a CSV file containing the probabilities across all classes for all the images in the test set. The dataset has a total of 38 classes, which are labelled as c0 through c37.<br>
    The first column of the said CSV file has to be the name of the file from the test set, and the subsequent 38 columns have to be the probability that the image belongs to the corresponding class.<br>
    The first row of the CSV file should look like :<br>
    &quot;<br>
    <strong>filename,c<em>0,c</em>1,c<em>2,c</em>3,c<em>4,c</em>5,c<em>6,c</em>7,c<em>8,c</em>9,c<em>10,c</em>11,c<em>12,c</em>13,c<em>14,c</em>15,c<em>16,c</em>17,c<em>18,c</em>19,c<em>20,c</em>21,c<em>22,c</em>23,c<em>24,c</em>25,c<em>26,c</em>27,c<em>28,c</em>29,c<em>30,c</em>31,c<em>32,c</em>33,c<em>34,c</em>35,c<em>36,c</em>37</strong><br>
    &quot;<br>
    Along with the CSV file, the participants also have to submit a prediction script (along with associated models, etc) as described here.<br>
    A sample submission can be downloaded from here.</p>
  license_markdown: "All images are released under the Creative Commons Attribution-ShareAlike
    3.0 Unported (CC BY-SA 3.0), with the clarification that algorithms trained on
    the data fall under the same license.\r\n\r\n"
  license: "<p>All images are released under the Creative Commons Attribution-ShareAlike
    3.0 Unported (CC BY-SA 3.0), with the clarification that algorithms trained on
    the data fall under the same license.</p>\n"
  dataset_description_markdown: 
  dataset_description: 
  image_file: 1.PV_avatar_original__1_.png
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### PlantVillage Disease Classification Challenge
    Results\r\n<br><br>\r\n\r\n| Rank | Participant | Prize |\r\n| **1.** | [chsasank](https://www.crowdai.org/participants/chsasank)
    | Invitation to [AMLD2017](https://www.appliedmldays.org/2017/) |\r\n| **2.**
    | [Huynh_Xuan_Phung](https://www.crowdai.org/participants/huynh_xuan_phung) |
    \ |\r\n| **3.** | [d_133](https://www.crowdai.org/participants/d_133) |  |\r\n\r\n<br>\r\nSpecial
    mention for [Santiago_Guauque](https://www.crowdai.org/participants/santiago_guauque)
    for writing a Torch tutorial for the challenge. As recognition, he received an
    invitation to [AMLD2017](https://www.appliedmldays.org/2017/)."
  winner_description: |
    <h3 id="plantvillage-disease-classification-challenge-results">PlantVillage Disease Classification Challenge Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/chsasank">chsasank</a></td>
          <td>Invitation to <a href="https://www.appliedmldays.org/2017/">AMLD2017</a></td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/huynh_xuan_phung">Huynh_Xuan_Phung</a></td>
          <td> </td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/d_133">d_133</a></td>
          <td> </td>
        </tr>
      </tbody>
    </table>

    <p><br />
    Special mention for <a href="https://www.crowdai.org/participants/santiago_guauque">Santiago_Guauque</a> for writing a Torch tutorial for the challenge. As recognition, he received an invitation to <a href="https://www.appliedmldays.org/2017/">AMLD2017</a>.</p>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: 
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_25:
  id: 25
  organizer_id: 10
  challenge: Mapping Challenge
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &24 2017-11-24 12:25:15.173138000 Z
    zone: *2
    time: *24
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &25 2018-10-04 09:42:27.762777000 Z
    zone: *2
    time: *25
  tagline: Building Missing Maps with Machine Learning
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 24774
  participant_count: 540
  submission_count: 718
  score_title: Average Precision (IoU >= 0.5)
  score_secondary_title: Average Recall  (IoU >= 0.5)
  slug: mapping-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: crowdAIMappingChallenge
  online_grading: true
  vote_count: 118
  description_markdown: "> **UPDATE: Submissions Instructions for ROUND 2** are available
    at : [https://github.com/crowdAI/mapping-challenge-round2-starter-kit](https://github.com/crowdAI/mapping-challenge-round2-starter-kit)
    \  \r\n\r\n> **UPDATE: The prizes for this challenge have been updated and can
    be found on the bottom of this page.**\r\n\r\nWe are in a period of increasing
    humanitarian crises, both in scale and number. Natural disasters continue to increase
    in frequency and impact, while long-term and reignited conflicts affect people
    in many parts of the world. **Often, accurate maps either do not exist or are
    outdated by disaster or conflict.**\r\n\r\n[Humanity & Inclusion](http://www.hi-us.org/inclusion)
    is an aid organization working in some 60 countries, alongside people with disabilities
    and vulnerable populations. Our emergency sector responds quickly and effectively
    to natural and civil disasters.\r\n \r\n- Many parts of the world have not been
    mapped; especially the most marginalized parts, that is, those most vulnerable
    to natural hazards.\r\nObtaining maps of these potential crisis areas greatly
    improves the response of emergency preparedness actors.\r\n \r\n- During a disaster
    it is extremely useful to be able to map the impassable sections of road for example,
    as well as the most damaged residential areas, the most vulnerable schools and
    public buildings, population movements, etc. The objective is to adapt as quickly
    as possible the intervention procedures to the evolution of the context generated
    by the crisis.\r\n \r\n- In the first days following the occurrence of a disaster,
    it is essential to have as fine a mapping as possible of communication networks,
    housing areas and infrastructures, areas dedicated to agriculture, etc.\r\n \r\nToday,
    when new maps are needed they are drawn by hand, often by volunteers who participate
    in so called Mapathons. They draw roads and buildings on satellite images, and
    contribute to Open StreetMap.\r\n \r\nFor instance, [Humanity & Inclusion](http://www.hi-us.org/inclusion)
    has been involved in organising numerous Mapathons to draw new maps for our clearance
    teams in Laos.\r\n \r\nIn this challenge we want to explore how Machine Learning
    can help pave the way for automated analysis of satellite imagery to generate
    relevant and real-time maps.\r\n\r\n## Task \r\n\r\nSatellite imagery is readily
    available to humanitarian organisations, but translating images into maps is an
    intensive effort. Today maps are produced by [specialized organisations](https://www.cartong.org)
    or in volunteer events such as [mapathons](https://www.missingmaps.org), where
    imagery is annotated with roads, buildings, farms, rivers etc.\r\n\r\nImages are
    increasingly available from a variety of sources, including nano-satellites, drones
    and conventional high altitude satellites. The data is available: the task is
    to produce intervention-specific maps with the relevant features, in a short timeframe
    and from disparate data sources.\r\n\r\n\r\n![4up-image.jpg](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg)\r\n\r\nIn
    this challenge you will be provided with a dataset of individual tiles of satellite
    imagery as RGB images, and their corresponding annotations of where an image is
    there a building. The goal is to train a model which given a new tile can annotate
    all buildings. \r\n\r\nAlso, in context of this challenge, to make the barrier
    to entry much lower, we tried to remove all the domain specific jargon of Remote
    Sensing and Satellite Imagery Analysis, and are presenting this as a problem of
    Object Detection and Object Segmentation in Images.\r\n\r\nThe idea being,  once
    we collectively demonstrate that an approach works really well on RGB images with
    just 3 channels of information, we can then work on extending it to multi-channel
    information from rich satellite imagery.\r\n\r\n# Datasets\r\n\r\nYou can download
    the datasets in the [Datasets Section](https://www.crowdai.org/challenges/mapping-challenge/dataset_files).
    You are provided with :   \r\n\r\n* `train.tar.gz` : This is the Training Set
    of **280741** tiles (as 300x300 pixel RGB images) of satellite imagery, along
    with their corresponding annotations in [MS-COCO format](http://cocodataset.org/#home)\r\n\r\n*
    `val.tar.gz`: This is the suggested Validation Set of **60317** tiles (as 300x300
    pixel RGB images) of satellite imagery, along with their corresponding annotations
    in [MS-COCO format](http://cocodataset.org/#home)\r\n\r\n* `test_images.tar.gz`
    : This is the Test Set for Round-1, where you are provided with **60697** files
    (as 300x300 pixel RGB images) and your are required to submit annotations for
    all these files.  \r\n\r\nFor more details about the dataset, and submission procedures
    etc, please refer to the following notebooks : \r\n\r\n* [Dataset Utils](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb)\r\n
    \ * [Import Dependencies](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies)\r\n
    \ * [Configuration Variables](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables)\r\n
    \ * [Parsing Annotations](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations)\r\n
    \ * [Collecting and Visualizing Images](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images)\r\n
    \ * [Understanding Annotations](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations)\r\n
    \ * [Visualizing Annotations](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations)\r\n
    \ * [Advanced](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced)\r\n
    \   * [Convert poly segmentation to rle](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle)\r\n
    \   * [Convert segmentation to pixel level masks](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks)\r\n*
    [Random Submission](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb)\r\n
    \ * [Submission Format](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submission-Format)\r\n
    \ * [Generating a Random Segmentation](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-segmentation)\r\n
    \ * [Generating a Random Annotation Object](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-annotation-object)\r\n
    \ * [Generating a Random Results Object](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-results-object)\r\n
    \ * [Submit to crowdAI for grading](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submit-to-crowdAI-for-grading)\r\n\r\n*
    [Locally test the evaluation function](https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb)
    \  \r\n\r\n* Train [Mask-RCNN](https://arxiv.org/abs/1703.06870)\r\n  * [Installation](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn)\r\n
    \ * [Training](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Training.ipynb)\r\n
    \ * [Prediction & Submission](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Prediction-and-Submission.ipynb)\r\n
    \ * **NOTE** : This is in a separate repository, and we have also now added the
    pretrained weights from the baseline submission to the [datasets page](https://www.crowdai.org/challenges/mapping-challenge/dataset_files).\r\n\r\n#
    Evaluation Criteria\r\n\r\nFor for a known ground truth mask $$A$$, you propose
    a mask $$B$$, then we first compute $$IoU$$ (Intersection Over Union) :      \r\n\r\n$$\r\nIoU(A,
    B) = \\frac{A \\cap B}{ A \\cup B}\r\n$$\r\n\r\n$$IoU$$ measures the overall overlap
    between the true region and the proposed region.\r\nThen we consider it a True
    detection, when there is atleast half an overlap, or when $$IoU \\geq 0.5$$\r\n\r\nThen
    we can define the following parameters :\r\n\r\n* Precision ($$IoU \\geq 0.5$$)
    \  \r\n$$\r\nP_{IoU \\geq 0.5} = \\frac{TP_{IoU \\geq 0.5}}{TP_{IoU \\geq 0.5}
    + FP_{IoU \\geq 0.5}}\r\n$$\r\n\r\n* Recall ($$IoU  \\geq 0.5$$)   \r\n$$\r\nR_{IoU
    \\geq 0.5} = \\frac{TP_{IoU \\geq 0.5}}{TP_{IoU \\geq 0.5} + FN_{IoU \\geq 0.5}}\r\n$$.
    \ \r\n\r\nThe final scoring parameters $$AP_{IoU \\geq 0.5}$$ and $$AR_{IoU \\geq
    0.5}$$ are computed by averaging over all the precision and recall values for
    all known annotations in the ground truth.\r\n\r\n# Challenge Rounds\r\n\r\n##
    Round 1\r\n\r\nWe will stop accepting submissions for Round 1 on June 1, 2018.\r\nAll
    participants of Round 1 will be invited to complete in Round 2.\r\n\r\nFor instructions
    on submitting solutions for Round-2, please refer to the [mapping-challenge-starter-kit](https://github.com/crowdAI/mapping-challenge-starter-kit).
    \ \r\n\r\n**Note**: We will be adding more content to the starter kit to help
    you get started in the challenge. So please do keep a close eye on the starter-kit
    for updates. In the meantime, you can have a look at the examples of [cocoapi](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb)
    on how to easily parse and explore the datasets.\r\n\r\n## Round 2\r\n\r\nRound
    2 participation is open to all. \r\nParticipants are required to submit their
    code and models which **will be internally tested by [UNOSAT](https://unitar.org/unosat/)
    \ and [UN Global Pulse](https://www.unglobalpulse.org/) on a dataset (in a similar
    format as the currently released data) of an undisclosed location.**\r\n\r\n**Starter-Kit
    for Round-2 can be found at** : [https://github.com/crowdAI/mapping-challenge-round2-starter-kit](https://github.com/crowdAI/mapping-challenge-round2-starter-kit)\r\n\r\n##
    Timeline\r\n\r\n* Round 1 : 28.03.2018 - 01.06.2018\r\n* Round 2 : 23.07.2018
    - 20.08.2018\r\n* Announcement of Overall Results : 20.08.2018"
  description: "<blockquote>\n  <p><strong>UPDATE: Submissions Instructions for ROUND
    2</strong> are available at : <a href=\"https://github.com/crowdAI/mapping-challenge-round2-starter-kit\">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>\n</blockquote>\n\n<blockquote>\n
    \ <p><strong>UPDATE: The prizes for this challenge have been updated and can be
    found on the bottom of this page.</strong></p>\n</blockquote>\n\n<p>We are in
    a period of increasing humanitarian crises, both in scale and number. Natural
    disasters continue to increase in frequency and impact, while long-term and reignited
    conflicts affect people in many parts of the world. <strong>Often, accurate maps
    either do not exist or are outdated by disaster or conflict.</strong></p>\n\n<p><a
    href=\"http://www.hi-us.org/inclusion\">Humanity &amp; Inclusion</a> is an aid
    organization working in some 60 countries, alongside people with disabilities
    and vulnerable populations. Our emergency sector responds quickly and effectively
    to natural and civil disasters.</p>\n\n<ul>\n  <li>\n    <p>Many parts of the
    world have not been mapped; especially the most marginalized parts, that is, those
    most vulnerable to natural hazards.\nObtaining maps of these potential crisis
    areas greatly improves the response of emergency preparedness actors.</p>\n  </li>\n
    \ <li>\n    <p>During a disaster it is extremely useful to be able to map the
    impassable sections of road for example, as well as the most damaged residential
    areas, the most vulnerable schools and public buildings, population movements,
    etc. The objective is to adapt as quickly as possible the intervention procedures
    to the evolution of the context generated by the crisis.</p>\n  </li>\n  <li>\n
    \   <p>In the first days following the occurrence of a disaster, it is essential
    to have as fine a mapping as possible of communication networks, housing areas
    and infrastructures, areas dedicated to agriculture, etc.</p>\n  </li>\n</ul>\n\n<p>Today,
    when new maps are needed they are drawn by hand, often by volunteers who participate
    in so called Mapathons. They draw roads and buildings on satellite images, and
    contribute to Open StreetMap.</p>\n\n<p>For instance, <a href=\"http://www.hi-us.org/inclusion\">Humanity
    &amp; Inclusion</a> has been involved in organising numerous Mapathons to draw
    new maps for our clearance teams in Laos.</p>\n\n<p>In this challenge we want
    to explore how Machine Learning can help pave the way for automated analysis of
    satellite imagery to generate relevant and real-time maps.</p>\n\n<h2 id=\"task\">Task</h2>\n\n<p>Satellite
    imagery is readily available to humanitarian organisations, but translating images
    into maps is an intensive effort. Today maps are produced by <a href=\"https://www.cartong.org\">specialized
    organisations</a> or in volunteer events such as <a href=\"https://www.missingmaps.org\">mapathons</a>,
    where imagery is annotated with roads, buildings, farms, rivers etc.</p>\n\n<p>Images
    are increasingly available from a variety of sources, including nano-satellites,
    drones and conventional high altitude satellites. The data is available: the task
    is to produce intervention-specific maps with the relevant features, in a short
    timeframe and from disparate data sources.</p>\n\n<p><img src=\"https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg\"
    alt=\"4up-image.jpg\" /></p>\n\n<p>In this challenge you will be provided with
    a dataset of individual tiles of satellite imagery as RGB images, and their corresponding
    annotations of where an image is there a building. The goal is to train a model
    which given a new tile can annotate all buildings.</p>\n\n<p>Also, in context
    of this challenge, to make the barrier to entry much lower, we tried to remove
    all the domain specific jargon of Remote Sensing and Satellite Imagery Analysis,
    and are presenting this as a problem of Object Detection and Object Segmentation
    in Images.</p>\n\n<p>The idea being,  once we collectively demonstrate that an
    approach works really well on RGB images with just 3 channels of information,
    we can then work on extending it to multi-channel information from rich satellite
    imagery.</p>\n\n<h1 id=\"datasets\">Datasets</h1>\n\n<p>You can download the datasets
    in the <a href=\"https://www.crowdai.org/challenges/mapping-challenge/dataset_files\">Datasets
    Section</a>. You are provided with :</p>\n\n<ul>\n  <li>\n    <p><code class=\"highlighter-rouge\">train.tar.gz</code>
    : This is the Training Set of <strong>280741</strong> tiles (as 300x300 pixel
    RGB images) of satellite imagery, along with their corresponding annotations in
    <a href=\"http://cocodataset.org/#home\">MS-COCO format</a></p>\n  </li>\n  <li>\n
    \   <p><code class=\"highlighter-rouge\">val.tar.gz</code>: This is the suggested
    Validation Set of <strong>60317</strong> tiles (as 300x300 pixel RGB images) of
    satellite imagery, along with their corresponding annotations in <a href=\"http://cocodataset.org/#home\">MS-COCO
    format</a></p>\n  </li>\n  <li>\n    <p><code class=\"highlighter-rouge\">test_images.tar.gz</code>
    : This is the Test Set for Round-1, where you are provided with <strong>60697</strong>
    files (as 300x300 pixel RGB images) and your are required to submit annotations
    for all these files.</p>\n  </li>\n</ul>\n\n<p>For more details about the dataset,
    and submission procedures etc, please refer to the following notebooks :</p>\n\n<ul>\n
    \ <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb\">Dataset
    Utils</a>\n    <ul>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies\">Import
    Dependencies</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables\">Configuration
    Variables</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations\">Parsing
    Annotations</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images\">Collecting
    and Visualizing Images</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations\">Understanding
    Annotations</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations\">Visualizing
    Annotations</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced\">Advanced</a>\n
    \       <ul>\n          <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle\">Convert
    poly segmentation to rle</a></li>\n          <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks\">Convert
    segmentation to pixel level masks</a></li>\n        </ul>\n      </li>\n    </ul>\n
    \ </li>\n  <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb\">Random
    Submission</a>\n    <ul>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submission-Format\">Submission
    Format</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-segmentation\">Generating
    a Random Segmentation</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-annotation-object\">Generating
    a Random Annotation Object</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-results-object\">Generating
    a Random Results Object</a></li>\n      <li><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submit-to-crowdAI-for-grading\">Submit
    to crowdAI for grading</a></li>\n    </ul>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb\">Locally
    test the evaluation function</a></p>\n  </li>\n  <li>Train <a href=\"https://arxiv.org/abs/1703.06870\">Mask-RCNN</a>\n
    \   <ul>\n      <li><a href=\"https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn\">Installation</a></li>\n
    \     <li><a href=\"https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Training.ipynb\">Training</a></li>\n
    \     <li><a href=\"https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Prediction-and-Submission.ipynb\">Prediction
    &amp; Submission</a></li>\n      <li><strong>NOTE</strong> : This is in a separate
    repository, and we have also now added the pretrained weights from the baseline
    submission to the <a href=\"https://www.crowdai.org/challenges/mapping-challenge/dataset_files\">datasets
    page</a>.</li>\n    </ul>\n  </li>\n</ul>\n\n<h1 id=\"evaluation-criteria\">Evaluation
    Criteria</h1>\n\n<p>For for a known ground truth mask <script type=\"math/tex\">A</script>,
    you propose a mask <script type=\"math/tex\">B</script>, then we first compute
    <script type=\"math/tex\">IoU</script> (Intersection Over Union) :</p>\n\n<script
    type=\"math/tex; mode=display\">IoU(A, B) = \\frac{A \\cap B}{ A \\cup B}</script>\n\n<p><script
    type=\"math/tex\">IoU</script> measures the overall overlap between the true region
    and the proposed region.\nThen we consider it a True detection, when there is
    atleast half an overlap, or when <script type=\"math/tex\">IoU \\geq 0.5</script></p>\n\n<p>Then
    we can define the following parameters :</p>\n\n<ul>\n  <li>\n    <p>Precision
    (<script type=\"math/tex\">IoU \\geq 0.5</script>) <br />\n<script type=\"math/tex\">P_{IoU
    \\geq 0.5} = \\frac{TP_{IoU \\geq 0.5}}{TP_{IoU \\geq 0.5} + FP_{IoU \\geq 0.5}}</script></p>\n
    \ </li>\n  <li>\n    <p>Recall (<script type=\"math/tex\">IoU  \\geq 0.5</script>)
    <br />\n<script type=\"math/tex\">R_{IoU \\geq 0.5} = \\frac{TP_{IoU \\geq 0.5}}{TP_{IoU
    \\geq 0.5} + FN_{IoU \\geq 0.5}}</script>.</p>\n  </li>\n</ul>\n\n<p>The final
    scoring parameters <script type=\"math/tex\">AP_{IoU \\geq 0.5}</script> and <script
    type=\"math/tex\">AR_{IoU \\geq 0.5}</script> are computed by averaging over all
    the precision and recall values for all known annotations in the ground truth.</p>\n\n<h1
    id=\"challenge-rounds\">Challenge Rounds</h1>\n\n<h2 id=\"round-1\">Round 1</h2>\n\n<p>We
    will stop accepting submissions for Round 1 on June 1, 2018.\nAll participants
    of Round 1 will be invited to complete in Round 2.</p>\n\n<p>For instructions
    on submitting solutions for Round-2, please refer to the <a href=\"https://github.com/crowdAI/mapping-challenge-starter-kit\">mapping-challenge-starter-kit</a>.</p>\n\n<p><strong>Note</strong>:
    We will be adding more content to the starter kit to help you get started in the
    challenge. So please do keep a close eye on the starter-kit for updates. In the
    meantime, you can have a look at the examples of <a href=\"https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb\">cocoapi</a>
    on how to easily parse and explore the datasets.</p>\n\n<h2 id=\"round-2\">Round
    2</h2>\n\n<p>Round 2 participation is open to all. \nParticipants are required
    to submit their code and models which <strong>will be internally tested by <a
    href=\"https://unitar.org/unosat/\">UNOSAT</a>  and <a href=\"https://www.unglobalpulse.org/\">UN
    Global Pulse</a> on a dataset (in a similar format as the currently released data)
    of an undisclosed location.</strong></p>\n\n<p><strong>Starter-Kit for Round-2
    can be found at</strong> : <a href=\"https://github.com/crowdAI/mapping-challenge-round2-starter-kit\">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>\n\n<h2
    id=\"timeline\">Timeline</h2>\n\n<ul>\n  <li>Round 1 : 28.03.2018 - 01.06.2018</li>\n
    \ <li>Round 2 : 23.07.2018 - 20.08.2018</li>\n  <li>Announcement of Overall Results
    : 20.08.2018</li>\n</ul>\n"
  evaluation_markdown: ''
  evaluation: "\n"
  rules_markdown: "The following rules have to be observed by all participants:\r\n\r\n*
    Participants are allowed at most 5 submissions per day.\r\n* Participants are
    welcome to form teams. Teams should submit their predictions under a single account.
    The submitted paper will mention all members.\r\n* Participants have to release
    their solution under an [Open Source License](https://opensource.org/licenses)
    of their choice to be eligible for prizes. We encourage all participants to open-source
    their code!\r\n* Participants are not allowed to use any other datasets other
    than the ones released in context of this challenge. The use of pre-trained models
    is nevertheless permitted.\r\n* While submissions by Admins and Organizers can
    serve as baselines, they won't be considered in the final leaderboard.\r\n* In
    case of conflicts, the decision of the Organizers will be final and binding.\r\n*
    Organizers reserve the right to make changes to the rules and timeline.\r\n* Violation
    of the rules or other unfair activity may result in disqualification.\r\n\r\n\r\n"
  rules: |+
    <p>The following rules have to be observed by all participants:</p>

    <ul>
      <li>Participants are allowed at most 5 submissions per day.</li>
      <li>Participants are welcome to form teams. Teams should submit their predictions under a single account. The submitted paper will mention all members.</li>
      <li>Participants have to release their solution under an <a href="https://opensource.org/licenses">Open Source License</a> of their choice to be eligible for prizes. We encourage all participants to open-source their code!</li>
      <li>Participants are not allowed to use any other datasets other than the ones released in context of this challenge. The use of pre-trained models is nevertheless permitted.</li>
      <li>While submissions by Admins and Organizers can serve as baselines, they won’t be considered in the final leaderboard.</li>
      <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
      <li>Organizers reserve the right to make changes to the rules and timeline.</li>
      <li>Violation of the rules or other unfair activity may result in disqualification.</li>
    </ul>

  prizes_markdown: "**UPDATE**\r\n\r\n**The prizes have been updated, as follows:**\r\n\r\n**Top-1
    participant of Round 2**: Invitation to the [Applied Machine Learning Days](https://www.appliedmldays.org)
    2019 at EPFL, Switzerland in January 2019, with travel and accommodation covered.\r\n\r\n**Top-5
    participants of Round 2**: Invitation to present at the [IEEE DSAA 2018](https://dsaa2018.isi.it/home)
    in Turin, Italy, October 1-4, 2018, with conference registration, travel and accommodation
    covered (up to EUR 1000).\r\nThis prize is sponsored by Humanity & Inclusion.\r\n\r\n**Top-5
    participants of Round 2**: Invitation to submit a paper describing their solution
    to be published in the proceedings of [IEEE DSAA 2018](https://dsaa2018.isi.it/home),
    Turin, Italy.\r\n\r\n**Top Community Contributor**: Invitation to the [IEEE DSAA
    2018](https://dsaa2018.isi.it/home) in Turin, Italy, October 1-4, 2018, with travel
    and accommodation covered (up to EUR 1000).\r\nThis prize is aimed to reward the
    participant or team who contributed the most for the community to this challenge
    (e.g. releasing own code openly during challenge, helping other paricipants, etc)\r\n\r\n**All
    participants (Round 1 and 2)** : Certificate of participation from [Handicap International](https://handicap-international.ch),
    [UNOSAT](https://unitar.org/unosat/), [UN Global Pulse](https://www.unglobalpulse.org/),
    [EPFL](https://www.epfl.ch/) and [crowdAI](https://www.crowdai.org/).\r\n\r\n##
    Starter Kit \r\n\r\nA starter kit has been prepared which explains all the nuts
    and bolts required to get started in the challenge.\r\nIt can be accessed at :
    [https://github.com/crowdAI/mapping-challenge-starter-kit](https://github.com/crowdAI/mapping-challenge-starter-kit)\r\n\r\n**Starter-Kit
    for Round-2 can be found at** : [https://github.com/crowdAI/mapping-challenge-round2-starter-kit](https://github.com/crowdAI/mapping-challenge-round2-starter-kit)"
  prizes: |
    <p><strong>UPDATE</strong></p>

    <p><strong>The prizes have been updated, as follows:</strong></p>

    <p><strong>Top-1 participant of Round 2</strong>: Invitation to the <a href="https://www.appliedmldays.org">Applied Machine Learning Days</a> 2019 at EPFL, Switzerland in January 2019, with travel and accommodation covered.</p>

    <p><strong>Top-5 participants of Round 2</strong>: Invitation to present at the <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with conference registration, travel and accommodation covered (up to EUR 1000).
    This prize is sponsored by Humanity &amp; Inclusion.</p>

    <p><strong>Top-5 participants of Round 2</strong>: Invitation to submit a paper describing their solution to be published in the proceedings of <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>, Turin, Italy.</p>

    <p><strong>Top Community Contributor</strong>: Invitation to the <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with travel and accommodation covered (up to EUR 1000).
    This prize is aimed to reward the participant or team who contributed the most for the community to this challenge (e.g. releasing own code openly during challenge, helping other paricipants, etc)</p>

    <p><strong>All participants (Round 1 and 2)</strong> : Certificate of participation from <a href="https://handicap-international.ch">Handicap International</a>, <a href="https://unitar.org/unosat/">UNOSAT</a>, <a href="https://www.unglobalpulse.org/">UN Global Pulse</a>, <a href="https://www.epfl.ch/">EPFL</a> and <a href="https://www.crowdai.org/">crowdAI</a>.</p>

    <h2 id="starter-kit">Starter Kit</h2>

    <p>A starter kit has been prepared which explains all the nuts and bolts required to get started in the challenge.
    It can be accessed at : <a href="https://github.com/crowdAI/mapping-challenge-starter-kit">https://github.com/crowdAI/mapping-challenge-starter-kit</a></p>

    <p><strong>Starter-Kit for Round-2 can be found at</strong> : <a href="https://github.com/crowdAI/mapping-challenge-round2-starter-kit">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>
  resources_markdown: "Here are some interesting blog posts written by participants:\r\n\r\n*
    [Mapping Challenge winning solution](https://towardsdatascience.com/mapping-challenge-winning-solution-1aa1a13161b3){:target='_blank'}\r\n\r\n*
    [Playing with Crowd-AI mapping challenge - or how to improve your CNN performance
    with self-supervised techniques](https://spark-in.me/post/a-small-case-for-search-of-structure-within-your-data){:target='_blank'}\r\n\r\nHere
    is an open solution for this challenge, proposed by neptune.ml:\r\n\r\n* [https://github.com/neptune-ml/open-solution-mapping-challenge](https://github.com/neptune-ml/open-solution-mapping-challenge){:target='_blank'}\r\n\r\n##
    Acknowledgements \r\n\r\nA big shout out to our awesome community members [@MasterScat
    (Florian Laurent)](https://www.crowdai.org/participants/masterscrat), [Snigdha
    Dagar](snigdha.dagar@gmail.com), and [Iuliana Voinea](https://www.crowdai.org/participants/iuliana),
    for their help in preparing the datasets and designing the challenge.\r\n\r\n##
    Contact Us\r\n\r\n* Gitter Channel : [crowdAI/crowdai-mapping-challenge](https://gitter.im/crowdAI/crowdai-mapping-challenge)\r\n*
    Technical issues : [https://github.com/crowdAI/mapping-challenge-starter-kit/issues
    ](https://github.com/crowdAI/mapping-challenge-starter-kit/issues){:target='_blank'}\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/mapping-challenge/topics](https://www.crowdai.org/challenges/mapping-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  [ sharada.mohanty@epfl.ch\r\n
    ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <p>Here are some interesting blog posts written by participants:</p>

    <ul>
      <li>
        <p><a href="https://towardsdatascience.com/mapping-challenge-winning-solution-1aa1a13161b3" target="_blank">Mapping Challenge winning solution</a></p>
      </li>
      <li>
        <p><a href="https://spark-in.me/post/a-small-case-for-search-of-structure-within-your-data" target="_blank">Playing with Crowd-AI mapping challenge - or how to improve your CNN performance with self-supervised techniques</a></p>
      </li>
    </ul>

    <p>Here is an open solution for this challenge, proposed by neptune.ml:</p>

    <ul>
      <li><a href="https://github.com/neptune-ml/open-solution-mapping-challenge" target="_blank">https://github.com/neptune-ml/open-solution-mapping-challenge</a></li>
    </ul>

    <h2 id="acknowledgements">Acknowledgements</h2>

    <p>A big shout out to our awesome community members <a href="https://www.crowdai.org/participants/masterscrat">@MasterScat (Florian Laurent)</a>, <a href="snigdha.dagar@gmail.com">Snigdha Dagar</a>, and <a href="https://www.crowdai.org/participants/iuliana">Iuliana Voinea</a>, for their help in preparing the datasets and designing the challenge.</p>

    <h2 id="contact-us">Contact Us</h2>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/crowdai-mapping-challenge">crowdAI/crowdai-mapping-challenge</a></li>
      <li>Technical issues : <a href="https://github.com/crowdAI/mapping-challenge-starter-kit/issues" target="_blank">https://github.com/crowdAI/mapping-challenge-starter-kit/issues </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/mapping-challenge/topics">https://www.crowdai.org/challenges/mapping-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: mapping_challenge_1.jpg
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### Mapping Challenge Results\r\n<br><br>\r\n\r\n\r\n|
    Rank | Participant | Average Precision | Average Recall | Prize & Notes |\r\n|
    **1.** | [neptune.ml](https://www.crowdai.org/participants/neptune-ml) | 0.938
    | 0.946 | Invitation to [IEEE DSAA 2018](https://dsaa2018.isi.it/home). 2x$1000
    travel grant<br>Invitation to [AMLD 2019](https://www.appliedmldays.org/).<br>also
    **Top Community Contributor** |\r\n| **2.** | [yak](https://www.crowdai.org/participants/yak)
    | 0.937 | 0.959 | Invitation to [IEEE DSAA 2018](https://dsaa2018.isi.it/home).
    $1000 travel grant |\r\n| **3.** | [janickrohrbach](https://www.crowdai.org/participants/janickrohrbach)
    | 0.930 | 0.956 | Invitation to [IEEE DSAA 2018](https://dsaa2018.isi.it/home).
    $1000 travel grant |\r\n| **4.** | [dave](https://www.crowdai.org/participants/dave)
    | 0.918 | 0.929 | Invitation to [IEEE DSAA 2018](https://dsaa2018.isi.it/home).
    $1000 travel grant |\r\n| **5.** | [Mprasad](https://www.crowdai.org/participants/mprasad)
    and team | 0.899 | 0.932 | Invitation to [IEEE DSAA 2018](https://dsaa2018.isi.it/home).
    $1000 travel grant |\r\n"
  winner_description: |
    <h3 id="mapping-challenge-results">Mapping Challenge Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Average Precision</td>
          <td>Average Recall</td>
          <td>Prize &amp; Notes</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/neptune-ml">neptune.ml</a></td>
          <td>0.938</td>
          <td>0.946</td>
          <td>Invitation to <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>. 2x$1000 travel grant<br />Invitation to <a href="https://www.appliedmldays.org/">AMLD 2019</a>.<br />also <strong>Top Community Contributor</strong></td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/yak">yak</a></td>
          <td>0.937</td>
          <td>0.959</td>
          <td>Invitation to <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/janickrohrbach">janickrohrbach</a></td>
          <td>0.930</td>
          <td>0.956</td>
          <td>Invitation to <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>4.</strong></td>
          <td><a href="https://www.crowdai.org/participants/dave">dave</a></td>
          <td>0.918</td>
          <td>0.929</td>
          <td>Invitation to <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>5.</strong></td>
          <td><a href="https://www.crowdai.org/participants/mprasad">Mprasad</a> and team</td>
          <td>0.899</td>
          <td>0.932</td>
          <td>Invitation to <a href="https://dsaa2018.isi.it/home">IEEE DSAA 2018</a>. $1000 travel grant</td>
        </tr>
      </tbody>
    </table>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_13:
  id: 13
  organizer_id: 6
  challenge: 'WWW 2018 Challenge: Learning to Recognize Musical Genre'
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &26 2017-11-04 09:09:59.746604000 Z
    zone: *2
    time: *26
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &27 2018-10-04 09:10:08.312604000 Z
    zone: *2
    time: *27
  tagline: Learning to Recognize Musical Genre from Audio on the Web
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 14665
  participant_count: 305
  submission_count: 728
  score_title: Log Loss
  score_secondary_title: F1 Score
  slug: www-2018-challenge-learning-to-recognize-musical-genre
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: WWWMusicalGenreRecognitionChallenge
  online_grading: true
  vote_count: 62
  description_markdown: "Like never before, the web has become a place for sharing
    creative work - such as music - among a global community of artists and art lovers.
    While music and music collections predate the web, the web enabled much larger
    scale collections. Whereas people used to own a handful of vinyls or CDs, they
    nowadays have instant access to the whole of published musical content via online
    platforms. Such dramatic increase in the size of music collections created two
    challenges: (i) the need to automatically organize a collection (as users and
    publishers cannot manage them manually anymore), and (ii) the need to automatically
    recommend new songs to a user knowing his listening habits. An underlying task
    in both those challenges is to be able to group songs in semantic categories.\r\n\r\nMusic
    genres are categories that have arisen through a complex interplay of cultures,
    artists, and market forces to characterize similarities between compositions and
    organize music collections. Yet, the boundaries between genres still remain fuzzy,
    making the problem of music genre recognition (MGR) a nontrivial task (Scaringella
    2006). While its utility has been debated, mostly because of its ambiguity and
    cultural definition, it is widely used and understood by end-users who find it
    useful to discuss musical categories (McKay 2006). As such, it is one of the most
    researched areas in the Music Information Retrieval (MIR) field (Sturm 2012).\r\n\r\nThe
    task of this challenge, one of the four official challenges of the [Web Conference
    (WWW2018) challenges track](https://www2018.thewebconf.org/program/challenges-track/),
    is to recognize the musical genre of a piece of music of which only a recording
    is available. Genres are broad, e.g. pop or rock, and each song only has one target
    genre. The data for this challenge comes from the recently published [FMA dataset](https://github.com/mdeff/fma)
    (Defferrard 2017), which is a dump of the [Free Music Archive (FMA)](https://freemusicarchive.org),
    an interactive library of high-quality and curated audio which is freely and openly
    available to the public.\r\n\r\n## Results\r\n\r\nYou can find the final results
    and the ranking on the [repository](https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit)
    and in the [slides used to announce them](https://doi.org/10.5281/zenodo.1243501).\r\n\r\nIn
    the interest of reproducibility and transparency for interested\r\nresearchers,
    you'll find below links to the source code repositories of all\r\nsystems submitted
    by the participants for the second round of the challenge.\r\n\r\n1. Transfer
    Learning of Artist Group Factors to Musical Genre Classification\r\n\t* Jaehun
    Kim ([@jaehun]), TU Delft and Minz Won ([@minzwon]), Universitat Pompeu Fabra\r\n\t*
    Code: <https://gitlab.crowdai.org/minzwon/WWWMusicalGenreRecognitionChallenge>\r\n\t*
    Paper: <https://doi.org/10.1145/3184558.3191823>\r\n1. Ensemble of CNN-based Models
    using various Short-Term Input\r\n\t* Hyungui Lim ([@hglim]), <http://cochlear.ai>\r\n\t*
    Code: <https://gitlab.crowdai.org/hglim/WWWMusicalGenreRecognitionChallenge>\r\n1.
    Detecting Music Genre Using Extreme Gradient Boosting\r\n\t* Benjamin Murauer
    ([@benjamin_murauer]), Universität Innsbruck\r\n\t* Code: <https://gitlab.crowdai.org/Benjamin_Murauer/WWWMusicalGenreRecognitionChallenge>\r\n\t*
    Paper: <https://doi.org/10.1145/3184558.3191822>\r\n1. ConvNet on STFT spectrograms\r\n\t*
    Daniyar Chumbalov ([@check]), EPFL and Philipp Pushnyakov ([@gg12]), Moscow Institute
    of Physics and Technologies (MIPT)\r\n\t* Code: <https://gitlab.crowdai.org/gg12/WWWMusicalGenreRecognitionChallenge>\r\n1.
    [Xception](https://arxiv.org/pdf/1610.02357) on mel-scaled spectrograms\r\n\t*
    [@viper] and [@algohunt]\r\n\t* Code: <https://gitlab.crowdai.org/viper/WWWMusicalGenreRecognitionChallenge>\r\n1.
    Audio [Dual Path Networks](https://arxiv.org/abs/1707.01629) on mel-scaled spectrograms\r\n\t*
    Sungkyun Chang ([@mimbres]), Seoul National University\r\n\t* Code: <https://gitlab.crowdai.org/mimbres/WWWMusicalGenreRecognitionChallenge>\r\n\r\n[@jaehun]:
    https://www.crowdai.org/participants/jaehun\r\n[@minzwon]: https://www.crowdai.org/participants/minzwon\r\n[@hglim]:
    https://www.crowdai.org/participants/hglim\r\n[@benjamin_murauer]: https://www.crowdai.org/participants/benjamin_murauer\r\n[@viper]:
    https://www.crowdai.org/participants/viper\r\n[@algohunt]: https://www.crowdai.org/participants/algohunt\r\n[@mimbres]:
    https://www.crowdai.org/participants/mimbres\r\n[@check]: https://www.crowdai.org/participants/check\r\n[@gg12]:
    https://www.crowdai.org/participants/gg12\r\n\r\nThe repositories should be self-contained
    and easily executable. You can\r\nexecute any of the systems on your own mp3s
    by following those steps:\r\n\r\n1. Clone the git repository.\r\n1. [Build a docker
    image with `repo2docker`](Round2_packaging_guidelines.md#building-a-docker-image)\r\n1.
    [Execute the docker image](Round2_packaging_guidelines.md#executing-the-docker-image)\r\n\r\nYou
    can find more details in the [slides used to announce the\r\nresults](https://doi.org/10.5281/zenodo.1243501)
    and in the [overview paper](https://arxiv.org/abs/1803.05337). The [overview paper](https://arxiv.org/abs/1803.05337)
    summarizes our experience running a challenge with open data for musical genre
    recognition. Those notes motivate the task and the challenge design, show some
    statistics about the submissions, and present the results. Please cite our paper
    in your scholarly work if you want to reference this challenge.\r\n\r\n~~~\r\n@inproceedings{fma_crowdai_challenge,\r\n
    \ title = {Learning to Recognize Musical Genre from Audio},\r\n  author = {Defferrard,
    Micha\\\"el and Mohanty, Sharada P. and Carroll, Sean F. and Salath\\'e, Marcel},\r\n
    \ booktitle = {WWW '18 Companion: The 2018 Web Conference Companion},\r\n  year
    = {2018},\r\n  url = {https://arxiv.org/abs/1803.05337},\r\n}\r\n~~~\r\n"
  description: |
    <p>Like never before, the web has become a place for sharing creative work - such as music - among a global community of artists and art lovers. While music and music collections predate the web, the web enabled much larger scale collections. Whereas people used to own a handful of vinyls or CDs, they nowadays have instant access to the whole of published musical content via online platforms. Such dramatic increase in the size of music collections created two challenges: (i) the need to automatically organize a collection (as users and publishers cannot manage them manually anymore), and (ii) the need to automatically recommend new songs to a user knowing his listening habits. An underlying task in both those challenges is to be able to group songs in semantic categories.</p>

    <p>Music genres are categories that have arisen through a complex interplay of cultures, artists, and market forces to characterize similarities between compositions and organize music collections. Yet, the boundaries between genres still remain fuzzy, making the problem of music genre recognition (MGR) a nontrivial task (Scaringella 2006). While its utility has been debated, mostly because of its ambiguity and cultural definition, it is widely used and understood by end-users who find it useful to discuss musical categories (McKay 2006). As such, it is one of the most researched areas in the Music Information Retrieval (MIR) field (Sturm 2012).</p>

    <p>The task of this challenge, one of the four official challenges of the <a href="https://www2018.thewebconf.org/program/challenges-track/">Web Conference (WWW2018) challenges track</a>, is to recognize the musical genre of a piece of music of which only a recording is available. Genres are broad, e.g. pop or rock, and each song only has one target genre. The data for this challenge comes from the recently published <a href="https://github.com/mdeff/fma">FMA dataset</a> (Defferrard 2017), which is a dump of the <a href="https://freemusicarchive.org">Free Music Archive (FMA)</a>, an interactive library of high-quality and curated audio which is freely and openly available to the public.</p>

    <h2 id="results">Results</h2>

    <p>You can find the final results and the ranking on the <a href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit">repository</a> and in the <a href="https://doi.org/10.5281/zenodo.1243501">slides used to announce them</a>.</p>

    <p>In the interest of reproducibility and transparency for interested
    researchers, you’ll find below links to the source code repositories of all
    systems submitted by the participants for the second round of the challenge.</p>

    <ol>
      <li>Transfer Learning of Artist Group Factors to Musical Genre Classification
        <ul>
          <li>Jaehun Kim (<a href="https://www.crowdai.org/participants/jaehun">@jaehun</a>), TU Delft and Minz Won (<a href="https://www.crowdai.org/participants/minzwon">@minzwon</a>), Universitat Pompeu Fabra</li>
          <li>Code: <a href="https://gitlab.crowdai.org/minzwon/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/minzwon/WWWMusicalGenreRecognitionChallenge</a></li>
          <li>Paper: <a href="https://doi.org/10.1145/3184558.3191823">https://doi.org/10.1145/3184558.3191823</a></li>
        </ul>
      </li>
      <li>Ensemble of CNN-based Models using various Short-Term Input
        <ul>
          <li>Hyungui Lim (<a href="https://www.crowdai.org/participants/hglim">@hglim</a>), <a href="http://cochlear.ai">http://cochlear.ai</a></li>
          <li>Code: <a href="https://gitlab.crowdai.org/hglim/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/hglim/WWWMusicalGenreRecognitionChallenge</a></li>
        </ul>
      </li>
      <li>Detecting Music Genre Using Extreme Gradient Boosting
        <ul>
          <li>Benjamin Murauer (<a href="https://www.crowdai.org/participants/benjamin_murauer">@benjamin_murauer</a>), Universität Innsbruck</li>
          <li>Code: <a href="https://gitlab.crowdai.org/Benjamin_Murauer/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/Benjamin_Murauer/WWWMusicalGenreRecognitionChallenge</a></li>
          <li>Paper: <a href="https://doi.org/10.1145/3184558.3191822">https://doi.org/10.1145/3184558.3191822</a></li>
        </ul>
      </li>
      <li>ConvNet on STFT spectrograms
        <ul>
          <li>Daniyar Chumbalov (<a href="https://www.crowdai.org/participants/check">@check</a>), EPFL and Philipp Pushnyakov (<a href="https://www.crowdai.org/participants/gg12">@gg12</a>), Moscow Institute of Physics and Technologies (MIPT)</li>
          <li>Code: <a href="https://gitlab.crowdai.org/gg12/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/gg12/WWWMusicalGenreRecognitionChallenge</a></li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/pdf/1610.02357">Xception</a> on mel-scaled spectrograms
        <ul>
          <li><a href="https://www.crowdai.org/participants/viper">@viper</a> and <a href="https://www.crowdai.org/participants/algohunt">@algohunt</a></li>
          <li>Code: <a href="https://gitlab.crowdai.org/viper/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/viper/WWWMusicalGenreRecognitionChallenge</a></li>
        </ul>
      </li>
      <li>Audio <a href="https://arxiv.org/abs/1707.01629">Dual Path Networks</a> on mel-scaled spectrograms
        <ul>
          <li>Sungkyun Chang (<a href="https://www.crowdai.org/participants/mimbres">@mimbres</a>), Seoul National University</li>
          <li>Code: <a href="https://gitlab.crowdai.org/mimbres/WWWMusicalGenreRecognitionChallenge">https://gitlab.crowdai.org/mimbres/WWWMusicalGenreRecognitionChallenge</a></li>
        </ul>
      </li>
    </ol>

    <p>The repositories should be self-contained and easily executable. You can
    execute any of the systems on your own mp3s by following those steps:</p>

    <ol>
      <li>Clone the git repository.</li>
      <li><a href="Round2_packaging_guidelines.md#building-a-docker-image">Build a docker image with <code>repo2docker</code></a></li>
      <li><a href="Round2_packaging_guidelines.md#executing-the-docker-image">Execute the docker image</a></li>
    </ol>

    <p>You can find more details in the <a href="https://doi.org/10.5281/zenodo.1243501">slides used to announce the
    results</a> and in the <a href="https://arxiv.org/abs/1803.05337">overview paper</a>. The <a href="https://arxiv.org/abs/1803.05337">overview paper</a> summarizes our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results. Please cite our paper in your scholarly work if you want to reference this challenge.</p>

    <pre><code>@inproceedings{fma_crowdai_challenge,
      title = {Learning to Recognize Musical Genre from Audio},
      author = {Defferrard, Micha\"el and Mohanty, Sharada P. and Carroll, Sean F. and Salath\'e, Marcel},
      booktitle = {WWW '18 Companion: The 2018 Web Conference Companion},
      year = {2018},
      url = {https://arxiv.org/abs/1803.05337},
    }
    </code></pre>
  evaluation_markdown: "To avoid overfitting and cheating, the challenge will happen
    in **two rounds**. The final ranking will be based on results from the second
    round.\r\nIn the first round, participants are provided a test set of 35,000 clips
    of 30 seconds each, and they have to submit their predictions for all the 35,000
    clips. The platform evaluates the predictions and ranks the participant upon submission.\r\nIn
    the second round, all the participants will have to wrap their models in a Docker
    container. We will evaluate those against a new unseen test set. These 30s clips
    will be sampled (at least in part) from new contributions to the [Free Music Archive](https://freemusicarchive.org).
    \r\n\r\nDetails of how to package your code as [Binder](https://mybinder.readthedocs.io/en/latest/)
    compatible repositories, please read the documentation here : [https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md](https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md)\r\n\r\nThe
    primary metric for evaluation will be the Mean [Log Loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss),
    and the secondary metric for the evaluation with be the Mean [F1-Score](https://en.wikipedia.org/wiki/F1_score).\r\n\r\nThe
    **Mean Log Loss** is defined by\r\n\r\n$ L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^{C}
    y_{nc} \\ln(p_{nc}), $\r\n\r\nwhere\r\n\r\n* $N=35000$ is the number of examples
    in the test set,\r\n* $C=16$ is the number of class labels, i.e. genres,\r\n*
    $y_{nc}$ is a binary value indicating if the n-th instance belongs to the c-th
    label,\r\n* $p_{nc}$ is the probability according to your submission that the
    n-th instance belongs to the c-th label,\r\n* $\\ln$ is the natural logarithmic
    function.\r\n\r\nThe $F_1$ score for a particular class $c$ is given by\r\n\r\n$
    F_1^c = 2\\frac{p^c r^c}{p^c + r^c}, $\r\n\r\nwhere\r\n\r\n* $p^c = \\frac{tp^c}{tp^c
    + fp^c}$ is the precision for class $c$,\r\n* $r^c = \\frac{tp^c}{tp^c + fn^c}$
    is the recall for class $c$,\r\n* $tp^c$ refers to the number of True Positives
    for class $c$,\r\n* $fp^c$ refers to the number of False Positives for class $c$,\r\n*
    $fn^c$ refers to the number of False Negatives for class $c$.\r\n\r\nThe final
    **Mean $F_1$ Score** is then defined as\r\n\r\n$ F_1 = \\frac{1}{C} \\sum_{c=1}^{C}
    F_1^c. $\r\n\r\nThe participants have to submit a CSV file with the following
    header:\r\n\r\n```\r\nfile_id,Blues,Classical,Country,Easy Listening,Electronic,Experimental,Folk,Hip-Hop,Instrumental,International,Jazz,Old-Time
    / Historic,Pop,Rock,Soul-RnB,Spoken\r\n```\r\n\r\nEach row is then an entry for
    every file in the test set (in the sorted order of the `file_id`s). The first
    column in every row represents the `file_id` (which is the name of the test file
    without its `.mp3` extension) and the rest of the $C=16$ columns are the predicted
    probabilities for each class in the order mentioned in the above CSV header."
  evaluation: |
    <p>To avoid overfitting and cheating, the challenge will happen in <strong>two rounds</strong>. The final ranking will be based on results from the second round.
    In the first round, participants are provided a test set of 35,000 clips of 30 seconds each, and they have to submit their predictions for all the 35,000 clips. The platform evaluates the predictions and ranks the participant upon submission.
    In the second round, all the participants will have to wrap their models in a Docker container. We will evaluate those against a new unseen test set. These 30s clips will be sampled (at least in part) from new contributions to the <a href="https://freemusicarchive.org">Free Music Archive</a>.</p>

    <p>Details of how to package your code as <a href="https://mybinder.readthedocs.io/en/latest/">Binder</a> compatible repositories, please read the documentation here : <a href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md">https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md</a></p>

    <p>The primary metric for evaluation will be the Mean <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss">Log Loss</a>, and the secondary metric for the evaluation with be the Mean <a href="https://en.wikipedia.org/wiki/F1_score">F1-Score</a>.</p>

    <p>The <strong>Mean Log Loss</strong> is defined by</p>

    <p>$ L = - \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^{C} y_{nc} \ln(p_{nc}), $</p>

    <p>where</p>

    <ul>
      <li>$N=35000$ is the number of examples in the test set,</li>
      <li>$C=16$ is the number of class labels, i.e. genres,</li>
      <li>$y_{nc}$ is a binary value indicating if the n-th instance belongs to the c-th label,</li>
      <li>$p_{nc}$ is the probability according to your submission that the n-th instance belongs to the c-th label,</li>
      <li>$\ln$ is the natural logarithmic function.</li>
    </ul>

    <p>The $F_1$ score for a particular class $c$ is given by</p>

    <p>$ F_1^c = 2\frac{p^c r^c}{p^c + r^c}, $</p>

    <p>where</p>

    <ul>
      <li>$p^c = \frac{tp^c}{tp^c + fp^c}$ is the precision for class $c$,</li>
      <li>$r^c = \frac{tp^c}{tp^c + fn^c}$ is the recall for class $c$,</li>
      <li>$tp^c$ refers to the number of True Positives for class $c$,</li>
      <li>$fp^c$ refers to the number of False Positives for class $c$,</li>
      <li>$fn^c$ refers to the number of False Negatives for class $c$.</li>
    </ul>

    <p>The final <strong>Mean $F_1$ Score</strong> is then defined as</p>

    <p>$ F_1 = \frac{1}{C} \sum_{c=1}^{C} F_1^c. $</p>

    <p>The participants have to submit a CSV file with the following header:</p>

    <p><code>
    file_id,Blues,Classical,Country,Easy Listening,Electronic,Experimental,Folk,Hip-Hop,Instrumental,International,Jazz,Old-Time / Historic,Pop,Rock,Soul-RnB,Spoken
    </code></p>

    <p>Each row is then an entry for every file in the test set (in the sorted order of the <code>file_id</code>s). The first column in every row represents the <code>file_id</code> (which is the name of the test file without its <code>.mp3</code> extension) and the rest of the $C=16$ columns are the predicted probabilities for each class in the order mentioned in the above CSV header.</p>
  rules_markdown: "The following rules have to be observed by all participants:\r\n\r\n*
    Participants are allowed at most 5 submissions per day.\r\n* Participants are
    welcome to form teams. Teams should submit their predictions under a single account.
    The submitted paper will mention all members.\r\n* Participants have to release
    their solution under an [Open Source License](https://opensource.org/licenses)
    of their choice to be eligible for prizes. We encourage all participants to open-source
    their code!\r\n* While submissions by Admins and Organizers can serve as baselines,
    they won't be considered in the final leaderboard.\r\n* Training must be done
    with the audio from the FMA medium subset only. In particular, the large and full
    subsets must not be used. Neither should `fma_features.csv` (from `fma_metadata.zip`),
    which was computed on the full set.\r\n* Metadata, e.g. the song title or artist
    name, cannot be used for the prediction. The submitted algorithms shall learn
    to map an audio signal, i.e. a time series, to one of the 16 target genres. In
    particular, no information (audio features or metadata) from external websites
    or APIs can be used. This will be enforced in the second round, when the submitted
    systems will only have access to a set of mp3s to make predictions.\r\n* In case
    of conflicts, the decision of the Organizers will be final and binding.\r\n* Organizers
    reserve the right to make changes to the rules and timeline.\r\n* Violation of
    the rules or other unfair activity may result in disqualification.\r\n\r\nFor
    the second round, the docker containers will have access to the following resources
    with a timeout of **10 hours** :\r\n\r\n* 1 Nvidia GTX GeForce 1080 Ti (11 GB
    GDDR5X)\r\n* 5 cores of an Intel Xeon E5-2650 v4 (2.20-2.90 GHz)\r\n* 60 GB of
    RAM\r\n* 100 GB of disk\r\n* no network access\r\n\r\nGuidelines for packaging
    the code, and submitting your models for Round-2 are now available at :    \r\n\r\n*
    [Round2_packaging_guidelines.md](https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_packaging_guidelines.md).
    \ \r\n* [Round2_submission_guidelines.md](https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_submission_guidelines.md)."
  rules: |
    <p>The following rules have to be observed by all participants:</p>

    <ul>
      <li>Participants are allowed at most 5 submissions per day.</li>
      <li>Participants are welcome to form teams. Teams should submit their predictions under a single account. The submitted paper will mention all members.</li>
      <li>Participants have to release their solution under an <a href="https://opensource.org/licenses">Open Source License</a> of their choice to be eligible for prizes. We encourage all participants to open-source their code!</li>
      <li>While submissions by Admins and Organizers can serve as baselines, they won’t be considered in the final leaderboard.</li>
      <li>Training must be done with the audio from the FMA medium subset only. In particular, the large and full subsets must not be used. Neither should <code>fma_features.csv</code> (from <code>fma_metadata.zip</code>), which was computed on the full set.</li>
      <li>Metadata, e.g. the song title or artist name, cannot be used for the prediction. The submitted algorithms shall learn to map an audio signal, i.e. a time series, to one of the 16 target genres. In particular, no information (audio features or metadata) from external websites or APIs can be used. This will be enforced in the second round, when the submitted systems will only have access to a set of mp3s to make predictions.</li>
      <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
      <li>Organizers reserve the right to make changes to the rules and timeline.</li>
      <li>Violation of the rules or other unfair activity may result in disqualification.</li>
    </ul>

    <p>For the second round, the docker containers will have access to the following resources with a timeout of <strong>10 hours</strong> :</p>

    <ul>
      <li>1 Nvidia GTX GeForce 1080 Ti (11 GB GDDR5X)</li>
      <li>5 cores of an Intel Xeon E5-2650 v4 (2.20-2.90 GHz)</li>
      <li>60 GB of RAM</li>
      <li>100 GB of disk</li>
      <li>no network access</li>
    </ul>

    <p>Guidelines for packaging the code, and submitting your models for Round-2 are now available at :</p>

    <ul>
      <li><a href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_packaging_guidelines.md">Round2_packaging_guidelines.md</a>.</li>
      <li><a href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_submission_guidelines.md">Round2_submission_guidelines.md</a>.</li>
    </ul>
  prizes_markdown: "The winner will be invited to present their solution to the 3rd
    [Applied Machine Learning Days](https://www.appliedmldays.org) at EPFL in Switzerland
    in January 2019, with travel and accommodation covered (up to $2000).\r\n\r\nMoreover,
    all participants are invited to submit a paper to the [Web Conference (WWW2018)
    challenges track](https://www2018.thewebconf.org/program/challenges-track/). The
    paper should describe the proposed solution and self-assessments of its performance.
    Papers must be submitted in PDF on [EasyChair](https://easychair.org/conferences/?conf=www2018-fma-challenge)
    for peer-review. The template to use is [ACM](https://www.acm.org/publications/proceedings-template),
    selecting the \"sigconf\" sample (as for the main conference). Submissions should
    not exceed five pages including any diagrams or appendices, plus unlimited pages
    of references. As the challenge is run publicly, reviews are not double-blind
    and papers should not be anonymized. Accepted papers will be published in the
    official satellite proceedings of the conference. As the challenge will continue
    after the submission deadline, authors of accepted papers will have the opportunity
    to submit a camera-ready version which will incorporate their latest tweaks. The
    event at the conference will be like a workshop, where participants present their
    solutions and we announce the winners.\r\n\r\n## Timeline\r\n\r\nBelow is the
    timeline of the challenge:\r\n\r\n* 2017-12-07 Challenge start.\r\n* 2018-02-09
    Paper submission deadline.\r\n* 2018-02-14 Paper acceptance notification.\r\n*
    2018-03-01 End of the first round. No new participants can enroll.\r\n* 2018-04-08
    Participants have to submit a docker container for the second round.\r\n* 2018-04-27
    Announcement of winners and presentation of accepted papers at the conference."
  prizes: |
    <p>The winner will be invited to present their solution to the 3rd <a href="https://www.appliedmldays.org">Applied Machine Learning Days</a> at EPFL in Switzerland in January 2019, with travel and accommodation covered (up to $2000).</p>

    <p>Moreover, all participants are invited to submit a paper to the <a href="https://www2018.thewebconf.org/program/challenges-track/">Web Conference (WWW2018) challenges track</a>. The paper should describe the proposed solution and self-assessments of its performance. Papers must be submitted in PDF on <a href="https://easychair.org/conferences/?conf=www2018-fma-challenge">EasyChair</a> for peer-review. The template to use is <a href="https://www.acm.org/publications/proceedings-template">ACM</a>, selecting the “sigconf” sample (as for the main conference). Submissions should not exceed five pages including any diagrams or appendices, plus unlimited pages of references. As the challenge is run publicly, reviews are not double-blind and papers should not be anonymized. Accepted papers will be published in the official satellite proceedings of the conference. As the challenge will continue after the submission deadline, authors of accepted papers will have the opportunity to submit a camera-ready version which will incorporate their latest tweaks. The event at the conference will be like a workshop, where participants present their solutions and we announce the winners.</p>

    <h2 id="timeline">Timeline</h2>

    <p>Below is the timeline of the challenge:</p>

    <ul>
      <li>2017-12-07 Challenge start.</li>
      <li>2018-02-09 Paper submission deadline.</li>
      <li>2018-02-14 Paper acceptance notification.</li>
      <li>2018-03-01 End of the first round. No new participants can enroll.</li>
      <li>2018-04-08 Participants have to submit a docker container for the second round.</li>
      <li>2018-04-27 Announcement of winners and presentation of accepted papers at the conference.</li>
    </ul>
  resources_markdown: "Please refer to the [dataset page](https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/dataset_files)
    for more information about the training and test data, as well as download links.\r\n\r\nThe
    [starter kit](https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit)
    includes code to handle the data an make a submission. Moreover, it features some
    examples and baselines.\r\n\r\nYou are encouraged to check out the [FMA dataset
    GitHub repository](https://github.com/mdeff/fma) for Jupyter notebooks showing
    how to use the data, exploring it, and training baseline models. This challenge
    uses the `rc1` version of the data, make sure to checkout that version of the
    code. The associated [paper](https://arxiv.org/abs/1612.01840) describes the data.\r\n\r\nAdditional
    resources:\r\n\r\n* [A list of scientific articles about deep learning applied
    to music](https://github.com/ybayle/awesome-deep-learning-music)\r\n* [A Tutorial
    on Deep Learning for Music Information Retrieval](https://arxiv.org/abs/1709.04396)\r\n*
    [A list of python software/tools for scientific research in audio/music](https://github.com/faroit/awesome-python-scientific-audio)\r\n*
    [Deep learning tutorial for genre recognition with Keras](https://github.com/tuwien-musicir/DL_MIR_Tutorial)\r\n*
    [Music Genre Classification with Deep Learning in TensorFlow](https://github.com/mlachmish/MusicGenreClassification)\r\n\r\nPublic
    contact channels:\r\n\r\n* [Gitter channel](https://gitter.im/crowdAI/WWW-Music-Genre-Recognition-Challenge)\r\n*
    [Discussion forum](https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at:\r\n\r\n*  [michael.defferrard@epfl.ch](mailto:michael.defferrard@epfl.ch){:target='_blank'}\r\n*
    \ [sharada.mohanty@epfl.ch](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <p>Please refer to the <a href="https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/dataset_files">dataset page</a> for more information about the training and test data, as well as download links.</p>

    <p>The <a href="https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit">starter kit</a> includes code to handle the data an make a submission. Moreover, it features some examples and baselines.</p>

    <p>You are encouraged to check out the <a href="https://github.com/mdeff/fma">FMA dataset GitHub repository</a> for Jupyter notebooks showing how to use the data, exploring it, and training baseline models. This challenge uses the <code>rc1</code> version of the data, make sure to checkout that version of the code. The associated <a href="https://arxiv.org/abs/1612.01840">paper</a> describes the data.</p>

    <p>Additional resources:</p>

    <ul>
      <li><a href="https://github.com/ybayle/awesome-deep-learning-music">A list of scientific articles about deep learning applied to music</a></li>
      <li><a href="https://arxiv.org/abs/1709.04396">A Tutorial on Deep Learning for Music Information Retrieval</a></li>
      <li><a href="https://github.com/faroit/awesome-python-scientific-audio">A list of python software/tools for scientific research in audio/music</a></li>
      <li><a href="https://github.com/tuwien-musicir/DL_MIR_Tutorial">Deep learning tutorial for genre recognition with Keras</a></li>
      <li><a href="https://github.com/mlachmish/MusicGenreClassification">Music Genre Classification with Deep Learning in TensorFlow</a></li>
    </ul>

    <p>Public contact channels:</p>

    <ul>
      <li><a href="https://gitter.im/crowdAI/WWW-Music-Genre-Recognition-Challenge">Gitter channel</a></li>
      <li><a href="https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/topics">Discussion forum</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

    <ul>
      <li><a href="mailto:michael.defferrard@epfl.ch" target="_blank">michael.defferrard@epfl.ch</a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank">sharada.mohanty@epfl.ch</a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: Screen_Shot_2017-11-09_at_07.56.11.png
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### WWW 2018 Challenge: Learning to Recognize Musical
    Genre Results\r\n<br><br>\r\n\r\n| Rank | Participant | Prize |\r\n| **1.** |
    [minzwon](https://www.crowdai.org/participants/minzwon) | Invitation to [AMLD2019](https://www.appliedmldays.org/)
    |"
  winner_description: |
    <h3 id="www-2018-challenge-learning-to-recognize-musical-genre-results">WWW 2018 Challenge: Learning to Recognize Musical Genre Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/minzwon">minzwon</a></td>
          <td>Invitation to <a href="https://www.appliedmldays.org/">AMLD2019</a></td>
        </tr>
      </tbody>
    </table>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: 
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_27:
  id: 27
  organizer_id: 13
  challenge: IEEE Investment Ranking Challenge
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &28 2018-02-09 15:27:19.387044000 Z
    zone: *2
    time: *28
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &29 2018-10-04 09:10:54.144860000 Z
    zone: *2
    time: *29
  tagline: Explore methodology to identify assets with extreme positive or negative
    returns.
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 10674
  participant_count: 194
  submission_count: 541
  score_title: Spearman correlation
  score_secondary_title: NDCG
  slug: ieee-investment-ranking-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: IEEEInvestmentRankingChallenge
  online_grading: true
  vote_count: 42
  description_markdown: "Using the provided data sets of financial predictors and
    semi-annual returns, participants are challenged to develop a model that will
    help identify the best-performing stocks in each time-period.\r\n\r\nResearch
    Question: **_Which stocks will experience the highest and lowest returns during
    the next six months?_**\r\n\r\nOut of the thousands of stocks in the market, small
    groups will experience exceptionally high or low returns. Considering the distribution
    of stock returns, a portfolio manager must buy the stocks in the right tail of
    the distribution and avoid the stocks in the left tail. The performance of an
    entire equity portfolio is often driven by these key investment decisions. The
    goal of this challenge is to explore methodology that will increase the probability
    that portfolio managers identify these stocks with extreme positive or negative
    returns. \r\n\r\nEach team must create a model that ranks a set of stocks based
    on the expected return over a forward 6-month window. This model can be a risk
    factor-based strategy (multi-factor model), predictive model, or any other data-based
    heuristic. There are many ways to approach this task and creative, non-traditional
    solutions are strongly encouraged. The final model will be tested on each 6-month
    period from 2002 to 2017.\r\n\r\n##Motivation\r\n\r\nAnalysts rely on a mix of
    quantitative and qualitative methodology to help investors consistently outperform
    the market. It’s not enough to be investment experts. Having the right data at
    the right time plays a critical role in successfully anticipating economic and
    environmental changes that may impact investment performance. Personalized solutions
    can be designed to provide a tailored mix of risk and return. Current baseline
    solutions rely on simple regressions and/or random forest solutions. Current approaches
    have high explanatory value and low predictive value. Improved solutions would
    increase predictive accuracy.\r\n\r\n##Dataset\r\n\r\nTeams are provided with
    predictors and semi-annual returns for a group of stocks from `1996` to `2017`.
    This span of **21 years** is represented as **42 non-overlapping 6-month periods**.
    In each of the `42 time periods`, roughly **900 stocks** with the largest market
    capitalization (i.e., total market value in USD) were selected. Therefore, the
    selected set of stocks at each time period changes as companies increase or decrease
    in value. **All stock identifiers have been removed** and **all numeric variables
    have been anonymized and normalized**. Training and test datasets were created
    by selecting a **random sample of stocks** at each time period. `60%` of stocks
    were sampled into the training set and the remaining `40%` created the test set.
    Finally, all data from the second half of 2017 was allocated to the test set.
    This 6-month period will provide a final out-of-sample test of a model’s performance.\r\n\r\n**Note**
    : Please refer to the [starter-kit](https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit)
    to quickly get started with the dataset, train a simple Random Forest based model,
    and make an example submission to crowdai.\r\n\r\n##Evaluation\r\n\r\nConsistent
    performance over time and through varying market conditions is crucial for any
    financial model. Each team must test their model using an expanding window procedure.
    For a given time period, $$T$$, an expanding window test allows the model to incorporate
    all available information up to time $$T$$, to generate predictions for time $$T+1$$.
    For example, when predicting the stock rankings in the first half of 2016, the
    model can include all data from 1996 to no later than year-end 2015. Predictions
    for the second half of 2016 could then include all the data from the first half
    of 2016. The quality of the predicted rankings at each time period will be evaluated
    in two ways, described below. \r\n\r\n\r\n* **Spearman correlation**: This metric
    will describe the overall relationship between the actual rankings and the predicted
    rankings from the model. Higher values indicate better performance.\r\n\r\n\r\n*
    **Normalized Discounted Cumulative Gain of Top 20%**: In reality, analysts and
    portfolio managers are not concerned with the entire distribution of stocks. They
    will instead focus on identifying and buying the best-ranking stocks. Normalized
    Discounted Cumulative Gain (NDCG) is a metric from the information retrieval domain
    that considers the relevance and confidence (rank position) to describe a model's
    rank quality.\r\n \r\n\r\n##Technical Details\r\n\r\n###Spearman correlation\r\n\r\nSpearman
    correlation describes how well a model is ranking the stocks at a given time period.
    Spearman correlation is calculated using the formula below.\r\n\r\n$$r_s = 1 -
    \\frac{6 \\sum_i{}{d_i^2}}{n(n^2 - 1)}$$\r\n\r\nWhere $d_i$ is the difference
    between the predicted and actual ranking of stock **_i_**.\r\n\r\nSpearman correlation
    has a range from -1 to 1. Models that rank stocks more accurately will produce
    higher Spearman correlation values. Correlation values will be averaged across
    all time periods.\r\n\r\n[ Click here for more details on Spearman correlation.
    ](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient){:target=''}\r\n\r\n###
    Normalized Discounted Cumulative Gain of Top 20%\r\n\r\nNormalized Discounted
    Cumulative Gain (NDCG) is the ratio between the Discounted Cumulative Gain (DCG)
    and Ideal Discounted Cumulative Gain (IDCG), shown below.\r\n\r\n$$NDCG = \\frac{DCG}{IDCG}$$\r\n\r\nWhere:\r\n\r\n$$DCG
    = \\sum_{i=1}^n{\\frac{n}{n+i} Z_i}$$\r\n\r\n$$Z_i$$ represents the normalized
    future 6-month return (Norm_Ret_F6M in the dataset) of the $$i^{th}$$ ranked stock.
    With this formula, stocks with better (lower) predicted ranks will have more influence
    on the ranking quality than stocks with higher predicted ranks. IDCG is the maximum
    possible DCG, which gives the NDCG score an upper bound of 1. The NDCG will be
    calculated for each individual 6-month period and then averaged across all periods.\r\n\r\nNote
    that the NDCG is calculated using only the top 20% of a model's predicted rankings.
    Therefore, NDCG rewards correctly identifying stocks in the top 20% and ranking
    them in the correct order. This aligns with the viewpoint of a 'long-only' portfolio
    manager who will focus on buying the best stocks and ignore stocks outside the
    top 20%.\r\n\r\n[ A more detailed description of NDCG can be found here. ](https://en.wikipedia.org/wiki/Discounted_cumulative_gain){:target=''}
    This challenge uses a modified formulation of DCG that is tailored to investment
    ranking.\r\n\r\n**Update: The evaluation script was incorrectly calculating NDCG
    as of the challenge launch. This was fixed 03/28. Solutions submitted prior to
    this date would have provided incorrect results.**\r\n\r\n#Testing your solution\r\n\r\nThroughout
    the competition, teams will be given the opportunity to evaluate their models
    on the test dataset. Teams can sign in and upload their predictions up to 5 times
    per day. This will provide an estimate for out-of-sample performance during the
    competition. Teams should rely on internal model validation procedures and be
    careful not to optimize results to this one small section of the test dataset.\r\n\r\n#Timeline\r\n\r\n\r\n*
    **March 26** : Challenge launch and start of Round 1 - contestants create models
    and upload predictions to crowdAI.\r\n\r\n* **April 30** : Deadline for Round
    1. All solutions must be submitted by 11:59 GMT; Top solutions from leader board
    invited to Round 2.\r\n\r\n* **May 1** : Start of Round 2 - Contestants explain
    their methods, results, and conclusions in short paper. Contestants also package
    code of submitted solution using Docker for testing and evaluation.\r\n\r\n\r\n*
    **May 20** : Deadline for Round 2. All solutions must be submitted by 11:59 GMT.\r\n\r\n\r\n*
    **May 21** :  Top 6 solutions selected; Winners provided travel stipend (maximum
    USD 1000) and invitation to present at the IEEE Data Science Workshop in Lausanne,
    Switzerland June 4 - June 6.\r\n\r\n\r\n# Details for Round 2:\r\n\r\nRound 2
    is open to all challenge participants. Round 1 focused on prototyping models that
    maximized statistical measures and Round 2 will enhance this with a deeper dive
    into your methodology and a new set of holdout data from 2017. To compete, all
    participants must submit the following items:\r\n\r\n\r\n* Final predictions for
    all time periods\r\n\r\n* A brief written solution using the [IEEE template for
    conference proceedings.](https://www.ieee.org/conferences/publishing/templates.html){:target=''}
    MS Word and LaTeX are both acceptable. At a minimum, the document should include
    an introduction, description of your methodology, results, and any other information
    needed to understand your solution and its merit. Tables, charts, and other visuals
    are highly encouraged.\r\n\r\n* All code and files needed to reproduce your results
    uploaded to Gitlab. (More details on this soon)\r\n\r\nThe top 6 solutions will
    be selected based on their statistical performance, calculated in the following
    manner:\r\n\r\nFinal score = (A+B+C+D)/4\r\n\r\nWhere:\r\n\r\n\r\n* A = Rank of
    spearman correlation on holdout data from 2002 – 2016\r\n\r\n\r\n* B = Rank of
    NDCG score on holdout data from 2002 – 2016\r\n\r\n\r\n* C = Rank of spearman
    correlation on holdout data from 2017\r\n\r\n\r\n* D = Rank of NDCG score on holdout
    data from 2017\r\n\r\n\r\nAll ranks will be determined using 3 significant digits.
    Performance on the data from 2017 will be used as a tiebreaker if needed. \r\n\r\n##
    Round 2 Submission Details \r\n**UPDATE**\r\n\r\nThe Round-2 of the IEEE Investment
    Ranking Challenge is now accepting submissions.\r\nPlease remember to update your
    crowdai client and follow the instructions [here](https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit#submission-of-predicted-file-to-crowdai)
    before making a submission.\r\n\r\nWe will accept the submissions until 21st of
    May, and to be eligible for the final leaderboard, you will also have to upload
    your code as a private repository to [gitlab.crowdai.org](https://gitlab.crowdai.org/).\r\n\r\nPlease
    add the following users as Members  of your private repository : benharlander,
    spMohanty\r\n\r\nApart from your code, please include a description of your approach
    using [this template](https://www.acm.org/publications/proceedings-template)\r\n\r\nThe
    leaderboard of this challenge will be shown only at the end of the Round on May
    21st, but the grading status of your submissions can still be checked under the
    Submissions Tab.\r\n\r\n"
  description: |+
    <p>Using the provided data sets of financial predictors and semi-annual returns, participants are challenged to develop a model that will help identify the best-performing stocks in each time-period.</p>

    <p>Research Question: <strong><em>Which stocks will experience the highest and lowest returns during the next six months?</em></strong></p>

    <p>Out of the thousands of stocks in the market, small groups will experience exceptionally high or low returns. Considering the distribution of stock returns, a portfolio manager must buy the stocks in the right tail of the distribution and avoid the stocks in the left tail. The performance of an entire equity portfolio is often driven by these key investment decisions. The goal of this challenge is to explore methodology that will increase the probability that portfolio managers identify these stocks with extreme positive or negative returns.</p>

    <p>Each team must create a model that ranks a set of stocks based on the expected return over a forward 6-month window. This model can be a risk factor-based strategy (multi-factor model), predictive model, or any other data-based heuristic. There are many ways to approach this task and creative, non-traditional solutions are strongly encouraged. The final model will be tested on each 6-month period from 2002 to 2017.</p>

    <h2 id="motivation">Motivation</h2>

    <p>Analysts rely on a mix of quantitative and qualitative methodology to help investors consistently outperform the market. It’s not enough to be investment experts. Having the right data at the right time plays a critical role in successfully anticipating economic and environmental changes that may impact investment performance. Personalized solutions can be designed to provide a tailored mix of risk and return. Current baseline solutions rely on simple regressions and/or random forest solutions. Current approaches have high explanatory value and low predictive value. Improved solutions would increase predictive accuracy.</p>

    <h2 id="dataset">Dataset</h2>

    <p>Teams are provided with predictors and semi-annual returns for a group of stocks from <code class="highlighter-rouge">1996</code> to <code class="highlighter-rouge">2017</code>. This span of <strong>21 years</strong> is represented as <strong>42 non-overlapping 6-month periods</strong>. In each of the <code class="highlighter-rouge">42 time periods</code>, roughly <strong>900 stocks</strong> with the largest market capitalization (i.e., total market value in USD) were selected. Therefore, the selected set of stocks at each time period changes as companies increase or decrease in value. <strong>All stock identifiers have been removed</strong> and <strong>all numeric variables have been anonymized and normalized</strong>. Training and test datasets were created by selecting a <strong>random sample of stocks</strong> at each time period. <code class="highlighter-rouge">60%</code> of stocks were sampled into the training set and the remaining <code class="highlighter-rouge">40%</code> created the test set. Finally, all data from the second half of 2017 was allocated to the test set. This 6-month period will provide a final out-of-sample test of a model’s performance.</p>

    <p><strong>Note</strong> : Please refer to the <a href="https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit">starter-kit</a> to quickly get started with the dataset, train a simple Random Forest based model, and make an example submission to crowdai.</p>

    <h2 id="evaluation">Evaluation</h2>

    <p>Consistent performance over time and through varying market conditions is crucial for any financial model. Each team must test their model using an expanding window procedure. For a given time period, <script type="math/tex">T</script>, an expanding window test allows the model to incorporate all available information up to time <script type="math/tex">T</script>, to generate predictions for time <script type="math/tex">T+1</script>. For example, when predicting the stock rankings in the first half of 2016, the model can include all data from 1996 to no later than year-end 2015. Predictions for the second half of 2016 could then include all the data from the first half of 2016. The quality of the predicted rankings at each time period will be evaluated in two ways, described below.</p>

    <ul>
      <li>
        <p><strong>Spearman correlation</strong>: This metric will describe the overall relationship between the actual rankings and the predicted rankings from the model. Higher values indicate better performance.</p>
      </li>
      <li>
        <p><strong>Normalized Discounted Cumulative Gain of Top 20%</strong>: In reality, analysts and portfolio managers are not concerned with the entire distribution of stocks. They will instead focus on identifying and buying the best-ranking stocks. Normalized Discounted Cumulative Gain (NDCG) is a metric from the information retrieval domain that considers the relevance and confidence (rank position) to describe a model’s rank quality.</p>
      </li>
    </ul>

    <h2 id="technical-details">Technical Details</h2>

    <h3 id="spearman-correlation">Spearman correlation</h3>

    <p>Spearman correlation describes how well a model is ranking the stocks at a given time period. Spearman correlation is calculated using the formula below.</p>

    <script type="math/tex; mode=display">r_s = 1 - \frac{6 \sum_i{}{d_i^2}}{n(n^2 - 1)}</script>

    <p>Where $d_i$ is the difference between the predicted and actual ranking of stock <strong><em>i</em></strong>.</p>

    <p>Spearman correlation has a range from -1 to 1. Models that rank stocks more accurately will produce higher Spearman correlation values. Correlation values will be averaged across all time periods.</p>

    <p><a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" target=""> Click here for more details on Spearman correlation. </a></p>

    <h3 id="normalized-discounted-cumulative-gain-of-top-20">Normalized Discounted Cumulative Gain of Top 20%</h3>

    <p>Normalized Discounted Cumulative Gain (NDCG) is the ratio between the Discounted Cumulative Gain (DCG) and Ideal Discounted Cumulative Gain (IDCG), shown below.</p>

    <script type="math/tex; mode=display">NDCG = \frac{DCG}{IDCG}</script>

    <p>Where:</p>

    <script type="math/tex; mode=display">DCG = \sum_{i=1}^n{\frac{n}{n+i} Z_i}</script>

    <p><script type="math/tex">Z_i</script> represents the normalized future 6-month return (Norm_Ret_F6M in the dataset) of the <script type="math/tex">i^{th}</script> ranked stock. With this formula, stocks with better (lower) predicted ranks will have more influence on the ranking quality than stocks with higher predicted ranks. IDCG is the maximum possible DCG, which gives the NDCG score an upper bound of 1. The NDCG will be calculated for each individual 6-month period and then averaged across all periods.</p>

    <p>Note that the NDCG is calculated using only the top 20% of a model’s predicted rankings. Therefore, NDCG rewards correctly identifying stocks in the top 20% and ranking them in the correct order. This aligns with the viewpoint of a ‘long-only’ portfolio manager who will focus on buying the best stocks and ignore stocks outside the top 20%.</p>

    <p><a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain" target=""> A more detailed description of NDCG can be found here. </a> This challenge uses a modified formulation of DCG that is tailored to investment ranking.</p>

    <p><strong>Update: The evaluation script was incorrectly calculating NDCG as of the challenge launch. This was fixed 03/28. Solutions submitted prior to this date would have provided incorrect results.</strong></p>

    <h1 id="testing-your-solution">Testing your solution</h1>

    <p>Throughout the competition, teams will be given the opportunity to evaluate their models on the test dataset. Teams can sign in and upload their predictions up to 5 times per day. This will provide an estimate for out-of-sample performance during the competition. Teams should rely on internal model validation procedures and be careful not to optimize results to this one small section of the test dataset.</p>

    <h1 id="timeline">Timeline</h1>

    <ul>
      <li>
        <p><strong>March 26</strong> : Challenge launch and start of Round 1 - contestants create models and upload predictions to crowdAI.</p>
      </li>
      <li>
        <p><strong>April 30</strong> : Deadline for Round 1. All solutions must be submitted by 11:59 GMT; Top solutions from leader board invited to Round 2.</p>
      </li>
      <li>
        <p><strong>May 1</strong> : Start of Round 2 - Contestants explain their methods, results, and conclusions in short paper. Contestants also package code of submitted solution using Docker for testing and evaluation.</p>
      </li>
      <li>
        <p><strong>May 20</strong> : Deadline for Round 2. All solutions must be submitted by 11:59 GMT.</p>
      </li>
      <li>
        <p><strong>May 21</strong> :  Top 6 solutions selected; Winners provided travel stipend (maximum USD 1000) and invitation to present at the IEEE Data Science Workshop in Lausanne, Switzerland June 4 - June 6.</p>
      </li>
    </ul>

    <h1 id="details-for-round-2">Details for Round 2:</h1>

    <p>Round 2 is open to all challenge participants. Round 1 focused on prototyping models that maximized statistical measures and Round 2 will enhance this with a deeper dive into your methodology and a new set of holdout data from 2017. To compete, all participants must submit the following items:</p>

    <ul>
      <li>
        <p>Final predictions for all time periods</p>
      </li>
      <li>
        <p>A brief written solution using the <a href="https://www.ieee.org/conferences/publishing/templates.html" target="">IEEE template for conference proceedings.</a> MS Word and LaTeX are both acceptable. At a minimum, the document should include an introduction, description of your methodology, results, and any other information needed to understand your solution and its merit. Tables, charts, and other visuals are highly encouraged.</p>
      </li>
      <li>
        <p>All code and files needed to reproduce your results uploaded to Gitlab. (More details on this soon)</p>
      </li>
    </ul>

    <p>The top 6 solutions will be selected based on their statistical performance, calculated in the following manner:</p>

    <p>Final score = (A+B+C+D)/4</p>

    <p>Where:</p>

    <ul>
      <li>
        <p>A = Rank of spearman correlation on holdout data from 2002 – 2016</p>
      </li>
      <li>
        <p>B = Rank of NDCG score on holdout data from 2002 – 2016</p>
      </li>
      <li>
        <p>C = Rank of spearman correlation on holdout data from 2017</p>
      </li>
      <li>
        <p>D = Rank of NDCG score on holdout data from 2017</p>
      </li>
    </ul>

    <p>All ranks will be determined using 3 significant digits. Performance on the data from 2017 will be used as a tiebreaker if needed.</p>

    <h2 id="round-2-submission-details">Round 2 Submission Details</h2>
    <p><strong>UPDATE</strong></p>

    <p>The Round-2 of the IEEE Investment Ranking Challenge is now accepting submissions.
    Please remember to update your crowdai client and follow the instructions <a href="https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit#submission-of-predicted-file-to-crowdai">here</a> before making a submission.</p>

    <p>We will accept the submissions until 21st of May, and to be eligible for the final leaderboard, you will also have to upload your code as a private repository to <a href="https://gitlab.crowdai.org/">gitlab.crowdai.org</a>.</p>

    <p>Please add the following users as Members  of your private repository : benharlander, spMohanty</p>

    <p>Apart from your code, please include a description of your approach using <a href="https://www.acm.org/publications/proceedings-template">this template</a></p>

    <p>The leaderboard of this challenge will be shown only at the end of the Round on May 21st, but the grading status of your submissions can still be checked under the Submissions Tab.</p>

  evaluation_markdown: "\r\n\r\n"
  evaluation: "\n"
  rules_markdown: ''
  rules: "\n"
  prizes_markdown: Top-6 participants on the leaderboard (except the organizers) will
    be invited (maximum USD 1000 travel stipend, provided by Principal Financial Group)
    to present at the [IEEE Data Science Workshop](https://2018.ieeedatascience.org/){:target='_blank'}
    in Lausanne, June 4-6, 2018.
  prizes: '<p>Top-6 participants on the leaderboard (except the organizers) will be
    invited (maximum USD 1000 travel stipend, provided by Principal Financial Group)
    to present at the <a href="https://2018.ieeedatascience.org/" target="_blank">IEEE
    Data Science Workshop</a> in Lausanne, June 4-6, 2018.</p>

'
  resources_markdown: "### Starter Kit \r\n\r\nA starter kit has been prepared which
    explains how to get access to the dataset, parse it, train a simple random forest
    based method, and make a submission.\r\nIt can be accessed at : [https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit](https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit)\r\n\r\n###
    Contact Us\r\n\r\n* Gitter Channel : [crowdAI/ieee-investment-ranking-challenge](https://gitter.im/crowdAI/ieee-investment-ranking-challenge)\r\n*
    Technical issues : [https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues
    ](https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues){:target='_blank'}\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics](https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organisers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  [ Harlander.Benjamin@principal.com
    ](mailto:Harlander.Benjamin@principal.com\r\n){:target='_blank'}\r\n*  [ sharada.mohanty@epfl.ch\r\n
    ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}"
  resources: |
    <h3 id="starter-kit">Starter Kit</h3>

    <p>A starter kit has been prepared which explains how to get access to the dataset, parse it, train a simple random forest based method, and make a submission.
    It can be accessed at : <a href="https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit">https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit</a></p>

    <h3 id="contact-us">Contact Us</h3>

    <ul>
      <li>Gitter Channel : <a href="https://gitter.im/crowdAI/ieee-investment-ranking-challenge">crowdAI/ieee-investment-ranking-challenge</a></li>
      <li>Technical issues : <a href="https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues" target="_blank">https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics">https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li><a href="mailto:Harlander.Benjamin@principal.com" target="_blank"> Harlander.Benjamin@principal.com </a></li>
      <li><a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch
     </a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: stock-market-2616931_1920.jpg
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### IEEE Investment Ranking Challenge Results\r\n<br><br>\r\n\r\n\r\n|
    Rank | Participant | `S1` | `S2` | `S3` | `S4` | `R1` | `R2` | `R3` | `R4` | Average
    rank | Prize & Notes |\r\n| **1.** | [mkoseoglu](https://www.crowdai.org/participants/mkoseoglu)
    | 0.2576 | 0.2577 | 0.2445 | 0.2188 | 9 | 9 | 10 | 10| 9.25 | Invitation to [IEEE
    Data Science Workshop](https://2018.ieeedatascience.org/). $1000 travel grant
    |\r\n| **2.** | [lance](https://www.crowdai.org/participants/lance) | 0.271 |
    0.2677 | 0.0328 | -0.0067 | 10 | 10 | 3 | 1 | 8.25 | Invitation to [IEEE Data
    Science Workshop](https://2018.ieeedatascience.org/). $1000 travel grant |\r\n|
    **3.** | [shanka](https://www.crowdai.org/participants/5128) | 0.1024 | 0.1187
    | 0.0967 | 0.0945 | 6 | 7 | 9 | 8 | 7.25 | Invitation to [IEEE Data Science Workshop](https://2018.ieeedatascience.org/).
    $1000 travel grant |\r\n| **4.** | [kvr](https://www.crowdai.org/participants/kvr)
    | 0.1915 | 0.19 | 0.0209 | 0.0525 | 8 | 8 | 2 | 5 | 6.5 | Invitation to [IEEE
    Data Science Workshop](https://2018.ieeedatascience.org/). $1000 travel grant
    |\r\n| **5.** | [Alphard.Liu](https://www.crowdai.org/participants/alphard-liu)
    | 0.0723 | 0.0787 | 0.0601 | 0.0409 | 4 | 5 | 6 | 3 | 5 | Invitation to [IEEE
    Data Science Workshop](https://2018.ieeedatascience.org/). $1000 travel grant
    <br> Won tie breaker for 5th over pranoot_hatwar due to better Round 2 results.|\r\n|
    **6.** | [pranoot_hatwar](https://www.crowdai.org/participants/pranoot_hatwar)
    | 0.1316 | 0.0882 | -0.0063 | 0.0525 | 7 | 6 | 1 | 6 | 5 | Invitation to [IEEE
    Data Science Workshop](https://2018.ieeedatascience.org/). $1000 travel grant
    |\r\n| **7.** | [storage_man](https://www.crowdai.org/participants/storage_man)
    | 0.0731 | 0.0664 | 0.0469 | 0.0413 | 5 | 4 | 5 | 4 | 4.5 | Not eligible. Documents
    required not submitted |\r\n| **8.** | [BearStrikesBack](https://www.crowdai.org/participants/bearstrikesback)
    | 0.0323 | 0.0333 | 0.0752 | 0.0247 | 3 | 3 | 7 | 2 | 4 | N/A |\r\n| **9.** |
    [aalekhn](https://www.crowdai.org/participants/aalekhn) | 0.0187 | 0.0052 | 0.0902
    | 0.1266 | 2 | 1 | 8 | 9 | 3 | Not eligible. Documents required not submitted
    |\r\n| **10.** | [spMohanty](https://www.crowdai.org/participants/spmohanty) |
    0.0023 | 0.023 | 0.0386 | 0.0682 | 1 | 2 | 4 | 7 | 2.25 | *Baseline test Submission*
    |\r\n\r\n<br>where :   \r\n\r\n*  `S1` : Round 1 Spearmann Correlation\r\n*  `S2`
    : Round 1 NDCG\r\n*  `S3` : Round 2  Spearmann Correlation\r\n*  `S4` : Round
    2 NDCG\r\n*  `R1` : Round 1 Spearmann Correlation Rank (inverted)\r\n*  `R2` :
    Round 1 NDCG Rank (inverted)\r\n*  `R3` : Round 2 Spearmann Correlation Rank (inverted)\r\n*
    `R4` : Round 2 NDCG Rank (inverted)\r\n\r\n"
  winner_description: |+
    <h3 id="ieee-investment-ranking-challenge-results">IEEE Investment Ranking Challenge Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td><code class="highlighter-rouge">S1</code></td>
          <td><code class="highlighter-rouge">S2</code></td>
          <td><code class="highlighter-rouge">S3</code></td>
          <td><code class="highlighter-rouge">S4</code></td>
          <td><code class="highlighter-rouge">R1</code></td>
          <td><code class="highlighter-rouge">R2</code></td>
          <td><code class="highlighter-rouge">R3</code></td>
          <td><code class="highlighter-rouge">R4</code></td>
          <td>Average rank</td>
          <td>Prize &amp; Notes</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/mkoseoglu">mkoseoglu</a></td>
          <td>0.2576</td>
          <td>0.2577</td>
          <td>0.2445</td>
          <td>0.2188</td>
          <td>9</td>
          <td>9</td>
          <td>10</td>
          <td>10</td>
          <td>9.25</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/lance">lance</a></td>
          <td>0.271</td>
          <td>0.2677</td>
          <td>0.0328</td>
          <td>-0.0067</td>
          <td>10</td>
          <td>10</td>
          <td>3</td>
          <td>1</td>
          <td>8.25</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/5128">shanka</a></td>
          <td>0.1024</td>
          <td>0.1187</td>
          <td>0.0967</td>
          <td>0.0945</td>
          <td>6</td>
          <td>7</td>
          <td>9</td>
          <td>8</td>
          <td>7.25</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>4.</strong></td>
          <td><a href="https://www.crowdai.org/participants/kvr">kvr</a></td>
          <td>0.1915</td>
          <td>0.19</td>
          <td>0.0209</td>
          <td>0.0525</td>
          <td>8</td>
          <td>8</td>
          <td>2</td>
          <td>5</td>
          <td>6.5</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>5.</strong></td>
          <td><a href="https://www.crowdai.org/participants/alphard-liu">Alphard.Liu</a></td>
          <td>0.0723</td>
          <td>0.0787</td>
          <td>0.0601</td>
          <td>0.0409</td>
          <td>4</td>
          <td>5</td>
          <td>6</td>
          <td>3</td>
          <td>5</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant <br /> Won tie breaker for 5th over pranoot_hatwar due to better Round 2 results.</td>
        </tr>
        <tr>
          <td><strong>6.</strong></td>
          <td><a href="https://www.crowdai.org/participants/pranoot_hatwar">pranoot_hatwar</a></td>
          <td>0.1316</td>
          <td>0.0882</td>
          <td>-0.0063</td>
          <td>0.0525</td>
          <td>7</td>
          <td>6</td>
          <td>1</td>
          <td>6</td>
          <td>5</td>
          <td>Invitation to <a href="https://2018.ieeedatascience.org/">IEEE Data Science Workshop</a>. $1000 travel grant</td>
        </tr>
        <tr>
          <td><strong>7.</strong></td>
          <td><a href="https://www.crowdai.org/participants/storage_man">storage_man</a></td>
          <td>0.0731</td>
          <td>0.0664</td>
          <td>0.0469</td>
          <td>0.0413</td>
          <td>5</td>
          <td>4</td>
          <td>5</td>
          <td>4</td>
          <td>4.5</td>
          <td>Not eligible. Documents required not submitted</td>
        </tr>
        <tr>
          <td><strong>8.</strong></td>
          <td><a href="https://www.crowdai.org/participants/bearstrikesback">BearStrikesBack</a></td>
          <td>0.0323</td>
          <td>0.0333</td>
          <td>0.0752</td>
          <td>0.0247</td>
          <td>3</td>
          <td>3</td>
          <td>7</td>
          <td>2</td>
          <td>4</td>
          <td>N/A</td>
        </tr>
        <tr>
          <td><strong>9.</strong></td>
          <td><a href="https://www.crowdai.org/participants/aalekhn">aalekhn</a></td>
          <td>0.0187</td>
          <td>0.0052</td>
          <td>0.0902</td>
          <td>0.1266</td>
          <td>2</td>
          <td>1</td>
          <td>8</td>
          <td>9</td>
          <td>3</td>
          <td>Not eligible. Documents required not submitted</td>
        </tr>
        <tr>
          <td><strong>10.</strong></td>
          <td><a href="https://www.crowdai.org/participants/spmohanty">spMohanty</a></td>
          <td>0.0023</td>
          <td>0.023</td>
          <td>0.0386</td>
          <td>0.0682</td>
          <td>1</td>
          <td>2</td>
          <td>4</td>
          <td>7</td>
          <td>2.25</td>
          <td><em>Baseline test Submission</em></td>
        </tr>
      </tbody>
    </table>

    <p><br />where :</p>

    <ul>
      <li><code class="highlighter-rouge">S1</code> : Round 1 Spearmann Correlation</li>
      <li><code class="highlighter-rouge">S2</code> : Round 1 NDCG</li>
      <li><code class="highlighter-rouge">S3</code> : Round 2  Spearmann Correlation</li>
      <li><code class="highlighter-rouge">S4</code> : Round 2 NDCG</li>
      <li><code class="highlighter-rouge">R1</code> : Round 1 Spearmann Correlation Rank (inverted)</li>
      <li><code class="highlighter-rouge">R2</code> : Round 1 NDCG Rank (inverted)</li>
      <li><code class="highlighter-rouge">R3</code> : Round 2 Spearmann Correlation Rank (inverted)</li>
      <li><code class="highlighter-rouge">R4</code> : Round 2 NDCG Rank (inverted)</li>
    </ul>

  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: ''
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_11:
  id: 11
  organizer_id: 8
  challenge: 'NIPS ''17 Workshop: Criteo Ad Placement Challenge'
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &30 2017-09-26 09:26:27.176620000 Z
    zone: *2
    time: *30
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &31 2018-10-04 09:10:10.201875000 Z
    zone: *2
    time: *31
  tagline: Counterfactual policy learning for display advertising
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 9326
  participant_count: 161
  submission_count: 246
  score_title: IPS
  score_secondary_title: IPS_std
  slug: nips-17-workshop-criteo-ad-placement-challenge
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: CriteoAdPlacementNIPS2017
  online_grading: true
  vote_count: 17
  description_markdown: "Welcome to the Criteo Ad Placement challenge, accompanying
    the [NIPS'17 Workshop on Causal Inference and Machine Learning](https://sites.google.com/view/causalnips2017){:target='_blank'}.
    \r\n\r\nConsider a display advertising scenario: a user arrives at a website where
    we have an advertisement slot (\"an impression\"). We have a pool of potential
    products to advertise during that impression (\"a candidate set\"). A \"policy\"
    chooses which product to display so as to maximize the number of clicks by the
    users.\r\n\r\nThe goal of the challenge is to find a good policy that, knowing
    the candidate set and impression context, chooses products to display such that
    the aggregate click-through-rate (\"CTR\") from deploying the policy is maximized.\r\n\r\nTo
    enable you to find a good policy, Criteo has generously donated several gigabytes
    of <user impression, candidate set, selected product, click/no-click> logs from
    a randomized version of their in-production policy! Go to the [Dataset section](https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files){:target='_blank'}
    of the challenge to access the dataset and visit the [GitHub repo](https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit){:target='_blank'}
    to access a starter-kit for counterfactual policy learning with the dataset.\r\n\r\n**Prizes
    worth $4500 sponsored by Criteo!**\r\n\r\n![criteo_research_logo.png](https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/18c2b716ccbcf02885aa6fb4c9e9e8c6_criteo_research_logo.png)\r\n\r\nThis
    policy learning challenge is subtly different from the [2014 Criteo challenge](https://www.kaggle.com/c/criteo-display-ad-challenge){:target='_blank'}
    to compare CTR prediction algorithms. CTR prediction is a standard supervised
    regression problem: for any ad in the training data, the correct regression target
    is given (click/no-click). For policy learning, when the in-production policy
    chose a product that was not clicked, we do not know \"what-if\" (counterfactual)
    we chose a different product to display. These differences and some baseline approaches
    are detailed in the [dataset companion paper](http://www.cs.cornell.edu/~adith/Criteo/){:target='_blank'}.\r\n\r\nThe
    objective of this challenge is to spur participants to explore several issues
    on a real-world benchmark dataset, and share their findings during the NIPS'17
    workshop, such as:\r\n\r\n* Discover new training objectives, learning algorithms
    and regularization mechanisms for counterfactual learning scenarios.\r\n* Find
    appropriate ways to perform model selection (analogous to cross-validation for
    supervised learning) using large amounts of logged interaction data.\r\n* Develop
    algorithms that can scale to massive datasets (typically orders of magnitude larger
    than labeled datasets).\r\n"
  description: |
    <p>Welcome to the Criteo Ad Placement challenge, accompanying the <a href="https://sites.google.com/view/causalnips2017" target="_blank">NIPS’17 Workshop on Causal Inference and Machine Learning</a>.</p>

    <p>Consider a display advertising scenario: a user arrives at a website where we have an advertisement slot (“an impression”). We have a pool of potential products to advertise during that impression (“a candidate set”). A “policy” chooses which product to display so as to maximize the number of clicks by the users.</p>

    <p>The goal of the challenge is to find a good policy that, knowing the candidate set and impression context, chooses products to display such that the aggregate click-through-rate (“CTR”) from deploying the policy is maximized.</p>

    <p>To enable you to find a good policy, Criteo has generously donated several gigabytes of &lt;user impression, candidate set, selected product, click/no-click&gt; logs from a randomized version of their in-production policy! Go to the <a href="https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files" target="_blank">Dataset section</a> of the challenge to access the dataset and visit the <a href="https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit" target="_blank">GitHub repo</a> to access a starter-kit for counterfactual policy learning with the dataset.</p>

    <p><strong>Prizes worth $4500 sponsored by Criteo!</strong></p>

    <p><img src="https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/18c2b716ccbcf02885aa6fb4c9e9e8c6_criteo_research_logo.png" alt="criteo_research_logo.png" /></p>

    <p>This policy learning challenge is subtly different from the <a href="https://www.kaggle.com/c/criteo-display-ad-challenge" target="_blank">2014 Criteo challenge</a> to compare CTR prediction algorithms. CTR prediction is a standard supervised regression problem: for any ad in the training data, the correct regression target is given (click/no-click). For policy learning, when the in-production policy chose a product that was not clicked, we do not know “what-if” (counterfactual) we chose a different product to display. These differences and some baseline approaches are detailed in the <a href="http://www.cs.cornell.edu/~adith/Criteo/" target="_blank">dataset companion paper</a>.</p>

    <p>The objective of this challenge is to spur participants to explore several issues on a real-world benchmark dataset, and share their findings during the NIPS’17 workshop, such as:</p>

    <ul>
      <li>Discover new training objectives, learning algorithms and regularization mechanisms for counterfactual learning scenarios.</li>
      <li>Find appropriate ways to perform model selection (analogous to cross-validation for supervised learning) using large amounts of logged interaction data.</li>
      <li>Develop algorithms that can scale to massive datasets (typically orders of magnitude larger than labeled datasets).</li>
    </ul>
  evaluation_markdown: "In the dataset, each impression is represented by `M` lines
    where `M` is the number of candidate ads. Each line has feature information for
    every other candidate ad.\r\nIn addition, the first line corresponds to the candidate
    that was displayed by the logging policy, an indicator whether the displayed product
    was clicked by the user (\"click\" encoded as `0.001`, \"no-click\" encoded as
    `0.999`), and the **inverse propensity** of the stochastic logging policy to pick
    that specific candidate (see the  [ companion paper ](http://www.cs.cornell.edu/~adith/Criteo/).
    for details).\r\nEach `<user context-candidate product>` pair is described using
    **33 categorical (multi-set) features and 2 numeric features**. Of these, 10 features
    are only-context-dependent while the remaining 25 features depend on both the
    context and the candidate product. These categorical feature representations have
    been post-processed to a 74000-dimensional vector with sparse one-hot encoding
    for each categorical feature. **The semantics behind the features will not be
    disclosed**. \r\n\r\nThese post-processed dataset files are available [ here ](https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files\r\n).\r\n\r\nYour
    task is to build a function `_policy` which takes `M` candidates, each represented
    by a **74000-dimensional sparse vector**, and outputs scores for each of the candidates
    between `1` and `M`.\r\n\r\nThe reward for an individual impression is, did the
    selected candidate get clicked _(reward = 1)_ or not _(reward = 0)_? The reward
    for function `f` is the aggregate reward over all impressions on a held out test
    set of impressions.\r\n\r\nWe will be using an unbiased estimate of the aggregate
    reward using inverse propensity scoring (see [ the companion paper ](http://www.cs.cornell.edu/~adith/Criteo/NIPS16_Benchmark.pdf)
    for details).\r\n\r\n For further details on evaluation please refer to the Getting
    Started guide in the [challenge starter kit](https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit)."
  evaluation: |
    <p>In the dataset, each impression is represented by <code>M</code> lines where <code>M</code> is the number of candidate ads. Each line has feature information for every other candidate ad.
    In addition, the first line corresponds to the candidate that was displayed by the logging policy, an indicator whether the displayed product was clicked by the user (“click” encoded as <code>0.001</code>, “no-click” encoded as <code>0.999</code>), and the <strong>inverse propensity</strong> of the stochastic logging policy to pick that specific candidate (see the  <a href="http://www.cs.cornell.edu/~adith/Criteo/"> companion paper </a>. for details).
    Each <code>&lt;user context-candidate product&gt;</code> pair is described using <strong>33 categorical (multi-set) features and 2 numeric features</strong>. Of these, 10 features are only-context-dependent while the remaining 25 features depend on both the context and the candidate product. These categorical feature representations have been post-processed to a 74000-dimensional vector with sparse one-hot encoding for each categorical feature. <strong>The semantics behind the features will not be disclosed</strong>.</p>

    <p>These post-processed dataset files are available <a href="https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files"> here </a>.</p>

    <p>Your task is to build a function <code>_policy</code> which takes <code>M</code> candidates, each represented by a <strong>74000-dimensional sparse vector</strong>, and outputs scores for each of the candidates between <code>1</code> and <code>M</code>.</p>

    <p>The reward for an individual impression is, did the selected candidate get clicked <em>(reward = 1)</em> or not <em>(reward = 0)</em>? The reward for function <code>f</code> is the aggregate reward over all impressions on a held out test set of impressions.</p>

    <p>We will be using an unbiased estimate of the aggregate reward using inverse propensity scoring (see <a href="http://www.cs.cornell.edu/~adith/Criteo/NIPS16_Benchmark.pdf"> the companion paper </a> for details).</p>

    <p>For further details on evaluation please refer to the Getting Started guide in the <a href="https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit">challenge starter kit</a>.</p>
  rules_markdown: "* You may use only the training dataset as outlined in the Dataset
    guide to develop your submission. In particular, please do not use external data
    sources or attempt to reverse-engineer the held-out testing set using external
    public sources.\r\n* The top 3 teams according to the leaderboard on December
    1, 2017 12pm EST must perform the following additional steps to be eligible for
    prizes:\r\n\r\n     - They must submit their model and training scripts to the
    organizers under an open source license. Their submissions will be checked and
    run offline to match their online submission to the leaderboard.\r\n\r\n    -
    They **must** describe their algorithms and their development process during the
    NIPS'17 Causal Inference and Machine Learning workshop; either in person, or as
    a remote presentation. Part of the prize money is meant to fund travel and registration
    at NIPS'17 for the winning teams.\r\n\r\n    - Only leaderboard scores above a
    minimum threshold will be eligible for prizes. The minimum threshold to beat is
    54.0 \r\n"
  rules: |
    <ul>
      <li>You may use only the training dataset as outlined in the Dataset guide to develop your submission. In particular, please do not use external data sources or attempt to reverse-engineer the held-out testing set using external public sources.</li>
      <li>
        <p>The top 3 teams according to the leaderboard on December 1, 2017 12pm EST must perform the following additional steps to be eligible for prizes:</p>

        <ul>
          <li>
            <p>They must submit their model and training scripts to the organizers under an open source license. Their submissions will be checked and run offline to match their online submission to the leaderboard.</p>
          </li>
          <li>
            <p>They <strong>must</strong> describe their algorithms and their development process during the NIPS’17 Causal Inference and Machine Learning workshop; either in person, or as a remote presentation. Part of the prize money is meant to fund travel and registration at NIPS’17 for the winning teams.</p>
          </li>
          <li>
            <p>Only leaderboard scores above a minimum threshold will be eligible for prizes. The minimum threshold to beat is 54.0</p>
          </li>
        </ul>
      </li>
    </ul>
  prizes_markdown: "\r\n* 1st  - **$2000**\r\n* 2nd - **$1500**\r\n* 3rd  - **$1000**\r\n\r\nAdditionally:\r\n\r\n*
    Invitation to [ NIPS'17 Causal Inference and Machine Learning Workshop ](https://sites.google.com/view/causalnips2017){:target='_blank'}\r\n\r\n*
    Invitation to the [2nd Applied Machine Learning Days](https://www.appliedmldays.org/)
    at EPFL in Switzerland on January 29 & 30, 2018.\r\n"
  prizes: |2

    <ul>
      <li>1st  - <strong>$2000</strong></li>
      <li>2nd - <strong>$1500</strong></li>
      <li>3rd  - <strong>$1000</strong></li>
    </ul>

    <p>Additionally:</p>

    <ul>
      <li>
        <p>Invitation to <a href="https://sites.google.com/view/causalnips2017" target="_blank"> NIPS’17 Causal Inference and Machine Learning Workshop </a></p>
      </li>
      <li>
        <p>Invitation to the <a href="https://www.appliedmldays.org/">2nd Applied Machine Learning Days</a> at EPFL in Switzerland on January 29 &amp; 30, 2018.</p>
      </li>
    </ul>
  resources_markdown: "\r\n### Contact:\r\n\r\n* Technical issues : [https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge](https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge)\r\n*
    Discussion Forum : [https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics](https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics)\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n*  Adith Swaminathan [ adswamin@microsoft.com
    ](mailto:adswamin@microsoft.com){:target='_blank'}\r\n*  Sharada Prasanna Mohanty
    [ sharada.mohanty@epfl.ch ](mailto:sharada.mohanty@epfl.ch){:target='_blank'}
    \r\n"
  resources: |2

    <h3 id="contact">Contact:</h3>

    <ul>
      <li>Technical issues : <a href="https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge">https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge</a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics">https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics</a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Adith Swaminathan <a href="mailto:adswamin@microsoft.com" target="_blank"> adswamin@microsoft.com </a></li>
      <li>Sharada Prasanna Mohanty <a href="mailto:sharada.mohanty@epfl.ch" target="_blank"> sharada.mohanty@epfl.ch </a></li>
    </ul>
  submission_instructions_markdown: ''
  submission_instructions: "\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: multiarmedbandit.jpg
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### NIPS '17 Workshop: Criteo Ad Placement Challenge
    Results\r\n<br><br>\r\n\r\n| Rank | Participant | Prize |\r\n| **1.** | [ololo](https://www.crowdai.org/participants/ololo)
    | $2,000 |\r\n| **2.** | [geffy](https://www.crowdai.org/participants/geffy) |
    $1,500 |\r\n| **3.** | [Group](https://www.crowdai.org/participants/group) | $1,000
    |"
  winner_description: |
    <h3 id="nips-17-workshop-criteo-ad-placement-challenge-results">NIPS ‘17 Workshop: Criteo Ad Placement Challenge Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/ololo">ololo</a></td>
          <td>$2,000</td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/geffy">geffy</a></td>
          <td>$1,500</td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><a href="https://www.crowdai.org/participants/group">Group</a></td>
          <td>$1,000</td>
        </tr>
      </tbody>
    </table>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: 
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_9:
  id: 9
  organizer_id: 6
  challenge: OpenSNP Height Prediction
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &32 2017-06-26 12:09:18.030078000 Z
    zone: *2
    time: *32
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &33 2018-10-04 09:40:11.611680000 Z
    zone: *2
    time: *33
  tagline: OpenSNP
  primary_sort_order_cd: descending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 12184
  participant_count: 123
  submission_count: 1268
  score_title: Coefficient of Determination (R^2)
  score_secondary_title: Mean Squared Error
  slug: opensnp-height-prediction
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: OpenSNPChallenge2017
  online_grading: true
  vote_count: 48
  description_markdown: "This challenge aims at predicting height based on genetics
    (DNA variation).\r\n\r\n### Background\r\n\r\nGenetics is the study of genes,
    genetic variation, and heredity in living organisms. DNA is the support that allows
    most living organism to pass information from generation to generation. It consists
    in long strands of nucleotides that build a higher order structures, the chromosomes.
    There are four different nucleotides represented by the letters A, T, C and G
    that together make up the genetic code. \r\n\r\nThe human genome is made of >
    3 billion nucleotides, and each individual harbors about 4 million genetic variants
    (mostly single nucleotide polymorphisms, or **SNPs**). A specific position on
    a chromosome is called a genetic locus, and different versions of the same **genetic
    locus** are called **alleles**. Humans being **diploid** organisms, they have
    two genome copies - one inherited from each parent - and thus two alleles at each
    genetic locus. For one particular genetic locus, an individual is **homozygous**
    if the two alleles are identical, and **heterozygous** if the two alleles are
    different.\r\n\r\nWe call **genotype** the DNA sequence of an individual that
    determines a specific observable characteristic. That characteristic is called
    **phenotype**.\r\n\r\n**Monogenic** phenotypes are under the control of a single
    gene. For example, if hair color was a monogenic phenotype, inheriting two brown
    alleles of a hypothetical hair color gene would result in the brown hair phenotype.
    Conversely, inheriting two ginger hair alleles would result in the ginger hair
    phenotype.\r\n\r\n![image1](https://s3.amazonaws.com/salathegroup-static/opensnp/images/image1.png)\r\n\r\n**Polygenic**
    phenotypes, on the contrary, are under the control of multiple genetic variants
    across the genome. If hair color was polygenic, it might for example work like
    the RGB (Red, Green, Blue) color model. In this case, three different genes would
    add their effects and interact to control hair color.\r\n\r\n![image2](https://s3.amazonaws.com/salathegroup-static/opensnp/images/image2.png)\r\n\r\n**Heritability**
    of a phenotype measures how much of the observed variance of the phenotype in
    the population is due to genetic factors. **Missing heritability** represents
    the difference between the estimated heritability of a given phenotype, and the
    heritability that is explained by known genetic factors. Heritability of human
    height is estimated to be as high as 80%, but large genomic studies have so far
    only been able to explain about 25% of the observed variance. Height is a model
    phenotype to study complex traits, and here we want to test whether part of the
    missing heritability can be explained using innovative approaches to genetic datasets,
    including deep learning. \r\n\r\n\r\n## Data \r\n\r\nThe data comes from [OpenSNP](https://opensnp.org/),
    which allows customers of direct-to-customer genetic tests to publicly share their
    genome-wide genotyping data.\r\n\r\n![openSNPLogo ](https://s3.amazonaws.com/salathegroup-static/opensnp/images/opensnp-logo-small.png){:class='img-logo'}\r\n\r\nWe
    provide two datasets for a total of 921 samples divided into a training set of
    784 sample `subset_cm_train.npy` and a test set of 137 samples in `subset_cm_test.npy`.
    \ \r\n\r\nIt contains a set of **9,894** genetic variants known to be associated
    with height[1] (9207 variants) and the one on Y chromosome (687 variants). This
    numpy file has shape `(784, 9894)` for the training set and `(137, 9894)` for
    the test set. \r\nEach genetic variant is represented by 0 (homozygous for reference)
    , 1 (heterozygous), 2 (homozygous for the genetic variant) or NA (missing information
    or absence of the position in the case of Y chromosome in women). The first 9207
    rows are the genetic variants known to be associated with height, the last 687
    correspond to the Y chromosome.  \r\n\r\nFinally, height is provided in a separate
    numpy file of shape `(784, 1)`  named `openSNP_heights.npy` for the training set
    only.\r\n\r\nWhile we recommend to start with this simplified dataset, more advanced
    user might try to analyze an extended version of OpenSNP data which description
    is available [here](https://github.com/crowdAI/opensnp-challenge-starter-kit/blob/master/overview_full_dataset.md).\r\n\r\n##
    Submission \r\n~~~~~~~~\r\nimport crowdai\r\nchallenge = crowdai.Challenge(\"OpenSNPChallenge2017\",
    \"YOUR_CROWDAI_API_KEY_HERE\")\r\n\r\ndata = ... #a list of 137 predicted heights
    for all the 137 corresponding data points in the test set\r\nchallenge.submit(data)\r\nchallenge.disconnect()\r\n~~~~~~~~\r\n{:
    .language-python}\r\n\r\nMore instructions to make submissions, and starter code
    is available at : \r\n\r\n**STARTER KIT**  [ https://github.com/crowdAI/opensnp-challenge-starter-kit
    ](https://github.com/crowdAI/opensnp-challenge-starter-kit)\r\n\r\n\r\n## Contact
    \r\n* Sharada Mohanty <[ sharada.mohanty@epfl.ch ](mailto:sharada.mohanty@epfl.ch)>\r\n*
    Olivier Naret <[ olivier.naret@epfl.ch ](olivier.naret@epfl.ch)>\r\n\r\n### Advisors\r\n*
    Jacques Fellay <[ jacques.fellay@epfl.ch ](jacques.fellay@epfl.ch)>\r\n* Marcel
    Salathe <[ marcel.salathe@epfl.ch ](marcel.salathe@epfl.ch)>\r\n\r\n**Challenge
    Image source**: [https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png](https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png)"
  description: "<p>This challenge aims at predicting height based on genetics (DNA
    variation).</p>\n\n<h3 id=\"background\">Background</h3>\n\n<p>Genetics is the
    study of genes, genetic variation, and heredity in living organisms. DNA is the
    support that allows most living organism to pass information from generation to
    generation. It consists in long strands of nucleotides that build a higher order
    structures, the chromosomes. There are four different nucleotides represented
    by the letters A, T, C and G that together make up the genetic code.</p>\n\n<p>The
    human genome is made of &gt; 3 billion nucleotides, and each individual harbors
    about 4 million genetic variants (mostly single nucleotide polymorphisms, or <strong>SNPs</strong>).
    A specific position on a chromosome is called a genetic locus, and different versions
    of the same <strong>genetic locus</strong> are called <strong>alleles</strong>.
    Humans being <strong>diploid</strong> organisms, they have two genome copies -
    one inherited from each parent - and thus two alleles at each genetic locus. For
    one particular genetic locus, an individual is <strong>homozygous</strong> if
    the two alleles are identical, and <strong>heterozygous</strong> if the two alleles
    are different.</p>\n\n<p>We call <strong>genotype</strong> the DNA sequence of
    an individual that determines a specific observable characteristic. That characteristic
    is called <strong>phenotype</strong>.</p>\n\n<p><strong>Monogenic</strong> phenotypes
    are under the control of a single gene. For example, if hair color was a monogenic
    phenotype, inheriting two brown alleles of a hypothetical hair color gene would
    result in the brown hair phenotype. Conversely, inheriting two ginger hair alleles
    would result in the ginger hair phenotype.</p>\n\n<p><img src=\"https://s3.amazonaws.com/salathegroup-static/opensnp/images/image1.png\"
    alt=\"image1\" /></p>\n\n<p><strong>Polygenic</strong> phenotypes, on the contrary,
    are under the control of multiple genetic variants across the genome. If hair
    color was polygenic, it might for example work like the RGB (Red, Green, Blue)
    color model. In this case, three different genes would add their effects and interact
    to control hair color.</p>\n\n<p><img src=\"https://s3.amazonaws.com/salathegroup-static/opensnp/images/image2.png\"
    alt=\"image2\" /></p>\n\n<p><strong>Heritability</strong> of a phenotype measures
    how much of the observed variance of the phenotype in the population is due to
    genetic factors. <strong>Missing heritability</strong> represents the difference
    between the estimated heritability of a given phenotype, and the heritability
    that is explained by known genetic factors. Heritability of human height is estimated
    to be as high as 80%, but large genomic studies have so far only been able to
    explain about 25% of the observed variance. Height is a model phenotype to study
    complex traits, and here we want to test whether part of the missing heritability
    can be explained using innovative approaches to genetic datasets, including deep
    learning.</p>\n\n<h2 id=\"data\">Data</h2>\n\n<p>The data comes from <a href=\"https://opensnp.org/\">OpenSNP</a>,
    which allows customers of direct-to-customer genetic tests to publicly share their
    genome-wide genotyping data.</p>\n\n<p><img src=\"https://s3.amazonaws.com/salathegroup-static/opensnp/images/opensnp-logo-small.png\"
    alt=\"openSNPLogo \" class=\"img-logo\" /></p>\n\n<p>We provide two datasets for
    a total of 921 samples divided into a training set of 784 sample <code>subset_cm_train.npy</code>
    and a test set of 137 samples in <code>subset_cm_test.npy</code>.</p>\n\n<p>It
    contains a set of <strong>9,894</strong> genetic variants known to be associated
    with height[1] (9207 variants) and the one on Y chromosome (687 variants). This
    numpy file has shape <code>(784, 9894)</code> for the training set and <code>(137,
    9894)</code> for the test set. \nEach genetic variant is represented by 0 (homozygous
    for reference) , 1 (heterozygous), 2 (homozygous for the genetic variant) or NA
    (missing information or absence of the position in the case of Y chromosome in
    women). The first 9207 rows are the genetic variants known to be associated with
    height, the last 687 correspond to the Y chromosome.</p>\n\n<p>Finally, height
    is provided in a separate numpy file of shape <code>(784, 1)</code>  named <code>openSNP_heights.npy</code>
    for the training set only.</p>\n\n<p>While we recommend to start with this simplified
    dataset, more advanced user might try to analyze an extended version of OpenSNP
    data which description is available <a href=\"https://github.com/crowdAI/opensnp-challenge-starter-kit/blob/master/overview_full_dataset.md\">here</a>.</p>\n\n<h2
    id=\"submission\">Submission</h2>\n<pre><code class=\"language-python\">import
    crowdai\nchallenge = crowdai.Challenge(\"OpenSNPChallenge2017\", \"YOUR_CROWDAI_API_KEY_HERE\")\n\ndata
    = ... #a list of 137 predicted heights for all the 137 corresponding data points
    in the test set\nchallenge.submit(data)\nchallenge.disconnect()\n</code></pre>\n\n<p>More
    instructions to make submissions, and starter code is available at :</p>\n\n<p><strong>STARTER
    KIT</strong>  <a href=\"https://github.com/crowdAI/opensnp-challenge-starter-kit\">
    https://github.com/crowdAI/opensnp-challenge-starter-kit </a></p>\n\n<h2 id=\"contact\">Contact</h2>\n<ul>\n
    \ <li>Sharada Mohanty &lt;<a href=\"mailto:sharada.mohanty@epfl.ch\"> sharada.mohanty@epfl.ch
    </a>&gt;</li>\n  <li>Olivier Naret &lt;<a href=\"olivier.naret@epfl.ch\"> olivier.naret@epfl.ch
    </a>&gt;</li>\n</ul>\n\n<h3 id=\"advisors\">Advisors</h3>\n<ul>\n  <li>Jacques
    Fellay &lt;<a href=\"jacques.fellay@epfl.ch\"> jacques.fellay@epfl.ch </a>&gt;</li>\n
    \ <li>Marcel Salathe &lt;<a href=\"marcel.salathe@epfl.ch\"> marcel.salathe@epfl.ch
    </a>&gt;</li>\n</ul>\n\n<p><strong>Challenge Image source</strong>: <a href=\"https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png\">https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png</a></p>\n"
  evaluation_markdown: "The evaluation will be done based on two scores :\r\n\r\n*
    [ Co-efficient of Determination ($$R^2$$) ](https://en.wikipedia.org/wiki/Coefficient_of_determination)
    (Primary Score)\r\n* [ Mean Squared Error ](https://en.wikipedia.org/wiki/Mean_squared_error)
    (Secondary Score)\r\n\r\nbetween the actual heights of the individuals in the
    test set and the submitted predictions.\r\n\r\n**NOTE** : During the challenge,
    the scores will be computed only on 20% of the test dataset. The final standings
    on the leaderboard will be decided computing the same scores on the 100% of the
    dataset after the challenge. \r\n"
  evaluation: |
    <p>The evaluation will be done based on two scores :</p>

    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"> Co-efficient of Determination (<script type="math/tex">R^2</script>) </a> (Primary Score)</li>
      <li><a href="https://en.wikipedia.org/wiki/Mean_squared_error"> Mean Squared Error </a> (Secondary Score)</li>
    </ul>

    <p>between the actual heights of the individuals in the test set and the submitted predictions.</p>

    <p><strong>NOTE</strong> : During the challenge, the scores will be computed only on 20% of the test dataset. The final standings on the leaderboard will be decided computing the same scores on the 100% of the dataset after the challenge.</p>
  rules_markdown: 
  rules: 
  prizes_markdown: The winner will be invited to the 2nd Applied Machine Learning
    Days at EPFL in Switzerland on January  29 & 30, 2018, with travel and accommodation
    covered.
  prizes: "<p>The winner will be invited to the 2nd Applied Machine Learning Days
    at EPFL in Switzerland on January  29 &amp; 30, 2018, with travel and accommodation
    covered.</p>\n"
  resources_markdown: "MIT Open Course Ware can help you go further in understanding
    biological concepts related to this challenge. \r\n\r\n* Lesson 19 on Discovering
    Quantitative Trait Loci (QTLs)\r\n* Lesson 20 on Human Genetics, SNPs, and Genome
    Wide Associate Studies\r\n\r\nThe most important publications describing associations
    between genetic factors and human height :\r\n\r\n* based on [ common SNPs ](http://www.nature.com/ng/journal/v46/n11/full/ng.3097.html)[1]\r\n*
    on [ rare SNPs ](http://www.nature.com/nature/journal/v542/n7640/full/nature21039.html)[2].\r\n\r\n\r\nTo
    transform the VCF files it can be convenient to use [ plink ](https://www.cog-genomics.org/plink2).\r\n\r\n\r\n##
    References\r\n\r\n1 [Wood, Andrew R, Tonu Esko, Jian Yang, Sailaja Vedantam, Tune
    H Pers, Stefan Gustafsson, Audrey Y Chu, et al. “Defining the Role of Common Variation
    in the Genomic and Biological Architecture of Adult Human Height.” Nature Genetics
    2014. doi:10.1038/ng.3097](https://www.nature.com/ng/journal/v46/n11/abs/ng.3097.html).\r\n\r\n2
    [Marouli, Eirini, Mariaelisa Graff, Carolina Medina-Gomez, Ken Sin Lo, Andrew
    R. Wood, Troels R. Kjaer, Rebecca S. Fine, et al. “Rare and Low-Frequency Coding
    Variants Alter Human Adult Height.” Nature 2017. doi:10.1038/nature21039](https://www.nature.com/nature/journal/v542/n7640/full/nature21039.html).\r\n\r\n\r\n"
  resources: |+
    <p>MIT Open Course Ware can help you go further in understanding biological concepts related to this challenge.</p>

    <ul>
      <li>Lesson 19 on Discovering Quantitative Trait Loci (QTLs)</li>
      <li>Lesson 20 on Human Genetics, SNPs, and Genome Wide Associate Studies</li>
    </ul>

    <p>The most important publications describing associations between genetic factors and human height :</p>

    <ul>
      <li>based on <a href="http://www.nature.com/ng/journal/v46/n11/full/ng.3097.html"> common SNPs </a>[1]</li>
      <li>on <a href="http://www.nature.com/nature/journal/v542/n7640/full/nature21039.html"> rare SNPs </a>[2].</li>
    </ul>

    <p>To transform the VCF files it can be convenient to use <a href="https://www.cog-genomics.org/plink2"> plink </a>.</p>

    <h2 id="references">References</h2>

    <p>1 <a href="https://www.nature.com/ng/journal/v46/n11/abs/ng.3097.html">Wood, Andrew R, Tonu Esko, Jian Yang, Sailaja Vedantam, Tune H Pers, Stefan Gustafsson, Audrey Y Chu, et al. “Defining the Role of Common Variation in the Genomic and Biological Architecture of Adult Human Height.” Nature Genetics 2014. doi:10.1038/ng.3097</a>.</p>

    <p>2 <a href="https://www.nature.com/nature/journal/v542/n7640/full/nature21039.html">Marouli, Eirini, Mariaelisa Graff, Carolina Medina-Gomez, Ken Sin Lo, Andrew R. Wood, Troels R. Kjaer, Rebecca S. Fine, et al. “Rare and Low-Frequency Coding Variants Alter Human Adult Height.” Nature 2017. doi:10.1038/nature21039</a>.</p>

  submission_instructions_markdown: 'Instructions to make submissions, and basic starter
    code is available at : [ https://github.com/spMohanty/opensnp-challenge-starter-kit
    ](https://github.com/spMohanty/opensnp-challenge-starter-kit)'
  submission_instructions: '<p>Instructions to make submissions, and basic starter
    code is available at : <a href="https://github.com/spMohanty/opensnp-challenge-starter-kit">
    https://github.com/spMohanty/opensnp-challenge-starter-kit </a></p>

'
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: 
  dataset_description: 
  image_file: Benzopyrene_DNA_adduct_1JDG.png
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: "### OpenSNP Height Prediction Results\r\n<br><br>\r\n\r\n|
    Rank | Participant | Prize |\r\n| **1.** | [David_Baranger](https://www.crowdai.org/participants/david_baranger)
    | Invitation to [AMLD2018](https://www.appliedmldays.org/) |\r\n| **2.** | [SergeKrier](https://www.crowdai.org/participants/sergekrier)
    |  |\r\n| **3.** | *Baseline Submission* |  |"
  winner_description: |
    <h3 id="opensnp-height-prediction-results">OpenSNP Height Prediction Results</h3>
    <p><br /><br /></p>

    <table>
      <tbody>
        <tr>
          <td>Rank</td>
          <td>Participant</td>
          <td>Prize</td>
        </tr>
        <tr>
          <td><strong>1.</strong></td>
          <td><a href="https://www.crowdai.org/participants/david_baranger">David_Baranger</a></td>
          <td>Invitation to <a href="https://www.appliedmldays.org/">AMLD2018</a></td>
        </tr>
        <tr>
          <td><strong>2.</strong></td>
          <td><a href="https://www.crowdai.org/participants/sergekrier">SergeKrier</a></td>
          <td> </td>
        </tr>
        <tr>
          <td><strong>3.</strong></td>
          <td><em>Baseline Submission</em></td>
          <td> </td>
        </tr>
      </tbody>
    </table>
  winners_tab_active: true
  clef_task_id: 
  clef_challenge: false
  submissions_page: false
  private_challenge: false
  show_leaderboard: true
  grader_identifier: 
  online_submissions: false
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: false
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_20:
  id: 20
  organizer_id: 7
  challenge: ImageCLEF 2018 Tuberculosis - Severity scoring
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &34 2017-11-07 09:54:11.420070000 Z
    zone: *2
    time: *34
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &35 2018-10-04 08:56:38.289140000 Z
    zone: *2
    time: *35
  tagline: Scoring the severity of TB cases based on chest CT images
  primary_sort_order_cd: ascending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 2700
  participant_count: 105
  submission_count: 59
  score_title: RMSE
  score_secondary_title: AUC
  slug: imageclef-2018-tuberculosis-severity-scoring
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: tuberculosis_severity_scoring_clef2018
  online_grading: true
  vote_count: 15
  description_markdown: "*****\r\n**Important note:**\r\n\r\n_The **ImageCLEF Tuberculosis
    - Severity Scoring challenge has officially ended** and we would like to thank
    everybody for their participation. You can find the official results at [http://imageclef.org/2018/tuberculosis
    ](http://imageclef.org/2018/tuberculosis)._\r\n\r\n_Post-challenge submissions
    and the leaderboard will remain enabled  for a few weeks so you will still be
    able to submit result files and have them continuously evaluated during a limited
    period. \r\nPlease consider that in order to see the version of the leaderboard
    with the post-challenge submissions integrated, you have to turn on the switch
    **Show post-challenge submission** right below the leaderboard._\r\n\r\n_At the
    same time we'd like to encourage you to submit a [CLEF Working notes paper](http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html)
    until the end of May._\r\n\r\n_Please also note that participants registering
    from now on will not be\r\nautomatically registered with CLEF anymore._\r\n\r\n*****\r\n\r\n_Note:
    ImageCLEF Tuberculosis 2018 is divided into 3 subtasks (challenges). This challenge
    is about **Severity Scoring**. For information on the **MDR (multi-drug-resistance)
    Detection** challenge click [ here ](/challenges/imageclef-2018-tuberculosis-mdr-detection){:target='_blank'}.
    For information on the **TBT (tuberculosis type) Classfication** challenge click
    [ here ](/challenges/imageclef-2018-tuberculosis-tbt-classification){:target='_blank'}.
    All of these challenges share the same dataset, so registering for one of these
    challenges will automatically give you access to the other ones._\r\n\r\n_Note:
    Do not forget to read the **Rules** section on this page_\r\n\r\n### Motivation\r\n\r\nAbout
    130 years after the discovery of Mycobacterium tuberculosis, the disease remains
    a persistent threat and a leading cause of death worldwide.\r\n\r\nThe greatest
    disaster that can happen to a patient with tuberculosis (TB) is that the organisms
    become resistant to two or more of the standard drugs. In contrast to drug sensitive
    (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult
    and expensive to recover from. Thus, early detection of the drug resistance (DR)
    status is of great importance for effective treatment. The most commonly used
    methods of DR detection are either expensive or take too much time (up to several
    month). Therefore there is a need for quick and at the same time cheap methods
    of DR detection. One of the possible approaches for this task is based on Computed
    Tomography (CT) image analysis. Another challenging task is automatic detection
    of TB types (TBT) using CT volumes.\r\n\r\n_Differences compared to 2017_: Scoring
    the severity of TB cases based on chest CT images is another task compared to
    both tuberculosis-related subtasks considered in 2017. There are no direct links
    between them. Note only that original CT image datasets used in 2017 and in 2018
    may slightly overlap.\r\n\r\n### Challenge description\r\n\r\nThe goal on this
    task is to score the TB severity.\r\n\r\n### Data\r\n\r\nThe dataset for this
    subtask includes chest CT scans of TB patients along with the corresponding severity
    score (1 to 5) and the severity level designated as \"low\" and \"high\".\r\n\r\n\r\n|
    Num. Patients       | Train | Test |\r\n| --- |:---:| :---:|\r\n| Low severity
    \         | 90     | 62    |\r\n| High severity         | 80     | 47    |\r\n|
    **Total patients** | **170**  | **109**  |\r\n\r\nWe provide 3D CT images with
    slice size of 512*512 pixels and number of slices varying from about 50 to 400.
    All the CT images are stored in NIFTI file format with .nii.gz file extension
    (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield
    units (HU) as well the corresponding image metadata such as image dimensions,
    voxel size in physical units, slice thickness, etc. A freely-available tool called
    [ \"VV\" ](https://www.creatis.insa-lyon.fr/rio/vv){:target='_blank'} can be used
    for viewing image files. Currently, there are various tools available for reading
    and writing NIFTI files. Among them there are [ load_nii ](https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m){:target='_blank'}
    and [ save_nii ](https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m){:target='_blank'}
    functions for Matlab and [ Niftilib ](http://niftilib.sourceforge.net/){:target='_blank'}
    library for C, Java, Matlab and Python.\r\n\r\nWe also provide automatic extracted
    masks of the lungs. This material can be downloaded together with the patients
    CT images. The details of this segmentation can be found [ here ](http://publications.hevs.ch/index.php/publications/show/1871){:target='_blank'}.\r\nIn
    case the participants use these masks in their experiments, please refer to the
    section \"Citations\" to find the appropriate citation for this lung segmentation
    technique.\r\n\r\n### Submission instructions\r\n\r\n*****\r\n_As soon as the
    submission is open, you will find a \"Create Submission\" button on this page
    (just next to the tabs)_\r\n\r\n*****\r\n\r\nSubmit a plain text file named with
    the prefix **SVR** (e.g. SVRfree-text.txt) with the following format:\r\n\r\n\\<Patient-ID\\>,\\<Severity
    score\\>,\\<Probability of \"HIGH\" severity\\>\r\n\r\ne.g.:\r\n\r\n~~~\r\nSVR_TST_001,1,0.93\r\nSVR_TST_002,3,0.54\r\nSVR_TST_003,5,0.1\r\nSVR_TST_004,4,0.245\r\nSVR_TST_005,2,0.7\r\n~~~\r\n\r\n**Please
    use an integer value between 1 and 5 to indicate the severity score.**\r\n\r\n**Please
    use a score between 0 and 1 to indicate the probability of the patient having
    \"HIGH\" severity (it corresponds to severity scores 1 to 3).**\r\n\r\nYou need
    to respect the following constraints:\r\n\r\n* Patient-IDs must be part of the
    predefined Patient-IDs\r\n* All patient-IDs must be present in the runfiles\r\n*
    Only use one integer value from 0 to 5 for the severity score\r\n* Only use numbers
    between 0 and 1 for the probability. Use the dot (.) as a decimal point (no commas
    accepted)\r\n\r\n### Citations\r\n\r\nInformation will be posted after the challenge
    ends.\r\n"
  description: "<hr />\n<p><strong>Important note:</strong></p>\n\n<p><em>The <strong>ImageCLEF
    Tuberculosis - Severity Scoring challenge has officially ended</strong> and we
    would like to thank everybody for their participation. You can find the official
    results at <a href=\"http://imageclef.org/2018/tuberculosis\">http://imageclef.org/2018/tuberculosis
    </a>.</em></p>\n\n<p><em>Post-challenge submissions and the leaderboard will remain
    enabled  for a few weeks so you will still be able to submit result files and
    have them continuously evaluated during a limited period. \nPlease consider that
    in order to see the version of the leaderboard with the post-challenge submissions
    integrated, you have to turn on the switch <strong>Show post-challenge submission</strong>
    right below the leaderboard.</em></p>\n\n<p><em>At the same time we’d like to
    encourage you to submit a <a href=\"http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html\">CLEF
    Working notes paper</a> until the end of May.</em></p>\n\n<p><em>Please also note
    that participants registering from now on will not be\nautomatically registered
    with CLEF anymore.</em></p>\n\n<hr />\n\n<p><em>Note: ImageCLEF Tuberculosis 2018
    is divided into 3 subtasks (challenges). This challenge is about <strong>Severity
    Scoring</strong>. For information on the <strong>MDR (multi-drug-resistance) Detection</strong>
    challenge click <a href=\"/challenges/imageclef-2018-tuberculosis-mdr-detection\"
    target=\"_blank\"> here </a>. For information on the <strong>TBT (tuberculosis
    type) Classfication</strong> challenge click <a href=\"/challenges/imageclef-2018-tuberculosis-tbt-classification\"
    target=\"_blank\"> here </a>. All of these challenges share the same dataset,
    so registering for one of these challenges will automatically give you access
    to the other ones.</em></p>\n\n<p><em>Note: Do not forget to read the <strong>Rules</strong>
    section on this page</em></p>\n\n<h3 id=\"motivation\">Motivation</h3>\n\n<p>About
    130 years after the discovery of Mycobacterium tuberculosis, the disease remains
    a persistent threat and a leading cause of death worldwide.</p>\n\n<p>The greatest
    disaster that can happen to a patient with tuberculosis (TB) is that the organisms
    become resistant to two or more of the standard drugs. In contrast to drug sensitive
    (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult
    and expensive to recover from. Thus, early detection of the drug resistance (DR)
    status is of great importance for effective treatment. The most commonly used
    methods of DR detection are either expensive or take too much time (up to several
    month). Therefore there is a need for quick and at the same time cheap methods
    of DR detection. One of the possible approaches for this task is based on Computed
    Tomography (CT) image analysis. Another challenging task is automatic detection
    of TB types (TBT) using CT volumes.</p>\n\n<p><em>Differences compared to 2017</em>:
    Scoring the severity of TB cases based on chest CT images is another task compared
    to both tuberculosis-related subtasks considered in 2017. There are no direct
    links between them. Note only that original CT image datasets used in 2017 and
    in 2018 may slightly overlap.</p>\n\n<h3 id=\"challenge-description\">Challenge
    description</h3>\n\n<p>The goal on this task is to score the TB severity.</p>\n\n<h3
    id=\"data\">Data</h3>\n\n<p>The dataset for this subtask includes chest CT scans
    of TB patients along with the corresponding severity score (1 to 5) and the severity
    level designated as “low” and “high”.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Num.
    Patients</th>\n      <th style=\"text-align: center\">Train</th>\n      <th style=\"text-align:
    center\">Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Low
    severity</td>\n      <td style=\"text-align: center\">90</td>\n      <td style=\"text-align:
    center\">62</td>\n    </tr>\n    <tr>\n      <td>High severity</td>\n      <td
    style=\"text-align: center\">80</td>\n      <td style=\"text-align: center\">47</td>\n
    \   </tr>\n    <tr>\n      <td><strong>Total patients</strong></td>\n      <td
    style=\"text-align: center\"><strong>170</strong></td>\n      <td style=\"text-align:
    center\"><strong>109</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<p>We provide
    3D CT images with slice size of 512*512 pixels and number of slices varying from
    about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz
    file extension (g-zipped .nii files). This file format stores raw voxel intensities
    in Hounsfield units (HU) as well the corresponding image metadata such as image
    dimensions, voxel size in physical units, slice thickness, etc. A freely-available
    tool called <a href=\"https://www.creatis.insa-lyon.fr/rio/vv\" target=\"_blank\">
    “VV” </a> can be used for viewing image files. Currently, there are various tools
    available for reading and writing NIFTI files. Among them there are <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m\"
    target=\"_blank\"> load_nii </a> and <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m\"
    target=\"_blank\"> save_nii </a> functions for Matlab and <a href=\"http://niftilib.sourceforge.net/\"
    target=\"_blank\"> Niftilib </a> library for C, Java, Matlab and Python.</p>\n\n<p>We
    also provide automatic extracted masks of the lungs. This material can be downloaded
    together with the patients CT images. The details of this segmentation can be
    found <a href=\"http://publications.hevs.ch/index.php/publications/show/1871\"
    target=\"_blank\"> here </a>.\nIn case the participants use these masks in their
    experiments, please refer to the section “Citations” to find the appropriate citation
    for this lung segmentation technique.</p>\n\n<h3 id=\"submission-instructions\">Submission
    instructions</h3>\n\n<hr />\n<p><em>As soon as the submission is open, you will
    find a “Create Submission” button on this page (just next to the tabs)</em></p>\n\n<hr
    />\n\n<p>Submit a plain text file named with the prefix <strong>SVR</strong> (e.g.
    SVRfree-text.txt) with the following format:</p>\n\n<p>&lt;Patient-ID&gt;,&lt;Severity
    score&gt;,&lt;Probability of “HIGH” severity&gt;</p>\n\n<p>e.g.:</p>\n\n<pre><code>SVR_TST_001,1,0.93\nSVR_TST_002,3,0.54\nSVR_TST_003,5,0.1\nSVR_TST_004,4,0.245\nSVR_TST_005,2,0.7\n</code></pre>\n\n<p><strong>Please
    use an integer value between 1 and 5 to indicate the severity score.</strong></p>\n\n<p><strong>Please
    use a score between 0 and 1 to indicate the probability of the patient having
    “HIGH” severity (it corresponds to severity scores 1 to 3).</strong></p>\n\n<p>You
    need to respect the following constraints:</p>\n\n<ul>\n  <li>Patient-IDs must
    be part of the predefined Patient-IDs</li>\n  <li>All patient-IDs must be present
    in the runfiles</li>\n  <li>Only use one integer value from 0 to 5 for the severity
    score</li>\n  <li>Only use numbers between 0 and 1 for the probability. Use the
    dot (.) as a decimal point (no commas accepted)</li>\n</ul>\n\n<h3 id=\"citations\">Citations</h3>\n\n<p>Information
    will be posted after the challenge ends.</p>\n"
  evaluation_markdown: "The results will be evaluated considering this task as a binary
    classification problem and as a regression problem. The classification problem
    will be evaluated using ROC-curves produced from the probabilities provided by
    the participants. For the regression problem, mean square error will be used.\r\n\r\nThe
    leaderboard will be visible from the 01.05.2018. However, the submission system
    will remain open few more days."
  evaluation: |
    <p>The results will be evaluated considering this task as a binary classification problem and as a regression problem. The classification problem will be evaluated using ROC-curves produced from the probabilities provided by the participants. For the regression problem, mean square error will be used.</p>

    <p>The leaderboard will be visible from the 01.05.2018. However, the submission system will remain open few more days.</p>
  rules_markdown: "_Note: In order to participate in this challenge you have to sign
    an End User Agreement (EUA). You will find more information on the 'Dataset' tab._\r\n\r\nImageCLEF
    lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF
    2018 consists of independent peer-reviewed workshops on a broad range of challenges
    in the fields of multilingual and multimodal information access evaluation, and
    a set of benchmarking activities carried in various labs designed to test different
    aspects of mono and cross-language Information retrieval systems. More details
    about the conference can be found [ here ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    .\r\n\r\nSubmitting a working note with the full description of the methods used
    in each run is mandatory. Any run that could not be reproduced thanks to its description
    in the working notes might be removed from the official publication of the results.
    Working notes are published within CEUR-WS proceedings, resulting in an assignment
    of an individual DOI (URN) and an indexing by many bibliography systems including
    DBLP. According to the CEUR-WS policies, a light review of the working notes will
    be conducted by ImageCLEF organizing committee to ensure quality. As an illustration,
    ImageCLEF 2017 working notes (task overviews and participant working notes) can
    be found within [ CLEF 2017 CEUR-WS ](http://ceur-ws.org/Vol-1866/){:target='_blank'}
    proceedings.\r\n\r\n### Important\r\n\r\nParticipants of this challenge will automatically
    be registered at CLEF 2018. In order to be compliant with the CLEF registration
    requirements, please edit your profile by providing the following additional information:\r\n\r\n*
    First name\r\n\r\n* Last name\r\n\r\n* Affiliation\r\n\r\n* Address\r\n\r\n* City\r\n\r\n*
    Country\r\n\r\n* _Regarding the username, please choose a name that represents
    your team._\r\n\r\n_This information will not be publicly visible and will be
    exclusively used to contact you and to send the registration data to CLEF, which
    is the main organizer of all CLEF labs_\r\n\r\n### Participating as an individual
    (non affiliated) researcher\r\n\r\nWe welcome individual researchers, i.e. not
    affiliated to any institution, to participate. We kindly ask you to provide us
    with a motivation letter containing the following information:\r\n\r\n- the presentation
    of your most relevant research activities related to the task/tasks\r\n\r\n- your
    motivation for participating in the task/tasks and how you want to exploit the
    results\r\n\r\n- a list of the most relevant 5 publications (if applicable)\r\n\r\n-
    the link to your personal webpage\r\n\r\nThe motivation letter should be directly
    concatenated to the End User Agreement document or sent as a PDF file to bionescu
    at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing
    committee. We reserve the right to refuse any applicants whose experience in the
    field is too narrow, and would therefore most likely prevent them from being able
    to finish the task/tasks.\r\n"
  rules: |
    <p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Dataset’ tab.</em></p>

    <p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF 2018 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href="http://clef2018.clef-initiative.eu/" target="_blank"> here </a> .</p>

    <p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2017 working notes (task overviews and participant working notes) can be found within <a href="http://ceur-ws.org/Vol-1866/" target="_blank"> CLEF 2017 CEUR-WS </a> proceedings.</p>

    <h3 id="important">Important</h3>

    <p>Participants of this challenge will automatically be registered at CLEF 2018. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

    <ul>
      <li>
        <p>First name</p>
      </li>
      <li>
        <p>Last name</p>
      </li>
      <li>
        <p>Affiliation</p>
      </li>
      <li>
        <p>Address</p>
      </li>
      <li>
        <p>City</p>
      </li>
      <li>
        <p>Country</p>
      </li>
      <li>
        <p><em>Regarding the username, please choose a name that represents your team.</em></p>
      </li>
    </ul>

    <p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

    <h3 id="participating-as-an-individual-non-affiliated-researcher">Participating as an individual (non affiliated) researcher</h3>

    <p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

    <ul>
      <li>
        <p>the presentation of your most relevant research activities related to the task/tasks</p>
      </li>
      <li>
        <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
      </li>
      <li>
        <p>a list of the most relevant 5 publications (if applicable)</p>
      </li>
      <li>
        <p>the link to your personal webpage</p>
      </li>
    </ul>

    <p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>
  prizes_markdown: ImageCLEF 2018 is an evaluation campaign that is being organized
    as part of the [ CLEF initiative ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    labs. The campaign offers several research tasks that welcome participation from
    teams around the world. The results of the campaign appear in the working notes
    proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions
    among the participants, will be invited for publication in the following year
    in the Springer Lecture Notes in Computer Science (LNCS) together with the annual
    lab overviews.
  prizes: '<p>ImageCLEF 2018 is an evaluation campaign that is being organized as
    part of the <a href="http://clef2018.clef-initiative.eu/" target="_blank"> CLEF
    initiative </a> labs. The campaign offers several research tasks that welcome
    participation from teams around the world. The results of the campaign appear
    in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org).
    Selected contributions among the participants, will be invited for publication
    in the following year in the Springer Lecture Notes in Computer Science (LNCS)
    together with the annual lab overviews.</p>

'
  resources_markdown: "###Contact us\r\n- Technical issues :[  https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring
    ]( https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring){:target='_blank'}\r\n-
    Discussion Forum :[  https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics
    ]( https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics){:target='_blank'}\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n- Sharada Prasanna Mohanty:
    sharada.mohanty@epfl.ch\r\n- Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch\r\n-
    Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch\r\n- Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch\r\n\r\n###
    More information\r\n\r\nYou can find additional information on the challenge here:\r\n[
    http://imageclef.org/2018/tuberculosis ](http://imageclef.org/2018/tuberculosis){:target='_blank'}"
  resources: |
    <h3 id="contact-us">Contact us</h3>
    <ul>
      <li>Technical issues :<a href="https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring" target="_blank">  https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring </a></li>
      <li>Discussion Forum :<a href="https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics" target="_blank">  https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics </a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
      <li>Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch</li>
      <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
      <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
    </ul>

    <h3 id="more-information">More information</h3>

    <p>You can find additional information on the challenge here:
    <a href="http://imageclef.org/2018/tuberculosis" target="_blank"> http://imageclef.org/2018/tuberculosis </a></p>
  submission_instructions_markdown: "_Please provide the necessary information and
    select a submission file. As soon as a submission file is selected the form is
    submitted automatically. \r\nAfter the submission of the form the grading process
    will be initiated where an external grader validates/evaluates the submitted file
    and eventually returns the score back to CrowdAI. Depending on the file size,
    the evaluation algorithm and the total grading workload this could take a while.
    Your can see the status of your submission in the \"Submissions\" tab of this
    challenge's page, where you will redirected to automatically after having submitted.
    In case the evaluation failed, the \"Status\" field shows \"failed\" and an error
    message in the \"Message\" field is displayed._"
  submission_instructions: "<p><em>Please provide the necessary information and select
    a submission file. As soon as a submission file is selected the form is submitted
    automatically. \nAfter the submission of the form the grading process will be
    initiated where an external grader validates/evaluates the submitted file and
    eventually returns the score back to CrowdAI. Depending on the file size, the
    evaluation algorithm and the total grading workload this could take a while. Your
    can see the status of your submission in the “Submissions” tab of this challenge’s
    page, where you will redirected to automatically after having submitted. In case
    the evaluation failed, the “Status” field shows “failed” and an error message
    in the “Message” field is displayed.</em></p>\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: tuberculosis_svr_2.JPG
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 2
  clef_challenge: true
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: CLEFChallenges
  online_submissions: true
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: true
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_15:
  id: 15
  organizer_id: 7
  challenge: ImageCLEF 2018 Caption - Concept Detection
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &36 2017-11-06 14:52:47.482661000 Z
    zone: *2
    time: *36
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &37 2018-10-03 15:58:07.098250000 Z
    zone: *2
    time: *37
  tagline: Identifying relevant concepts in a large corpus of medical images
  primary_sort_order_cd: descending
  secondary_sort_order_cd: not_used
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 2324
  participant_count: 103
  submission_count: 30
  score_title: F1
  score_secondary_title: ''
  slug: imageclef-2018-caption-concept-detection
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: concept_detection_clef2018
  online_grading: true
  vote_count: 14
  description_markdown: "*****\r\n**Important note:**\r\n\r\n_The **ImageCLEF Caption
    - Concept Detection challenge has officially ended** and we would like to thank
    everybody for their participation. You can find the official results at [http://imageclef.org/2018/caption](http://imageclef.org/2018/caption)._\r\n\r\n_Post-challenge
    submissions and the leaderboard will remain enabled  for a few weeks so you will
    still be able to submit result files and have them continuously evaluated during
    a limited period. \r\nPlease consider that in order to see the version of the
    leaderboard with the post-challenge submissions integrated, you have to turn on
    the switch **Show post-challenge submission** right below the leaderboard._\r\n\r\n_At
    the same time we'd like to encourage you to submit a [CLEF Working notes paper](http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html)
    until the end of May._\r\n\r\n_Please also note that participants registering
    from now on will not be\r\nautomatically registered with CLEF anymore._\r\n\r\n*****\r\n\r\n_Note:
    ImageCLEF Caption 2018 is divided into 2 subtasks (challenges). This challenge
    is about **Concept Detection**. For information on the **Caption Prediction**
    challenge click [ here ](/challenges/imageclef-2018-caption-caption-prediction){:target='_blank'}.
    Both challenges share the same dataset, so registering for one of these challenges
    will automatically give you access to the other one._\r\n\r\n_Note: Do not forget
    to read the **Rules** section on this page_\r\n\r\n### Motivation\r\n\r\nInterpreting
    and summarizing the insights gained from medical images such as radiology output
    is a time-consuming task that involves highly trained experts and often represents
    a bottleneck in clinical diagnosis pipelines.\r\n\r\nConsequently, there is a
    considerable need for automatic methods that can approximate this mapping from
    visual information to condensed textual descriptions. In this task, we cast the
    problem of image understanding as a cross-modality matching scenario in which
    visual content and textual descriptors need to be aligned and concise textual
    interpretations of medical images are generated. We work on the basis of a large-scale
    collection of figures from open access biomedical journal articles (PubMed Central).
    Each image is accompanied by its original caption, constituting a natural testbed
    for this image captioning task.\r\n\r\n_Lessons learned_: In the first edition
    of this task, held at CLEF 2017, participants noted a broad topical variability
    among training images. This year, we will further group training data into image
    types (e.g., radiology vs. biopsy) and task participants will building either
    cross category models or category-specific ones. An additional source of uncertainty
    was noted in the use of external material. In this second edition of the task,
    we will clearly separate systems using exclusively the official training data
    from those that incorporate additional sources of evidence.\r\n\r\n### Challenge
    description\r\n\r\nAs a first step to automatic image captioning and scene understanding,
    participating systems are tasked with identifying the presence and location of
    relevant concepts in a large corpus of medical images. Based on the visual image
    content, this subtask provides the building blocks for the scene understanding
    step by identifying the individual components from which captions are composed.
    Evaluation is conducted in terms of set coverage metrics such as precision, recall
    and combinations thereof. This task will be run on a subset of the data as manual
    ground truthing is required.\r\n\r\n### Data\r\n\r\nThe collection comprises a
    total of 4 million image-caption pairs that could potentially all be used for
    training with a small subset being removed for testing. To focus on useful radiology/clinical
    images and non-compound figures is likely good for this task to reduce the number
    of image-caption pairs to around 400,000, so significantly larger that in 2017.\r\n\r\n###
    Submission instructions\r\n\r\n*****\r\n_As soon as the submission is open, you
    will find a \"Create Submission\" button on this page (just next to the tabs)_\r\n\r\n*****\r\n\r\nFor
    the submission we expect the following format:\r\n\r\n[Figure-ID] [TAB] [Concept-ID-1];[Concept-ID-2];[Concept-ID-n]\r\n\r\ne.g.:\r\n\r\n~~~
    \r\n1743-422X-4-12-1-4 C1;C6;C100\r\n1743-422X-4-12-1-3 C89;C374\r\n1743-422X-4-12-1-2
    C8374\r\n~~~\r\n\r\nYou need to respect the following constraints:\r\n\r\n- The
    separator between the figure ID and the concepts has to be a tabular whitespace\r\n-
    The separator between the UMLS concepts has to be a semicolon (;)\r\n- The same
    concept cannot be specified more than once for a given figure ID\r\n- Each figure
    ID of the testset must be included in the submission file exactly once (even if
    there are no concepts)\r\n\r\n\r\n### Acknowledgements\r\n\r\n[ PubMed Central
    ](http://www.ncbi.nlm.nih.gov/pmc/){:target='_blank'}"
  description: "<hr />\n<p><strong>Important note:</strong></p>\n\n<p><em>The <strong>ImageCLEF
    Caption - Concept Detection challenge has officially ended</strong> and we would
    like to thank everybody for their participation. You can find the official results
    at <a href=\"http://imageclef.org/2018/caption\">http://imageclef.org/2018/caption</a>.</em></p>\n\n<p><em>Post-challenge
    submissions and the leaderboard will remain enabled  for a few weeks so you will
    still be able to submit result files and have them continuously evaluated during
    a limited period. \nPlease consider that in order to see the version of the leaderboard
    with the post-challenge submissions integrated, you have to turn on the switch
    <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>\n\n<p><em>At
    the same time we’d like to encourage you to submit a <a href=\"http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html\">CLEF
    Working notes paper</a> until the end of May.</em></p>\n\n<p><em>Please also note
    that participants registering from now on will not be\nautomatically registered
    with CLEF anymore.</em></p>\n\n<hr />\n\n<p><em>Note: ImageCLEF Caption 2018 is
    divided into 2 subtasks (challenges). This challenge is about <strong>Concept
    Detection</strong>. For information on the <strong>Caption Prediction</strong>
    challenge click <a href=\"/challenges/imageclef-2018-caption-caption-prediction\"
    target=\"_blank\"> here </a>. Both challenges share the same dataset, so registering
    for one of these challenges will automatically give you access to the other one.</em></p>\n\n<p><em>Note:
    Do not forget to read the <strong>Rules</strong> section on this page</em></p>\n\n<h3
    id=\"motivation\">Motivation</h3>\n\n<p>Interpreting and summarizing the insights
    gained from medical images such as radiology output is a time-consuming task that
    involves highly trained experts and often represents a bottleneck in clinical
    diagnosis pipelines.</p>\n\n<p>Consequently, there is a considerable need for
    automatic methods that can approximate this mapping from visual information to
    condensed textual descriptions. In this task, we cast the problem of image understanding
    as a cross-modality matching scenario in which visual content and textual descriptors
    need to be aligned and concise textual interpretations of medical images are generated.
    We work on the basis of a large-scale collection of figures from open access biomedical
    journal articles (PubMed Central). Each image is accompanied by its original caption,
    constituting a natural testbed for this image captioning task.</p>\n\n<p><em>Lessons
    learned</em>: In the first edition of this task, held at CLEF 2017, participants
    noted a broad topical variability among training images. This year, we will further
    group training data into image types (e.g., radiology vs. biopsy) and task participants
    will building either cross category models or category-specific ones. An additional
    source of uncertainty was noted in the use of external material. In this second
    edition of the task, we will clearly separate systems using exclusively the official
    training data from those that incorporate additional sources of evidence.</p>\n\n<h3
    id=\"challenge-description\">Challenge description</h3>\n\n<p>As a first step
    to automatic image captioning and scene understanding, participating systems are
    tasked with identifying the presence and location of relevant concepts in a large
    corpus of medical images. Based on the visual image content, this subtask provides
    the building blocks for the scene understanding step by identifying the individual
    components from which captions are composed. Evaluation is conducted in terms
    of set coverage metrics such as precision, recall and combinations thereof. This
    task will be run on a subset of the data as manual ground truthing is required.</p>\n\n<h3
    id=\"data\">Data</h3>\n\n<p>The collection comprises a total of 4 million image-caption
    pairs that could potentially all be used for training with a small subset being
    removed for testing. To focus on useful radiology/clinical images and non-compound
    figures is likely good for this task to reduce the number of image-caption pairs
    to around 400,000, so significantly larger that in 2017.</p>\n\n<h3 id=\"submission-instructions\">Submission
    instructions</h3>\n\n<hr />\n<p><em>As soon as the submission is open, you will
    find a “Create Submission” button on this page (just next to the tabs)</em></p>\n\n<hr
    />\n\n<p>For the submission we expect the following format:</p>\n\n<p>[Figure-ID]
    [TAB] [Concept-ID-1];[Concept-ID-2];[Concept-ID-n]</p>\n\n<p>e.g.:</p>\n\n<pre><code>1743-422X-4-12-1-4
    C1;C6;C100\n1743-422X-4-12-1-3 C89;C374\n1743-422X-4-12-1-2 C8374\n</code></pre>\n\n<p>You
    need to respect the following constraints:</p>\n\n<ul>\n  <li>The separator between
    the figure ID and the concepts has to be a tabular whitespace</li>\n  <li>The
    separator between the UMLS concepts has to be a semicolon (;)</li>\n  <li>The
    same concept cannot be specified more than once for a given figure ID</li>\n  <li>Each
    figure ID of the testset must be included in the submission file exactly once
    (even if there are no concepts)</li>\n</ul>\n\n<h3 id=\"acknowledgements\">Acknowledgements</h3>\n\n<p><a
    href=\"http://www.ncbi.nlm.nih.gov/pmc/\" target=\"_blank\"> PubMed Central </a></p>\n"
  evaluation_markdown: "Evaluation is conducted in terms of **F1 scores** between
    system predicted and ground truth concepts, using the following methodology and
    parameters:\r\n\r\n* The default implementation of the Python scikit-learn (v0.17.1-2)
    F1 scoring method is used. It is documented here.\r\n\r\n* A Python (3.x) script
    loads the candidate run file, as well as the ground truth (GT) file, and processes
    each candidate-GT concept sets\r\n\r\n* For each candidate-GT concept set, the
    **y_pred** and **y_true** arrays are generated. They are binary arrays indicating
    for each concept contained in both candidate and GT set if it is present (1) or
    not (0).\r\n\r\n* The F1 score is then calculated. The default 'binary' averaging
    method is used.\r\n\r\n* All F1 scores are summed and averaged over the number
    of elements in the test set (10'000), giving the final score.\r\n\r\nThe ground
    truth for the test set was generated based on the [ UMLS Full Release 2016AB ](https://download.nlm.nih.gov/umls/kss/2016AB/umls-2016AB-full.zip){:target='_blank'}.\r\n\r\n**NOTE**
    : The source code of the evaluation tool is available here. It **must** be executed
    using Python **3.x**, on a system where the scikit-learn (**>= v0.17.1-2**) Python
    library is installed. The script should be run like this:\r\n\r\n```\r\n/path/to/python3
    evaluate-f1.py /path/to/candidate/file /path/to/ground-truth/file\r\n```\r\n\r\n*****\r\n_The
    leaderboard will be visible from 01.05.2018 (official deadline) on. The submission
    system will remain open few more days. Results submitted after deadline will not
    be part of the official results._\r\n\r\n*****\r\n"
  evaluation: |
    <p>Evaluation is conducted in terms of <strong>F1 scores</strong> between system predicted and ground truth concepts, using the following methodology and parameters:</p>

    <ul>
      <li>
        <p>The default implementation of the Python scikit-learn (v0.17.1-2) F1 scoring method is used. It is documented here.</p>
      </li>
      <li>
        <p>A Python (3.x) script loads the candidate run file, as well as the ground truth (GT) file, and processes each candidate-GT concept sets</p>
      </li>
      <li>
        <p>For each candidate-GT concept set, the <strong>y_pred</strong> and <strong>y_true</strong> arrays are generated. They are binary arrays indicating for each concept contained in both candidate and GT set if it is present (1) or not (0).</p>
      </li>
      <li>
        <p>The F1 score is then calculated. The default ‘binary’ averaging method is used.</p>
      </li>
      <li>
        <p>All F1 scores are summed and averaged over the number of elements in the test set (10’000), giving the final score.</p>
      </li>
    </ul>

    <p>The ground truth for the test set was generated based on the <a href="https://download.nlm.nih.gov/umls/kss/2016AB/umls-2016AB-full.zip" target="_blank"> UMLS Full Release 2016AB </a>.</p>

    <p><strong>NOTE</strong> : The source code of the evaluation tool is available here. It <strong>must</strong> be executed using Python <strong>3.x</strong>, on a system where the scikit-learn (<strong>&gt;= v0.17.1-2</strong>) Python library is installed. The script should be run like this:</p>

    <p><code>
    /path/to/python3 evaluate-f1.py /path/to/candidate/file /path/to/ground-truth/file
    </code></p>

    <hr />
    <p><em>The leaderboard will be visible from 01.05.2018 (official deadline) on. The submission system will remain open few more days. Results submitted after deadline will not be part of the official results.</em></p>

    <hr />
  rules_markdown: "_Note: In order to participate in this challenge you have to sign
    an End User Agreement (EUA). You will find more information on the 'Dataset' tab._\r\nImageCLEF
    lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF
    2018 consists of independent peer-reviewed workshops on a broad range of challenges
    in the fields of multilingual and multimodal information access evaluation, and
    a set of benchmarking activities carried in various labs designed to test different
    aspects of mono and cross-language Information retrieval systems. More details
    about the conference can be found [ here ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    .\r\n\r\nSubmitting a working note with the full description of the methods used
    in each run is mandatory. Any run that could not be reproduced thanks to its description
    in the working notes might be removed from the official publication of the results.
    Working notes are published within CEUR-WS proceedings, resulting in an assignment
    of an individual DOI (URN) and an indexing by many bibliography systems including
    DBLP. According to the CEUR-WS policies, a light review of the working notes will
    be conducted by ImageCLEF organizing committee to ensure quality. As an illustration,
    ImageCLEF 2017 working notes (task overviews and participant working notes) can
    be found within [ CLEF 2017 CEUR-WS ](http://ceur-ws.org/Vol-1866/){:target='_blank'}
    proceedings.\r\n\r\n### Important\r\n\r\nParticipants of this challenge will automatically
    be registered at CLEF 2018. In order to be compliant with the CLEF registration
    requirements, please edit your profile by providing the following additional information:\r\n\r\n*
    First name\r\n\r\n* Last name\r\n\r\n* Affiliation\r\n\r\n* Address\r\n\r\n* City\r\n\r\n*
    Country\r\n\r\n* _Regarding the username, please choose a name that represents
    your team._\r\n\r\n_This information will not be publicly visible and will be
    exclusively used to contact you and to send the registration data to CLEF, which
    is the main organizer of all CLEF labs_\r\n\r\n### Participating as an individual
    (non affiliated) researcher\r\n\r\nWe welcome individual researchers, i.e. not
    affiliated to any institution, to participate. We kindly ask you to provide us
    with a motivation letter containing the following information:\r\n\r\n- the presentation
    of your most relevant research activities related to the task/tasks\r\n\r\n- your
    motivation for participating in the task/tasks and how you want to exploit the
    results\r\n\r\n- a list of the most relevant 5 publications (if applicable)\r\n\r\n-
    the link to your personal webpage\r\n\r\nThe motivation letter should be directly
    concatenated to the End User Agreement document or sent as a PDF file to bionescu
    at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing
    committee. We reserve the right to refuse any applicants whose experience in the
    field is too narrow, and would therefore most likely prevent them from being able
    to finish the task/tasks.\r\n"
  rules: |
    <p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Dataset’ tab.</em>
    ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF 2018 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href="http://clef2018.clef-initiative.eu/" target="_blank"> here </a> .</p>

    <p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2017 working notes (task overviews and participant working notes) can be found within <a href="http://ceur-ws.org/Vol-1866/" target="_blank"> CLEF 2017 CEUR-WS </a> proceedings.</p>

    <h3 id="important">Important</h3>

    <p>Participants of this challenge will automatically be registered at CLEF 2018. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

    <ul>
      <li>
        <p>First name</p>
      </li>
      <li>
        <p>Last name</p>
      </li>
      <li>
        <p>Affiliation</p>
      </li>
      <li>
        <p>Address</p>
      </li>
      <li>
        <p>City</p>
      </li>
      <li>
        <p>Country</p>
      </li>
      <li>
        <p><em>Regarding the username, please choose a name that represents your team.</em></p>
      </li>
    </ul>

    <p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

    <h3 id="participating-as-an-individual-non-affiliated-researcher">Participating as an individual (non affiliated) researcher</h3>

    <p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

    <ul>
      <li>
        <p>the presentation of your most relevant research activities related to the task/tasks</p>
      </li>
      <li>
        <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
      </li>
      <li>
        <p>a list of the most relevant 5 publications (if applicable)</p>
      </li>
      <li>
        <p>the link to your personal webpage</p>
      </li>
    </ul>

    <p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>
  prizes_markdown: ImageCLEF 2018 is an evaluation campaign that is being organized
    as part of the [ CLEF initiative ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    labs. The campaign offers several research tasks that welcome participation from
    teams around the world. The results of the campaign appear in the working notes
    proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions
    among the participants, will be invited for publication in the following year
    in the Springer Lecture Notes in Computer Science (LNCS) together with the annual
    lab overviews.
  prizes: '<p>ImageCLEF 2018 is an evaluation campaign that is being organized as
    part of the <a href="http://clef2018.clef-initiative.eu/" target="_blank"> CLEF
    initiative </a> labs. The campaign offers several research tasks that welcome
    participation from teams around the world. The results of the campaign appear
    in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org).
    Selected contributions among the participants, will be invited for publication
    in the following year in the Springer Lecture Notes in Computer Science (LNCS)
    together with the annual lab overviews.</p>

'
  resources_markdown: "###Contact us\r\n\r\n- Technical issues : [ https://gitter.im/crowdAI/imageclef-2018-caption-concept-detection
    ](https://gitter.im/crowdAI/imageclef-2018-caption-concept-detection){:target='_blank'}\r\n-
    Discussion Forum : [ https://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/topics
    ](https://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/topics){:target='_blank'}\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n- Sharada Prasanna Mohanty:
    sharada.mohanty@epfl.ch\r\n- Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk\r\n-
    Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch\r\n- Vincent Adrearczyk: vincent[DOT]andrearczyk[AT]hevs[DOT]ch\r\n-
    Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch\r\n\r\n### More information\r\n\r\nYou
    can find additional information on the challenge here:\r\n[ http://imageclef.org/2018/caption
    ](http://imageclef.org/2018/caption){:target='_blank'}"
  resources: |
    <h3 id="contact-us">Contact us</h3>

    <ul>
      <li>Technical issues : <a href="https://gitter.im/crowdAI/imageclef-2018-caption-concept-detection" target="_blank"> https://gitter.im/crowdAI/imageclef-2018-caption-concept-detection </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/topics" target="_blank"> https://www.crowdai.org/challenges/imageclef-2018-caption-concept-detection/topics </a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
      <li>Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk</li>
      <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
      <li>Vincent Adrearczyk: vincent[DOT]andrearczyk[AT]hevs[DOT]ch</li>
      <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
    </ul>

    <h3 id="more-information">More information</h3>

    <p>You can find additional information on the challenge here:
    <a href="http://imageclef.org/2018/caption" target="_blank"> http://imageclef.org/2018/caption </a></p>
  submission_instructions_markdown: "_Please provide the necessary information and
    select a submission file. As soon as a submission file is selected the form is
    submitted automatically. \r\nAfter the submission of the form the grading process
    will be initiated where an external grader validates/evaluates the submitted file
    and eventually returns the score back to CrowdAI. Depending on the file size,
    the evaluation algorithm and the total grading workload this could take a while.
    Your can see the status of your submission in the \"Submissions\" tab of this
    challenge's page, where you will redirected to automatically after having submitted.
    In case the evaluation failed, the \"Status\" field shows \"failed\" and an error
    message in the \"Message\" field is displayed._"
  submission_instructions: "<p><em>Please provide the necessary information and select
    a submission file. As soon as a submission file is selected the form is submitted
    automatically. \nAfter the submission of the form the grading process will be
    initiated where an external grader validates/evaluates the submitted file and
    eventually returns the score back to CrowdAI. Depending on the file size, the
    evaluation algorithm and the total grading workload this could take a while. Your
    can see the status of your submission in the “Submissions” tab of this challenge’s
    page, where you will redirected to automatically after having submitted. In case
    the evaluation failed, the “Status” field shows “failed” and an error message
    in the “Message” field is displayed.</em></p>\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: concept_det.jpg
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 1
  clef_challenge: true
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: CLEFChallenges
  online_submissions: true
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: true
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_19:
  id: 19
  organizer_id: 7
  challenge: ImageCLEF 2018 Tuberculosis - TBT classification
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &38 2017-11-07 09:48:16.344618000 Z
    zone: *2
    time: *38
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &39 2018-10-04 09:17:52.076693000 Z
    zone: *2
    time: *39
  tagline: Classification of tuberculosis types using CT volumes
  primary_sort_order_cd: descending
  secondary_sort_order_cd: descending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 2046
  participant_count: 103
  submission_count: 52
  score_title: ACC
  score_secondary_title: KAPPA
  slug: imageclef-2018-tuberculosis-tbt-classification
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: tuberculosis_tb_type_clef2018
  online_grading: true
  vote_count: 13
  description_markdown: "*****\r\n**Important note:**\r\n\r\n_The **ImageCLEF Tuberculosis
    - TBT classification challenge has officially ended** and we would like to thank
    everybody for their participation. You can find the official results at [http://imageclef.org/2018/tuberculosis
    ](http://imageclef.org/2018/tuberculosis)._\r\n\r\n_Post-challenge submissions
    and the leaderboard will remain enabled  for a few weeks so you will still be
    able to submit result files and have them continuously evaluated during a limited
    period. \r\nPlease consider that in order to see the version of the leaderboard
    with the post-challenge submissions integrated, you have to turn on the switch
    **Show post-challenge submission** right below the leaderboard._\r\n\r\n_At the
    same time we'd like to encourage you to submit a [CLEF Working notes paper](http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html)
    until the end of May._\r\n\r\n_Please also note that participants registering
    from now on will not be\r\nautomatically registered with CLEF anymore._\r\n\r\n*****\r\n\r\n_Note:
    ImageCLEF Tuberculosis 2018 is divided into 3 subtasks (challenges). This challenge
    is about **TBT (tuberculosis type) Classfication**. For information on the **MDR
    (multi-drug-resistance) Detection** challenge click [ here ](/challenges/imageclef-2018-tuberculosis-mdr-detection){:target='_blank'}.
    For information on the **Severity Scoring** challenge click [ here ](/challenges/imageclef-2018-tuberculosis-severity-scoring){:target='_blank'}.
    All of these challenges share the same dataset, so registering for one of these
    challenges will automatically give you access to the other ones._\r\n\r\n_Note:
    Do not forget to read the **Rules** section on this page_\r\n\r\n### Motivation\r\n\r\nAbout
    130 years after the discovery of Mycobacterium tuberculosis, the disease remains
    a persistent threat and a leading cause of death worldwide.\r\n\r\nThe greatest
    disaster that can happen to a patient with tuberculosis (TB) is that the organisms
    become resistant to two or more of the standard drugs. In contrast to drug sensitive
    (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult
    and expensive to recover from. Thus, early detection of the drug resistance (DR)
    status is of great importance for effective treatment. The most commonly used
    methods of DR detection are either expensive or take too much time (up to several
    month). Therefore there is a need for quick and at the same time cheap methods
    of DR detection. One of the possible approaches for this task is based on Computed
    Tomography (CT) image analysis. Another challenging task is automatic detection
    of TB types (TBT) using CT volumes.\r\n\r\n_Differences compared to 2017_: Scoring
    the severity of TB cases based on chest CT images is another task compared to
    both tuberculosis-related subtasks considered in 2017. There are no direct links
    between them. Note only that original CT image datasets used in 2017 and in 2018
    may slightly overlap.\r\n\r\n### Challenge description\r\n\r\nThe goal of this
    subtask is to automatically categorize each TB case into one of the following
    five types: Infiltrative, Focal, Tuberculoma, Miliary, Fibro-cavernous.\r\n\r\n###
    Data\r\n\r\nThe dataset used in this task includes chest CT scans of TB patients
    along with the TB type. Some patients include more than one scan. All scans belonging
    to the same patient present the same TB type.\r\n\r\n\r\n| Num. Patients (Num.
    CTs)      | Train | Test |\r\n| --- |:---:| :---:|\r\n| Type 1         | 228 (376)
    \    | 89 (176)    |\r\n| Type 2         | 210 (273)     | 80 (115)    |\r\n|
    Type 3         | 100 (154)     | 60 (86)    |\r\n| Type 4         | 79 (106)     |
    50 (71)    |\r\n| Type 5         | 60 (99)     | 38 (57)    |\r\n| **Total patients
    (CTs)** | **677 (1008)**  | **317 (505)**  |\r\n\r\nWe provide 3D CT images with
    slice size of 512*512 pixels and number of slices varying from about 50 to 400.
    All the CT images are stored in NIFTI file format with .nii.gz file extension
    (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield
    units (HU) as well the corresponding image metadata such as image dimensions,
    voxel size in physical units, slice thickness, etc. A freely-available tool called
    [ \"VV\" ](https://www.creatis.insa-lyon.fr/rio/vv){:target='_blank'} can be used
    for viewing image files. Currently, there are various tools available for reading
    and writing NIFTI files. Among them there are [ load_nii ](https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m){:target='_blank'}
    and [ save_nii ](https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m){:target='_blank'}
    functions for Matlab and [ Niftilib ](http://niftilib.sourceforge.net/){:target='_blank'}
    library for C, Java, Matlab and Python.\r\n\r\nWe also provide automatic extracted
    masks of the lungs. This material can be downloaded together with the patients
    CT images. The details of this segmentation can be found [ here ](http://publications.hevs.ch/index.php/publications/show/1871){:target='_blank'}.\r\nIn
    case the participants use these masks in their experiments, please refer to the
    section \"Citations\" to find the appropriate citation for this lung segmentation
    technique.\r\n\r\n### Submission instructions\r\n\r\n*****\r\n_As soon as the
    submission is open, you will find a \"Create Submission\" button on this page
    (just next to the tabs)_\r\n\r\n*****\r\n\r\nSubmit a plain text file named with
    the prefix **TBT** (e.g. TBTfree-text.txt) with the following format:\r\n\r\n\\<Patient-ID\\>,\\<TB-Type\\>\r\n\r\ne.g.:\r\n\r\n~~~\r\nTBT_TST_501,1\r\nTBT_TST_502,3\r\nTBT_TST_503,5\r\nTBT_TST_504,4\r\nTBT_TST_505,2\r\n~~~\r\n\r\n\r\n**Please
    use the following Codes for the TB types:**\r\n\r\n* 1 for Infiltrative\r\n* 2
    for Focal\r\n* 3 for Tuberculoma\r\n* 4 for Miliary\r\n* 5 for Fibro-cavernous\r\n\r\n**You
    need to respect the following constraints:**\r\n\r\n* Patient-IDs are obtained
    as follows:\r\n  * Image-IDs: {TBT_TST_001_01, TBT_TST_001_02, TBT_TST_001_03}
    --> Patient-ID: TBT_TST_001\r\n  * Image-IDs: {TBT_TST_002_01} --> Patient-ID:
    TBT_TST_002\r\n* Patient-IDs must be part of the predefined Patient-IDs\r\n* All
    patient-IDs must be present in the runfiles\r\n* Only use the defined codes for
    the various TB types\r\n* Only use one TB type per patient\r\n\r\n### Citations\r\n\r\nInformation
    will be posted after the challenge ends.\r\n\r\n"
  description: "<hr />\n<p><strong>Important note:</strong></p>\n\n<p><em>The <strong>ImageCLEF
    Tuberculosis - TBT classification challenge has officially ended</strong> and
    we would like to thank everybody for their participation. You can find the official
    results at <a href=\"http://imageclef.org/2018/tuberculosis\">http://imageclef.org/2018/tuberculosis
    </a>.</em></p>\n\n<p><em>Post-challenge submissions and the leaderboard will remain
    enabled  for a few weeks so you will still be able to submit result files and
    have them continuously evaluated during a limited period. \nPlease consider that
    in order to see the version of the leaderboard with the post-challenge submissions
    integrated, you have to turn on the switch <strong>Show post-challenge submission</strong>
    right below the leaderboard.</em></p>\n\n<p><em>At the same time we’d like to
    encourage you to submit a <a href=\"http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html\">CLEF
    Working notes paper</a> until the end of May.</em></p>\n\n<p><em>Please also note
    that participants registering from now on will not be\nautomatically registered
    with CLEF anymore.</em></p>\n\n<hr />\n\n<p><em>Note: ImageCLEF Tuberculosis 2018
    is divided into 3 subtasks (challenges). This challenge is about <strong>TBT (tuberculosis
    type) Classfication</strong>. For information on the <strong>MDR (multi-drug-resistance)
    Detection</strong> challenge click <a href=\"/challenges/imageclef-2018-tuberculosis-mdr-detection\"
    target=\"_blank\"> here </a>. For information on the <strong>Severity Scoring</strong>
    challenge click <a href=\"/challenges/imageclef-2018-tuberculosis-severity-scoring\"
    target=\"_blank\"> here </a>. All of these challenges share the same dataset,
    so registering for one of these challenges will automatically give you access
    to the other ones.</em></p>\n\n<p><em>Note: Do not forget to read the <strong>Rules</strong>
    section on this page</em></p>\n\n<h3 id=\"motivation\">Motivation</h3>\n\n<p>About
    130 years after the discovery of Mycobacterium tuberculosis, the disease remains
    a persistent threat and a leading cause of death worldwide.</p>\n\n<p>The greatest
    disaster that can happen to a patient with tuberculosis (TB) is that the organisms
    become resistant to two or more of the standard drugs. In contrast to drug sensitive
    (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult
    and expensive to recover from. Thus, early detection of the drug resistance (DR)
    status is of great importance for effective treatment. The most commonly used
    methods of DR detection are either expensive or take too much time (up to several
    month). Therefore there is a need for quick and at the same time cheap methods
    of DR detection. One of the possible approaches for this task is based on Computed
    Tomography (CT) image analysis. Another challenging task is automatic detection
    of TB types (TBT) using CT volumes.</p>\n\n<p><em>Differences compared to 2017</em>:
    Scoring the severity of TB cases based on chest CT images is another task compared
    to both tuberculosis-related subtasks considered in 2017. There are no direct
    links between them. Note only that original CT image datasets used in 2017 and
    in 2018 may slightly overlap.</p>\n\n<h3 id=\"challenge-description\">Challenge
    description</h3>\n\n<p>The goal of this subtask is to automatically categorize
    each TB case into one of the following five types: Infiltrative, Focal, Tuberculoma,
    Miliary, Fibro-cavernous.</p>\n\n<h3 id=\"data\">Data</h3>\n\n<p>The dataset used
    in this task includes chest CT scans of TB patients along with the TB type. Some
    patients include more than one scan. All scans belonging to the same patient present
    the same TB type.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Num. Patients
    (Num. CTs)</th>\n      <th style=\"text-align: center\">Train</th>\n      <th
    style=\"text-align: center\">Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n
    \     <td>Type 1</td>\n      <td style=\"text-align: center\">228 (376)</td>\n
    \     <td style=\"text-align: center\">89 (176)</td>\n    </tr>\n    <tr>\n      <td>Type
    2</td>\n      <td style=\"text-align: center\">210 (273)</td>\n      <td style=\"text-align:
    center\">80 (115)</td>\n    </tr>\n    <tr>\n      <td>Type 3</td>\n      <td
    style=\"text-align: center\">100 (154)</td>\n      <td style=\"text-align: center\">60
    (86)</td>\n    </tr>\n    <tr>\n      <td>Type 4</td>\n      <td style=\"text-align:
    center\">79 (106)</td>\n      <td style=\"text-align: center\">50 (71)</td>\n
    \   </tr>\n    <tr>\n      <td>Type 5</td>\n      <td style=\"text-align: center\">60
    (99)</td>\n      <td style=\"text-align: center\">38 (57)</td>\n    </tr>\n    <tr>\n
    \     <td><strong>Total patients (CTs)</strong></td>\n      <td style=\"text-align:
    center\"><strong>677 (1008)</strong></td>\n      <td style=\"text-align: center\"><strong>317
    (505)</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<p>We provide 3D CT images
    with slice size of 512*512 pixels and number of slices varying from about 50 to
    400. All the CT images are stored in NIFTI file format with .nii.gz file extension
    (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield
    units (HU) as well the corresponding image metadata such as image dimensions,
    voxel size in physical units, slice thickness, etc. A freely-available tool called
    <a href=\"https://www.creatis.insa-lyon.fr/rio/vv\" target=\"_blank\"> “VV” </a>
    can be used for viewing image files. Currently, there are various tools available
    for reading and writing NIFTI files. Among them there are <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m\"
    target=\"_blank\"> load_nii </a> and <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m\"
    target=\"_blank\"> save_nii </a> functions for Matlab and <a href=\"http://niftilib.sourceforge.net/\"
    target=\"_blank\"> Niftilib </a> library for C, Java, Matlab and Python.</p>\n\n<p>We
    also provide automatic extracted masks of the lungs. This material can be downloaded
    together with the patients CT images. The details of this segmentation can be
    found <a href=\"http://publications.hevs.ch/index.php/publications/show/1871\"
    target=\"_blank\"> here </a>.\nIn case the participants use these masks in their
    experiments, please refer to the section “Citations” to find the appropriate citation
    for this lung segmentation technique.</p>\n\n<h3 id=\"submission-instructions\">Submission
    instructions</h3>\n\n<hr />\n<p><em>As soon as the submission is open, you will
    find a “Create Submission” button on this page (just next to the tabs)</em></p>\n\n<hr
    />\n\n<p>Submit a plain text file named with the prefix <strong>TBT</strong> (e.g.
    TBTfree-text.txt) with the following format:</p>\n\n<p>&lt;Patient-ID&gt;,&lt;TB-Type&gt;</p>\n\n<p>e.g.:</p>\n\n<pre><code>TBT_TST_501,1\nTBT_TST_502,3\nTBT_TST_503,5\nTBT_TST_504,4\nTBT_TST_505,2\n</code></pre>\n\n<p><strong>Please
    use the following Codes for the TB types:</strong></p>\n\n<ul>\n  <li>1 for Infiltrative</li>\n
    \ <li>2 for Focal</li>\n  <li>3 for Tuberculoma</li>\n  <li>4 for Miliary</li>\n
    \ <li>5 for Fibro-cavernous</li>\n</ul>\n\n<p><strong>You need to respect the
    following constraints:</strong></p>\n\n<ul>\n  <li>Patient-IDs are obtained as
    follows:\n    <ul>\n      <li>Image-IDs: {TBT_TST_001_01, TBT_TST_001_02, TBT_TST_001_03}
    –&gt; Patient-ID: TBT_TST_001</li>\n      <li>Image-IDs: {TBT_TST_002_01} –&gt;
    Patient-ID: TBT_TST_002</li>\n    </ul>\n  </li>\n  <li>Patient-IDs must be part
    of the predefined Patient-IDs</li>\n  <li>All patient-IDs must be present in the
    runfiles</li>\n  <li>Only use the defined codes for the various TB types</li>\n
    \ <li>Only use one TB type per patient</li>\n</ul>\n\n<h3 id=\"citations\">Citations</h3>\n\n<p>Information
    will be posted after the challenge ends.</p>\n\n"
  evaluation_markdown: "The results will be evaluated using unweighted Cohen’s Kappa
    (sample [ Matlab code ](http://de.mathworks.com/matlabcentral/fileexchange/15365-cohen-s-kappa/content/kappa.m){:target='_blank'}).\r\n\r\nThe
    leaderboard will be visible from the 01.05.2018. However, the submission system
    will remain open few more days."
  evaluation: |
    <p>The results will be evaluated using unweighted Cohen’s Kappa (sample <a href="http://de.mathworks.com/matlabcentral/fileexchange/15365-cohen-s-kappa/content/kappa.m" target="_blank"> Matlab code </a>).</p>

    <p>The leaderboard will be visible from the 01.05.2018. However, the submission system will remain open few more days.</p>
  rules_markdown: "_Note: In order to participate in this challenge you have to sign
    an End User Agreement (EUA). You will find more information on the 'Dataset' tab._\r\n\r\nImageCLEF
    lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF
    2018 consists of independent peer-reviewed workshops on a broad range of challenges
    in the fields of multilingual and multimodal information access evaluation, and
    a set of benchmarking activities carried in various labs designed to test different
    aspects of mono and cross-language Information retrieval systems. More details
    about the conference can be found [ here ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    .\r\n\r\nSubmitting a working note with the full description of the methods used
    in each run is mandatory. Any run that could not be reproduced thanks to its description
    in the working notes might be removed from the official publication of the results.
    Working notes are published within CEUR-WS proceedings, resulting in an assignment
    of an individual DOI (URN) and an indexing by many bibliography systems including
    DBLP. According to the CEUR-WS policies, a light review of the working notes will
    be conducted by ImageCLEF organizing committee to ensure quality. As an illustration,
    ImageCLEF 2017 working notes (task overviews and participant working notes) can
    be found within [ CLEF 2017 CEUR-WS ](http://ceur-ws.org/Vol-1866/){:target='_blank'}
    proceedings.\r\n\r\n### Important\r\n\r\nParticipants of this challenge will automatically
    be registered at CLEF 2018. In order to be compliant with the CLEF registration
    requirements, please edit your profile by providing the following additional information:\r\n\r\n*
    First name\r\n\r\n* Last name\r\n\r\n* Affiliation\r\n\r\n* Address\r\n\r\n* City\r\n\r\n*
    Country\r\n\r\n* _Regarding the username, please choose a name that represents
    your team._\r\n\r\n_This information will not be publicly visible and will be
    exclusively used to contact you and to send the registration data to CLEF, which
    is the main organizer of all CLEF labs_\r\n\r\n### Participating as an individual
    (non affiliated) researcher\r\n\r\nWe welcome individual researchers, i.e. not
    affiliated to any institution, to participate. We kindly ask you to provide us
    with a motivation letter containing the following information:\r\n\r\n- the presentation
    of your most relevant research activities related to the task/tasks\r\n\r\n- your
    motivation for participating in the task/tasks and how you want to exploit the
    results\r\n\r\n- a list of the most relevant 5 publications (if applicable)\r\n\r\n-
    the link to your personal webpage\r\n\r\nThe motivation letter should be directly
    concatenated to the End User Agreement document or sent as a PDF file to bionescu
    at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing
    committee. We reserve the right to refuse any applicants whose experience in the
    field is too narrow, and would therefore most likely prevent them from being able
    to finish the task/tasks.\r\n"
  rules: |
    <p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Dataset’ tab.</em></p>

    <p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF 2018 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href="http://clef2018.clef-initiative.eu/" target="_blank"> here </a> .</p>

    <p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2017 working notes (task overviews and participant working notes) can be found within <a href="http://ceur-ws.org/Vol-1866/" target="_blank"> CLEF 2017 CEUR-WS </a> proceedings.</p>

    <h3 id="important">Important</h3>

    <p>Participants of this challenge will automatically be registered at CLEF 2018. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

    <ul>
      <li>
        <p>First name</p>
      </li>
      <li>
        <p>Last name</p>
      </li>
      <li>
        <p>Affiliation</p>
      </li>
      <li>
        <p>Address</p>
      </li>
      <li>
        <p>City</p>
      </li>
      <li>
        <p>Country</p>
      </li>
      <li>
        <p><em>Regarding the username, please choose a name that represents your team.</em></p>
      </li>
    </ul>

    <p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

    <h3 id="participating-as-an-individual-non-affiliated-researcher">Participating as an individual (non affiliated) researcher</h3>

    <p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

    <ul>
      <li>
        <p>the presentation of your most relevant research activities related to the task/tasks</p>
      </li>
      <li>
        <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
      </li>
      <li>
        <p>a list of the most relevant 5 publications (if applicable)</p>
      </li>
      <li>
        <p>the link to your personal webpage</p>
      </li>
    </ul>

    <p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>
  prizes_markdown: ImageCLEF 2018 is an evaluation campaign that is being organized
    as part of the [ CLEF initiative ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    labs. The campaign offers several research tasks that welcome participation from
    teams around the world. The results of the campaign appear in the working notes
    proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions
    among the participants, will be invited for publication in the following year
    in the Springer Lecture Notes in Computer Science (LNCS) together with the annual
    lab overviews.
  prizes: '<p>ImageCLEF 2018 is an evaluation campaign that is being organized as
    part of the <a href="http://clef2018.clef-initiative.eu/" target="_blank"> CLEF
    initiative </a> labs. The campaign offers several research tasks that welcome
    participation from teams around the world. The results of the campaign appear
    in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org).
    Selected contributions among the participants, will be invited for publication
    in the following year in the Springer Lecture Notes in Computer Science (LNCS)
    together with the annual lab overviews.</p>

'
  resources_markdown: "###Contact us\r\n\r\n- Technical issues : [ https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification
    ](https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification){:target='_blank'}\r\n-
    Discussion Forum : [ https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics
    ](https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics){:target='_blank'}\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n- Sharada Prasanna Mohanty:
    sharada.mohanty@epfl.ch\r\n- Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch\r\n-
    Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch\r\n- Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch\r\n\r\n###
    More information\r\n\r\nYou can find additional information on the challenge here:\r\n[
    http://imageclef.org/2018/tuberculosis ](http://imageclef.org/2018/tuberculosis){:target='_blank'}"
  resources: |
    <h3 id="contact-us">Contact us</h3>

    <ul>
      <li>Technical issues : <a href="https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification" target="_blank"> https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics" target="_blank"> https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics </a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
      <li>Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch</li>
      <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
      <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
    </ul>

    <h3 id="more-information">More information</h3>

    <p>You can find additional information on the challenge here:
    <a href="http://imageclef.org/2018/tuberculosis" target="_blank"> http://imageclef.org/2018/tuberculosis </a></p>
  submission_instructions_markdown: "_Please provide the necessary information and
    select a submission file. As soon as a submission file is selected the form is
    submitted automatically. \r\nAfter the submission of the form the grading process
    will be initiated where an external grader validates/evaluates the submitted file
    and eventually returns the score back to CrowdAI. Depending on the file size,
    the evaluation algorithm and the total grading workload this could take a while.
    Your can see the status of your submission in the \"Submissions\" tab of this
    challenge's page, where you will redirected to automatically after having submitted.
    In case the evaluation failed, the \"Status\" field shows \"failed\" and an error
    message in the \"Message\" field is displayed._"
  submission_instructions: "<p><em>Please provide the necessary information and select
    a submission file. As soon as a submission file is selected the form is submitted
    automatically. \nAfter the submission of the form the grading process will be
    initiated where an external grader validates/evaluates the submitted file and
    eventually returns the score back to CrowdAI. Depending on the file size, the
    evaluation algorithm and the total grading workload this could take a while. Your
    can see the status of your submission in the “Submissions” tab of this challenge’s
    page, where you will redirected to automatically after having submitted. In case
    the evaluation failed, the “Status” field shows “failed” and an error message
    in the “Message” field is displayed.</em></p>\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: tuberculosis_tbt_5.png
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 2
  clef_challenge: true
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: CLEFChallenges
  online_submissions: true
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: true
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
challenge_10:
  id: 10
  organizer_id: 7
  challenge: ImageCLEF 2018 Caption - Caption prediction
  status_cd: completed
  created_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &40 2017-08-15 05:35:24.953920000 Z
    zone: *2
    time: *40
  updated_at: !ruby/object:ActiveSupport::TimeWithZone
    utc: &41 2018-10-04 10:01:17.425132000 Z
    zone: *2
    time: *41
  tagline: Composing coherent captions for the entirety of an image
  primary_sort_order_cd: descending
  secondary_sort_order_cd: ascending
  perpetual_challenge: false
  answer_file_s3_key: 
  page_views: 2555
  participant_count: 101
  submission_count: 29
  score_title: BLEU
  score_secondary_title: ''
  slug: imageclef-2018-caption-caption-prediction
  submission_license: Please upload your submissions and include a detailed description
    of the methodology, techniques and insights leveraged with this submission. After
    the end of the challenge, these comments will be made public, and the submitted
    code and models will be freely available to other crowdAI participants. All submitted
    content will be licensed under Creative Commons (CC).
  api_required: false
  media_on_leaderboard: false
  challenge_client_name: caption_prediction_clef2018
  online_grading: true
  vote_count: 8
  description_markdown: "*****\r\n**Important note:**\r\n\r\n_The **ImageCLEF Caption
    - Caption Prediction challenge has officially ended** and we would like to thank
    everybody for their participation. You can find the official results at [http://imageclef.org/2018/caption](http://imageclef.org/2018/caption)._\r\n\r\n_Post-challenge
    submissions and the leaderboard will remain enabled  for a few weeks so you will
    still be able to submit result files and have them continuously evaluated during
    a limited period. \r\nPlease consider that in order to see the version of the
    leaderboard with the post-challenge submissions integrated, you have to turn on
    the switch **Show post-challenge submission** right below the leaderboard._\r\n\r\n_At
    the same time we'd like to encourage you to submit a [CLEF Working notes paper](http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html)
    until the end of May._\r\n\r\n_Please also note that participants registering
    from now on will not be\r\nautomatically registered with CLEF anymore._\r\n\r\n*****\r\n\r\n_Note:
    ImageCLEF Caption 2018 is divided into 2 subtasks (challenges). This challenge
    is about **Caption Prediction**. For information on the **Concept Detection**
    challenge click [ here ](/challenges/imageclef-2018-caption-concept-detection){:target='_blank'}.
    Both challenges share the same dataset, so registering for one of these challenges
    will automatically give you access to the other one._\r\n\r\n_Note: Do not forget
    to read the **Rules** section on this page_\r\n\r\n### Motivation\r\n\r\nInterpreting
    and summarizing the insights gained from medical images such as radiology output
    is a time-consuming task that involves highly trained experts and often represents
    a bottleneck in clinical diagnosis pipelines.\r\n\r\nConsequently, there is a
    considerable need for automatic methods that can approximate this mapping from
    visual information to condensed textual descriptions. In this task, we cast the
    problem of image understanding as a cross-modality matching scenario in which
    visual content and textual descriptors need to be aligned and concise textual
    interpretations of medical images are generated. We work on the basis of a large-scale
    collection of figures from open access biomedical journal articles (PubMed Central).
    Each image is accompanied by its original caption, constituting a natural testbed
    for this image captioning task.\r\n\r\n_Lessons learned_: In the first edition
    of this task, held at CLEF 2017, participants noted a broad topical variability
    among training images. This year, we will further group training data into image
    types (e.g., radiology vs. biopsy) and task participants will building either
    cross category models or category-specific ones. An additional source of uncertainty
    was noted in the use of external material. In this second edition of the task,
    we will clearly separate systems using exclusively the official training data
    from those that incorporate additional sources of evidence.\r\n\r\n### Challenge
    description\r\n\r\nOn the basis of the concept vocabulary detected in the first
    subtask as well as the visual information of their interaction in the image, participating
    systems are tasked with composing coherent captions for the entirety of an image.
    In this step, rather than the mere coverage of visual concepts, detecting the
    interplay of visible elements is crucial for strong performance. Evaluation of
    this second step is based on metrics such as BLEU that have been designed to be
    robust to variability in style and wording.\r\n\r\n### Data\r\n\r\nThe collection
    comprises a total of 4 million image-caption pairs that could potentially all
    be used for training with a small subset being removed for testing. To focus on
    useful radiology/clinical images and non-compound figures is likely good for this
    task to reduce the number of image-caption pairs to around 400,000, so significantly
    larger that in 2017.\r\n\r\n### Submission instructions\r\n\r\n*****\r\n_As soon
    as the submission is open, you will find a \"Create Submission\" button on this
    page (just next to the tabs)_\r\n\r\n*****\r\n\r\nFor the submission we expect
    the following format:\r\n\r\n[Figure-ID] [TAB] [description]\r\n\r\ne.g.:\r\n\r\n~~~\r\n1743-422X-4-12-1-4
    \  description of the first image in one single line\r\n1743-422X-4-12-1-3   description
    of the second image....\r\n1743-422X-4-12-1-2   descrition of the third image...\r\n~~~\r\n\r\nYou
    need to respect the following constraints:\r\n\r\n- The separator between the
    figure ID and the description has to be a tabular whitespace\r\n- Each figure
    ID of the testset must be included in the runfile exactly once\r\n- You should
    not include special characters in the description.\r\n\r\n\r\n### Acknowledgements\r\n\r\n[
    PubMed Central ](http://www.ncbi.nlm.nih.gov/pmc/){:target='_blank'}"
  description: "<hr />\n<p><strong>Important note:</strong></p>\n\n<p><em>The <strong>ImageCLEF
    Caption - Caption Prediction challenge has officially ended</strong> and we would
    like to thank everybody for their participation. You can find the official results
    at <a href=\"http://imageclef.org/2018/caption\">http://imageclef.org/2018/caption</a>.</em></p>\n\n<p><em>Post-challenge
    submissions and the leaderboard will remain enabled  for a few weeks so you will
    still be able to submit result files and have them continuously evaluated during
    a limited period. \nPlease consider that in order to see the version of the leaderboard
    with the post-challenge submissions integrated, you have to turn on the switch
    <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>\n\n<p><em>At
    the same time we’d like to encourage you to submit a <a href=\"http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html\">CLEF
    Working notes paper</a> until the end of May.</em></p>\n\n<p><em>Please also note
    that participants registering from now on will not be\nautomatically registered
    with CLEF anymore.</em></p>\n\n<hr />\n\n<p><em>Note: ImageCLEF Caption 2018 is
    divided into 2 subtasks (challenges). This challenge is about <strong>Caption
    Prediction</strong>. For information on the <strong>Concept Detection</strong>
    challenge click <a href=\"/challenges/imageclef-2018-caption-concept-detection\"
    target=\"_blank\"> here </a>. Both challenges share the same dataset, so registering
    for one of these challenges will automatically give you access to the other one.</em></p>\n\n<p><em>Note:
    Do not forget to read the <strong>Rules</strong> section on this page</em></p>\n\n<h3
    id=\"motivation\">Motivation</h3>\n\n<p>Interpreting and summarizing the insights
    gained from medical images such as radiology output is a time-consuming task that
    involves highly trained experts and often represents a bottleneck in clinical
    diagnosis pipelines.</p>\n\n<p>Consequently, there is a considerable need for
    automatic methods that can approximate this mapping from visual information to
    condensed textual descriptions. In this task, we cast the problem of image understanding
    as a cross-modality matching scenario in which visual content and textual descriptors
    need to be aligned and concise textual interpretations of medical images are generated.
    We work on the basis of a large-scale collection of figures from open access biomedical
    journal articles (PubMed Central). Each image is accompanied by its original caption,
    constituting a natural testbed for this image captioning task.</p>\n\n<p><em>Lessons
    learned</em>: In the first edition of this task, held at CLEF 2017, participants
    noted a broad topical variability among training images. This year, we will further
    group training data into image types (e.g., radiology vs. biopsy) and task participants
    will building either cross category models or category-specific ones. An additional
    source of uncertainty was noted in the use of external material. In this second
    edition of the task, we will clearly separate systems using exclusively the official
    training data from those that incorporate additional sources of evidence.</p>\n\n<h3
    id=\"challenge-description\">Challenge description</h3>\n\n<p>On the basis of
    the concept vocabulary detected in the first subtask as well as the visual information
    of their interaction in the image, participating systems are tasked with composing
    coherent captions for the entirety of an image. In this step, rather than the
    mere coverage of visual concepts, detecting the interplay of visible elements
    is crucial for strong performance. Evaluation of this second step is based on
    metrics such as BLEU that have been designed to be robust to variability in style
    and wording.</p>\n\n<h3 id=\"data\">Data</h3>\n\n<p>The collection comprises a
    total of 4 million image-caption pairs that could potentially all be used for
    training with a small subset being removed for testing. To focus on useful radiology/clinical
    images and non-compound figures is likely good for this task to reduce the number
    of image-caption pairs to around 400,000, so significantly larger that in 2017.</p>\n\n<h3
    id=\"submission-instructions\">Submission instructions</h3>\n\n<hr />\n<p><em>As
    soon as the submission is open, you will find a “Create Submission” button on
    this page (just next to the tabs)</em></p>\n\n<hr />\n\n<p>For the submission
    we expect the following format:</p>\n\n<p>[Figure-ID] [TAB] [description]</p>\n\n<p>e.g.:</p>\n\n<pre><code>1743-422X-4-12-1-4
    \  description of the first image in one single line\n1743-422X-4-12-1-3   description
    of the second image....\n1743-422X-4-12-1-2   descrition of the third image...\n</code></pre>\n\n<p>You
    need to respect the following constraints:</p>\n\n<ul>\n  <li>The separator between
    the figure ID and the description has to be a tabular whitespace</li>\n  <li>Each
    figure ID of the testset must be included in the runfile exactly once</li>\n  <li>You
    should not include special characters in the description.</li>\n</ul>\n\n<h3 id=\"acknowledgements\">Acknowledgements</h3>\n\n<p><a
    href=\"http://www.ncbi.nlm.nih.gov/pmc/\" target=\"_blank\"> PubMed Central </a></p>\n"
  evaluation_markdown: "Evaluation is based on **BLEU scores**, using the following
    methodology and parameters:\r\n\r\n* The default implementation of the Python
    [ NLTK ](http://nltk.org/){:target='_blank'} (v3.2.2) (Natural Language ToolKit)
    BLEU scoring method is used. It is documented here and based on the original article
    describing the [ BLEU evaluation method ](http://www.aclweb.org/anthology/P02-1040.pdf){:target='_blank'}\r\n\r\n*
    A Python (3.6) script loads the candidate run file, as well as the ground truth
    (GT) file, and processes each candidate-GT caption pair\r\n\r\n* Each caption
    is pre-processed in the following way:\r\n\r\n   - The caption is converted to
    lower-case\r\n\r\n   - All punctuation is removed an the caption is [ tokenized
    ](http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize){:target='_blank'}
    into its individual words\r\n\r\n   - Stopwords are removed using NLTK's \"english\"
    stopword list\r\n\r\n   - Stemming is applied using NLTK's [ Snowball stemmer
    ](http://www.nltk.org/_modules/nltk/stem/snowball.html){:target='_blank'}\r\n\r\n*
    The BLEU score is then calculated. Note that the caption is always considered
    as a single sentence, even if it actually contains several sentences. No smoothing
    function is used.\r\n\r\n* All BLEU scores are summed and averaged over the number
    of captions (10'000), giving the final score.\r\n\r\n**NOTE** : The source code
    of the evaluation tool is available [ here ](http://fast.hevs.ch/temp/ImageCLEF-CaptionPrediction-Evaluation.zip){:target='_blank'}.
    It **must** be executed using Python **3.6.x**, on a system where the NLTK (**v3.2.2**)
    Python library is installed. The script should be run like this:\r\n\r\n\r\n```
    \r\n/path/to/python3.6 evaluate-bleu.py /path/to/candidate/file /path/to/ground-truth/file
    \r\n```\r\n\r\n*****\r\n_The leaderboard will be visible from 01.05.2018 (official
    deadline) on. The submission system will remain open few more days. Results submitted
    after deadline will not be part of the official results._\r\n\r\n*****\r\n"
  evaluation: "<p>Evaluation is based on <strong>BLEU scores</strong>, using the following
    methodology and parameters:</p>\n\n<ul>\n  <li>\n    <p>The default implementation
    of the Python <a href=\"http://nltk.org/\" target=\"_blank\"> NLTK </a> (v3.2.2)
    (Natural Language ToolKit) BLEU scoring method is used. It is documented here
    and based on the original article describing the <a href=\"http://www.aclweb.org/anthology/P02-1040.pdf\"
    target=\"_blank\"> BLEU evaluation method </a></p>\n  </li>\n  <li>\n    <p>A
    Python (3.6) script loads the candidate run file, as well as the ground truth
    (GT) file, and processes each candidate-GT caption pair</p>\n  </li>\n  <li>\n
    \   <p>Each caption is pre-processed in the following way:</p>\n\n    <ul>\n      <li>\n
    \       <p>The caption is converted to lower-case</p>\n      </li>\n      <li>\n
    \       <p>All punctuation is removed an the caption is <a href=\"http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize\"
    target=\"_blank\"> tokenized </a> into its individual words</p>\n      </li>\n
    \     <li>\n        <p>Stopwords are removed using NLTK’s “english” stopword list</p>\n
    \     </li>\n      <li>\n        <p>Stemming is applied using NLTK’s <a href=\"http://www.nltk.org/_modules/nltk/stem/snowball.html\"
    target=\"_blank\"> Snowball stemmer </a></p>\n      </li>\n    </ul>\n  </li>\n
    \ <li>\n    <p>The BLEU score is then calculated. Note that the caption is always
    considered as a single sentence, even if it actually contains several sentences.
    No smoothing function is used.</p>\n  </li>\n  <li>\n    <p>All BLEU scores are
    summed and averaged over the number of captions (10’000), giving the final score.</p>\n
    \ </li>\n</ul>\n\n<p><strong>NOTE</strong> : The source code of the evaluation
    tool is available <a href=\"http://fast.hevs.ch/temp/ImageCLEF-CaptionPrediction-Evaluation.zip\"
    target=\"_blank\"> here </a>. It <strong>must</strong> be executed using Python
    <strong>3.6.x</strong>, on a system where the NLTK (<strong>v3.2.2</strong>) Python
    library is installed. The script should be run like this:</p>\n\n<p><code>\n/path/to/python3.6
    evaluate-bleu.py /path/to/candidate/file /path/to/ground-truth/file \n</code></p>\n\n<hr
    />\n<p><em>The leaderboard will be visible from 01.05.2018 (official deadline)
    on. The submission system will remain open few more days. Results submitted after
    deadline will not be part of the official results.</em></p>\n\n<hr />\n"
  rules_markdown: "_Note: In order to participate in this challenge you have to sign
    an End User Agreement (EUA). You will find more information on the 'Dataset' tab._\r\n\r\nImageCLEF
    lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF
    2018 consists of independent peer-reviewed workshops on a broad range of challenges
    in the fields of multilingual and multimodal information access evaluation, and
    a set of benchmarking activities carried in various labs designed to test different
    aspects of mono and cross-language Information retrieval systems. More details
    about the conference can be found [ here ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    .\r\n\r\nSubmitting a working note with the full description of the methods used
    in each run is mandatory. Any run that could not be reproduced thanks to its description
    in the working notes might be removed from the official publication of the results.
    Working notes are published within CEUR-WS proceedings, resulting in an assignment
    of an individual DOI (URN) and an indexing by many bibliography systems including
    DBLP. According to the CEUR-WS policies, a light review of the working notes will
    be conducted by ImageCLEF organizing committee to ensure quality. As an illustration,
    ImageCLEF 2017 working notes (task overviews and participant working notes) can
    be found within [ CLEF 2017 CEUR-WS ](http://ceur-ws.org/Vol-1866/){:target='_blank'}
    proceedings.\r\n\r\n### Important\r\n\r\nParticipants of this challenge will automatically
    be registered at CLEF 2018. In order to be compliant with the CLEF registration
    requirements, please edit your profile by providing the following additional information:\r\n\r\n*
    First name\r\n\r\n* Last name\r\n\r\n* Affiliation\r\n\r\n* Address\r\n\r\n* City\r\n\r\n*
    Country\r\n\r\n* _Regarding the username, please choose a name that represents
    your team._\r\n\r\n_This information will not be publicly visible and will be
    exclusively used to contact you and to send the registration data to CLEF, which
    is the main organizer of all CLEF labs_\r\n\r\n### Participating as an individual
    (non affiliated) researcher\r\n\r\nWe welcome individual researchers, i.e. not
    affiliated to any institution, to participate. We kindly ask you to provide us
    with a motivation letter containing the following information:\r\n\r\n- the presentation
    of your most relevant research activities related to the task/tasks\r\n\r\n- your
    motivation for participating in the task/tasks and how you want to exploit the
    results\r\n\r\n- a list of the most relevant 5 publications (if applicable)\r\n\r\n-
    the link to your personal webpage\r\n\r\nThe motivation letter should be directly
    concatenated to the End User Agreement document or sent as a PDF file to bionescu
    at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing
    committee. We reserve the right to refuse any applicants whose experience in the
    field is too narrow, and would therefore most likely prevent them from being able
    to finish the task/tasks."
  rules: |
    <p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Dataset’ tab.</em></p>

    <p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2018. CLEF 2018 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href="http://clef2018.clef-initiative.eu/" target="_blank"> here </a> .</p>

    <p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2017 working notes (task overviews and participant working notes) can be found within <a href="http://ceur-ws.org/Vol-1866/" target="_blank"> CLEF 2017 CEUR-WS </a> proceedings.</p>

    <h3 id="important">Important</h3>

    <p>Participants of this challenge will automatically be registered at CLEF 2018. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

    <ul>
      <li>
        <p>First name</p>
      </li>
      <li>
        <p>Last name</p>
      </li>
      <li>
        <p>Affiliation</p>
      </li>
      <li>
        <p>Address</p>
      </li>
      <li>
        <p>City</p>
      </li>
      <li>
        <p>Country</p>
      </li>
      <li>
        <p><em>Regarding the username, please choose a name that represents your team.</em></p>
      </li>
    </ul>

    <p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

    <h3 id="participating-as-an-individual-non-affiliated-researcher">Participating as an individual (non affiliated) researcher</h3>

    <p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

    <ul>
      <li>
        <p>the presentation of your most relevant research activities related to the task/tasks</p>
      </li>
      <li>
        <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
      </li>
      <li>
        <p>a list of the most relevant 5 publications (if applicable)</p>
      </li>
      <li>
        <p>the link to your personal webpage</p>
      </li>
    </ul>

    <p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>
  prizes_markdown: ImageCLEF 2018 is an evaluation campaign that is being organized
    as part of the [ CLEF initiative ](http://clef2018.clef-initiative.eu/){:target='_blank'}
    labs. The campaign offers several research tasks that welcome participation from
    teams around the world. The results of the campaign appear in the working notes
    proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions
    among the participants, will be invited for publication in the following year
    in the Springer Lecture Notes in Computer Science (LNCS) together with the annual
    lab overviews.
  prizes: '<p>ImageCLEF 2018 is an evaluation campaign that is being organized as
    part of the <a href="http://clef2018.clef-initiative.eu/" target="_blank"> CLEF
    initiative </a> labs. The campaign offers several research tasks that welcome
    participation from teams around the world. The results of the campaign appear
    in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org).
    Selected contributions among the participants, will be invited for publication
    in the following year in the Springer Lecture Notes in Computer Science (LNCS)
    together with the annual lab overviews.</p>

'
  resources_markdown: "###Contact us\r\n\r\n- Technical issues : [ https://gitter.im/crowdAI/imageclef-2018-caption-prediction
    ](https://gitter.im/crowdAI/imageclef-2018-caption-prediction){:target='_blank'}\r\n-
    Discussion Forum : [ https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics
    ](https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics){:target='_blank'}\r\n\r\nWe
    strongly encourage you to use the public channels mentioned above for communications
    between the participants and the organizers. In extreme cases, if there are any
    queries or comments that you would like to make using a private communication
    channel, then you can send us an email at :\r\n\r\n- Sharada Prasanna Mohanty:
    sharada.mohanty@epfl.ch\r\n- Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk\r\n-
    Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch\r\n- Vincent Adrearczyk: vincent[DOT]andrearczyk[AT]hevs[DOT]ch\r\n-
    Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch\r\n\r\n### More information\r\n\r\nYou
    can find additional information on the challenge here:\r\n[ http://imageclef.org/2018/caption
    ](http://imageclef.org/2018/caption){:target='_blank'}"
  resources: |
    <h3 id="contact-us">Contact us</h3>

    <ul>
      <li>Technical issues : <a href="https://gitter.im/crowdAI/imageclef-2018-caption-prediction" target="_blank"> https://gitter.im/crowdAI/imageclef-2018-caption-prediction </a></li>
      <li>Discussion Forum : <a href="https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics" target="_blank"> https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics </a></li>
    </ul>

    <p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

    <ul>
      <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
      <li>Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk</li>
      <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
      <li>Vincent Adrearczyk: vincent[DOT]andrearczyk[AT]hevs[DOT]ch</li>
      <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
    </ul>

    <h3 id="more-information">More information</h3>

    <p>You can find additional information on the challenge here:
    <a href="http://imageclef.org/2018/caption" target="_blank"> http://imageclef.org/2018/caption </a></p>
  submission_instructions_markdown: "_Please provide the necessary information and
    select a submission file. As soon as a submission file is selected the form is
    submitted automatically. \r\nAfter the submission of the form the grading process
    will be initiated where an external grader validates/evaluates the submitted file
    and eventually returns the score back to CrowdAI. Depending on the file size,
    the evaluation algorithm and the total grading workload this could take a while.
    Your can see the status of your submission in the \"Submissions\" tab of this
    challenge's page, where you will redirected to automatically after having submitted.
    In case the evaluation failed, the \"Status\" field shows \"failed\" and an error
    message in the \"Message\" field is displayed._"
  submission_instructions: "<p><em>Please provide the necessary information and select
    a submission file. As soon as a submission file is selected the form is submitted
    automatically. \nAfter the submission of the form the grading process will be
    initiated where an external grader validates/evaluates the submitted file and
    eventually returns the score back to CrowdAI. Depending on the file size, the
    evaluation algorithm and the total grading workload this could take a while. Your
    can see the status of your submission in the “Submissions” tab of this challenge’s
    page, where you will redirected to automatically after having submitted. In case
    the evaluation failed, the “Status” field shows “failed” and an error message
    in the “Message” field is displayed.</em></p>\n"
  license_markdown: ''
  license: "\n"
  dataset_description_markdown: ''
  dataset_description: "\n"
  image_file: caption-prediction_1.jpg
  featured_sequence: 0
  dynamic_content_flag: false
  dynamic_content: 
  dynamic_content_tab: 
  winner_description_markdown: 
  winner_description: 
  winners_tab_active: false
  clef_task_id: 1
  clef_challenge: true
  submissions_page: true
  private_challenge: false
  show_leaderboard: true
  grader_identifier: CLEFChallenges
  online_submissions: true
  grader_logs: false
  require_registration: false
  grading_history: false
  post_challenge_submissions: true
  submissions_downloadable: false
  dataset_note_markdown: 
  dataset_note: 
